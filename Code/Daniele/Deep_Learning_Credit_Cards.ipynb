{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Gemma's function for removing outliers\n",
    "sys.path.insert(0, './../Gemma/Part 1')\n",
    "from outliers import removeOutliers\n",
    "\n",
    "#import Riccardo's function for removing missing values\n",
    "sys.path.insert(0, './../Riccardo')\n",
    "from MissingValues_3 import remove_missing_values\n",
    "\n",
    "#import Daniele's function for converting education into a numerical attribute\n",
    "#import also Daniele's function for adding mean columns' value to the data frame\n",
    "from dependencies import create_data_frame_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_credit_default_to_numerical_attribute(credit_cards_input):    \n",
    "    credit_default_column = credit_cards_input[\"credit_default\"]\n",
    "    credit_default_column_new = []\n",
    "   \n",
    "    for default_row  in credit_default_column:\n",
    "        credit_default_column_new.append(default_to_number(default_row))\n",
    "       \n",
    "    credit_cards_input[\"credit_default\"] = credit_default_column_new\n",
    "    return credit_cards_input\n",
    "\n",
    "def default_to_number(category):\n",
    "    if category == \"no\":\n",
    "        return 0\n",
    "    elif category == \"yes\":\n",
    "        return 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_education_to_numerical_attribute(credit_cards_input):    \n",
    "    education_column = credit_cards_input[\"education\"]\n",
    "    education_column_new = []\n",
    "   \n",
    "    for education_row  in education_column:\n",
    "        education_column_new.append(educ_category_to_number(education_row))\n",
    "       \n",
    "    credit_cards_input[\"education\"] = education_column_new\n",
    "    return credit_cards_input\n",
    "\n",
    "def educ_category_to_number(category):\n",
    "    if category == \"others\":\n",
    "        return 0\n",
    "    elif category == \"high school\":\n",
    "        return 1\n",
    "    elif category == \"university\":\n",
    "        return 2\n",
    "    elif category == \"graduate school\":\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training data\n",
    "url = \"../../Dataset/credit_default_train.csv\"\n",
    "credit_cards_df = pd.read_csv(url)\n",
    "\n",
    "#firstly, remove missing values\n",
    "credit_cards_no_missing_outliers = remove_missing_values(credit_cards_df)\n",
    "#and remove outliers (this function operates in place)\n",
    "removeOutliers(credit_cards_no_missing_outliers)\n",
    "#create mean value columns\n",
    "credit_cards_avg = create_data_frame_avg(credit_cards_no_missing_outliers, [\"ba-apr\", \"ba-may\", \"ba-jun\", \"ba-jul\", \"ba-aug\", \"ba-sep\"], [\"pa-apr\", \"pa-may\", \"pa-jun\", \"pa-jul\", \"pa-aug\", \"pa-sep\"],  [\"ps-apr\", \"ps-may\", \"ps-jun\", \"ps-jul\", \"ps-aug\", \"ps-sep\"])\n",
    "credit_cards_edu_numerical = convert_education_to_numerical_attribute(credit_cards_avg)\n",
    "#and convert the credit_default into a numerical attribute as well\n",
    "credit_cards_default_num = convert_credit_default_to_numerical_attribute(credit_cards_edu_numerical)\n",
    "\n",
    "#pick the attributes you wanna use for deep learning\n",
    "attributes_deep_learning = [\"limit\", \"age\", \"education\", \"limit\", \"pa\", 'ps-may', 'ps-jun', 'ps-jul', 'ps-aug', 'ps-sep']\n",
    "credit_cards_deep_learning_train = credit_cards_default_num[attributes_deep_learning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial size of data frame:  (20000, 24)\n",
      "Visual analysis, number of rows to be dropped:  44\n",
      "Final size of data frame:  (19956, 24)\n"
     ]
    }
   ],
   "source": [
    "#Load the training data\n",
    "url = \"../../Dataset/credit_default_test.csv\"\n",
    "credit_cards_df = pd.read_csv(url)\n",
    "\n",
    "#firstly, remove missing values\n",
    "credit_cards_no_missing_outliers = remove_missing_values(credit_cards_df)\n",
    "#and remove outliers (this function operates in place)\n",
    "removeOutliers(credit_cards_no_missing_outliers)\n",
    "#create mean value columns\n",
    "credit_cards_avg = create_data_frame_avg(credit_cards_no_missing_outliers, [\"ba-apr\", \"ba-may\", \"ba-jun\", \"ba-jul\", \"ba-aug\", \"ba-sep\"], [\"pa-apr\", \"pa-may\", \"pa-jun\", \"pa-jul\", \"pa-aug\", \"pa-sep\"],  [\"ps-apr\", \"ps-may\", \"ps-jun\", \"ps-jul\", \"ps-aug\", \"ps-sep\"])\n",
    "credit_cards_edu_numerical = convert_education_to_numerical_attribute(credit_cards_avg)\n",
    "#and convert the credit_default into a numerical attribute as well\n",
    "\n",
    "#pick the attributes you wanna use for deep learning\n",
    "attributes_deep_learning = [\"limit\", \"age\", \"education\", \"limit\", \"pa\", 'ps-may', 'ps-jun', 'ps-jul', 'ps-aug', 'ps-sep']\n",
    "credit_cards_deep_learning_test = credit_cards_edu_numerical[attributes_deep_learning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9970, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>limit</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>limit</th>\n",
       "      <th>pa</th>\n",
       "      <th>ps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>180000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>180000</td>\n",
       "      <td>4093.500000</td>\n",
       "      <td>-1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>30000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3</td>\n",
       "      <td>30000</td>\n",
       "      <td>1902.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8267</th>\n",
       "      <td>130000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>130000</td>\n",
       "      <td>1073.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>10000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8869</th>\n",
       "      <td>30000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2</td>\n",
       "      <td>30000</td>\n",
       "      <td>1229.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       limit   age  education   limit           pa        ps\n",
       "746   180000  41.0          1  180000  4093.500000 -1.500000\n",
       "628    30000  27.0          3   30000  1902.000000  0.000000\n",
       "8267  130000  46.0          1  130000  1073.166667  0.000000\n",
       "561    10000  26.0          2   10000     0.000000  5.333333\n",
       "8869   30000  38.0          2   30000  1229.500000  0.333333"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data \n",
    "print(credit_cards_train.shape)\n",
    "credit_cards_train.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.00000000e+04 2.50000000e+01 3.00000000e+00 5.00000000e+04\n",
      "  1.54100000e+03]\n",
      " [2.00000000e+05 5.40000000e+01 2.00000000e+00 2.00000000e+05\n",
      "  7.06416667e+03]\n",
      " [3.00000000e+04 4.10000000e+01 1.00000000e+00 3.00000000e+04\n",
      "  8.59833333e+02]\n",
      " ...\n",
      " [9.00000000e+04 4.70000000e+01 2.00000000e+00 9.00000000e+04\n",
      "  2.00000000e+03]\n",
      " [2.00000000e+04 5.90000000e+01 2.00000000e+00 2.00000000e+04\n",
      "  5.95500000e+02]\n",
      " [2.80000000e+05 3.50000000e+01 2.00000000e+00 2.80000000e+05\n",
      "  4.82843333e+04]]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Input: X is the dataframe without the credit_default label\n",
    "\n",
    "X = credit_cards_train.iloc[:, :-1].values\n",
    "#the output consists of the state with diabetes\n",
    "y = credit_cards_default_num[\"credit_default\"].values\n",
    "\n",
    "print(X)\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22146439317953862, 0.7785356068204614)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "#we train both with X input data and Y input data\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.783\n",
      "roc-auc is 0.717\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test) #HARD\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test) #SOFT\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 9-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(9,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 12)                120       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 133\n",
      "Trainable params: 133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7477 samples, validate on 2493 samples\n",
      "Epoch 1/200\n",
      "7477/7477 [==============================] - 0s 46us/step - loss: 0.6089 - acc: 0.7055 - val_loss: 0.5690 - val_acc: 0.7621\n",
      "Epoch 2/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.5491 - acc: 0.7820 - val_loss: 0.5358 - val_acc: 0.7942\n",
      "Epoch 3/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.5256 - acc: 0.7963 - val_loss: 0.5214 - val_acc: 0.7886\n",
      "Epoch 4/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.5141 - acc: 0.7934 - val_loss: 0.5137 - val_acc: 0.7906\n",
      "Epoch 5/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.5074 - acc: 0.7928 - val_loss: 0.5088 - val_acc: 0.7894\n",
      "Epoch 6/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.5028 - acc: 0.7939 - val_loss: 0.5052 - val_acc: 0.7914\n",
      "Epoch 7/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4994 - acc: 0.7952 - val_loss: 0.5024 - val_acc: 0.7922\n",
      "Epoch 8/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4966 - acc: 0.7964 - val_loss: 0.5000 - val_acc: 0.7954\n",
      "Epoch 9/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4943 - acc: 0.7976 - val_loss: 0.4980 - val_acc: 0.7990\n",
      "Epoch 10/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4923 - acc: 0.7988 - val_loss: 0.4962 - val_acc: 0.8006\n",
      "Epoch 11/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4906 - acc: 0.8010 - val_loss: 0.4946 - val_acc: 0.8006\n",
      "Epoch 12/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4890 - acc: 0.8010 - val_loss: 0.4932 - val_acc: 0.8014\n",
      "Epoch 13/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4876 - acc: 0.8013 - val_loss: 0.4919 - val_acc: 0.8030\n",
      "Epoch 14/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4863 - acc: 0.8023 - val_loss: 0.4907 - val_acc: 0.8018\n",
      "Epoch 15/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4851 - acc: 0.8026 - val_loss: 0.4896 - val_acc: 0.8014\n",
      "Epoch 16/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4841 - acc: 0.8039 - val_loss: 0.4886 - val_acc: 0.8014\n",
      "Epoch 17/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4831 - acc: 0.8043 - val_loss: 0.4876 - val_acc: 0.8014\n",
      "Epoch 18/200\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4822 - acc: 0.8047 - val_loss: 0.4868 - val_acc: 0.8022\n",
      "Epoch 19/200\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4813 - acc: 0.8051 - val_loss: 0.4860 - val_acc: 0.8022\n",
      "Epoch 20/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4806 - acc: 0.8046 - val_loss: 0.4852 - val_acc: 0.8026\n",
      "Epoch 21/200\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4798 - acc: 0.8046 - val_loss: 0.4846 - val_acc: 0.8026\n",
      "Epoch 22/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4792 - acc: 0.8057 - val_loss: 0.4839 - val_acc: 0.8022\n",
      "Epoch 23/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4785 - acc: 0.8057 - val_loss: 0.4833 - val_acc: 0.8022\n",
      "Epoch 24/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4779 - acc: 0.8051 - val_loss: 0.4827 - val_acc: 0.8022\n",
      "Epoch 25/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4774 - acc: 0.8061 - val_loss: 0.4822 - val_acc: 0.8022\n",
      "Epoch 26/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4768 - acc: 0.8058 - val_loss: 0.4816 - val_acc: 0.8026\n",
      "Epoch 27/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4764 - acc: 0.8058 - val_loss: 0.4812 - val_acc: 0.8026\n",
      "Epoch 28/200\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4759 - acc: 0.8054 - val_loss: 0.4807 - val_acc: 0.8030\n",
      "Epoch 29/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4755 - acc: 0.8050 - val_loss: 0.4803 - val_acc: 0.8030\n",
      "Epoch 30/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4751 - acc: 0.8051 - val_loss: 0.4799 - val_acc: 0.8030\n",
      "Epoch 31/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4747 - acc: 0.8047 - val_loss: 0.4795 - val_acc: 0.8030\n",
      "Epoch 32/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4743 - acc: 0.8051 - val_loss: 0.4791 - val_acc: 0.8026\n",
      "Epoch 33/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4740 - acc: 0.8049 - val_loss: 0.4787 - val_acc: 0.8026\n",
      "Epoch 34/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4737 - acc: 0.8045 - val_loss: 0.4784 - val_acc: 0.8026\n",
      "Epoch 35/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4734 - acc: 0.8054 - val_loss: 0.4781 - val_acc: 0.8030\n",
      "Epoch 36/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4731 - acc: 0.8043 - val_loss: 0.4778 - val_acc: 0.8030\n",
      "Epoch 37/200\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4728 - acc: 0.8042 - val_loss: 0.4775 - val_acc: 0.8034\n",
      "Epoch 38/200\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4726 - acc: 0.8041 - val_loss: 0.4772 - val_acc: 0.8034\n",
      "Epoch 39/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4723 - acc: 0.8045 - val_loss: 0.4769 - val_acc: 0.8039\n",
      "Epoch 40/200\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4721 - acc: 0.8045 - val_loss: 0.4767 - val_acc: 0.8039\n",
      "Epoch 41/200\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4718 - acc: 0.8045 - val_loss: 0.4764 - val_acc: 0.8039\n",
      "Epoch 42/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4716 - acc: 0.8042 - val_loss: 0.4762 - val_acc: 0.8039\n",
      "Epoch 43/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4714 - acc: 0.8047 - val_loss: 0.4759 - val_acc: 0.8047\n",
      "Epoch 44/200\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4712 - acc: 0.8041 - val_loss: 0.4757 - val_acc: 0.8051\n",
      "Epoch 45/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4710 - acc: 0.8045 - val_loss: 0.4755 - val_acc: 0.8055\n",
      "Epoch 46/200\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4708 - acc: 0.8045 - val_loss: 0.4753 - val_acc: 0.8055\n",
      "Epoch 47/200\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4707 - acc: 0.8046 - val_loss: 0.4751 - val_acc: 0.8067\n",
      "Epoch 48/200\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4705 - acc: 0.8046 - val_loss: 0.4749 - val_acc: 0.8059\n",
      "Epoch 49/200\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4703 - acc: 0.8049 - val_loss: 0.4747 - val_acc: 0.8047\n",
      "Epoch 50/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4702 - acc: 0.8045 - val_loss: 0.4745 - val_acc: 0.8047\n",
      "Epoch 51/200\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4700 - acc: 0.8047 - val_loss: 0.4743 - val_acc: 0.8047\n",
      "Epoch 52/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4699 - acc: 0.8045 - val_loss: 0.4742 - val_acc: 0.8047\n",
      "Epoch 53/200\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4697 - acc: 0.8047 - val_loss: 0.4740 - val_acc: 0.8043\n",
      "Epoch 54/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4696 - acc: 0.8046 - val_loss: 0.4738 - val_acc: 0.8047\n",
      "Epoch 55/200\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4695 - acc: 0.8047 - val_loss: 0.4737 - val_acc: 0.8043\n",
      "Epoch 56/200\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4693 - acc: 0.8041 - val_loss: 0.4735 - val_acc: 0.8043\n",
      "Epoch 57/200\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4692 - acc: 0.8046 - val_loss: 0.4734 - val_acc: 0.8043\n",
      "Epoch 58/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4691 - acc: 0.8042 - val_loss: 0.4732 - val_acc: 0.8055\n",
      "Epoch 59/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4690 - acc: 0.8049 - val_loss: 0.4731 - val_acc: 0.8051\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4689 - acc: 0.8043 - val_loss: 0.4729 - val_acc: 0.8059\n",
      "Epoch 61/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4688 - acc: 0.8045 - val_loss: 0.4728 - val_acc: 0.8055\n",
      "Epoch 62/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4687 - acc: 0.8045 - val_loss: 0.4727 - val_acc: 0.8055\n",
      "Epoch 63/200\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4686 - acc: 0.8042 - val_loss: 0.4725 - val_acc: 0.8055\n",
      "Epoch 64/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4685 - acc: 0.8050 - val_loss: 0.4724 - val_acc: 0.8055\n",
      "Epoch 65/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4684 - acc: 0.8043 - val_loss: 0.4723 - val_acc: 0.8055\n",
      "Epoch 66/200\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4683 - acc: 0.8046 - val_loss: 0.4722 - val_acc: 0.8055\n",
      "Epoch 67/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4682 - acc: 0.8049 - val_loss: 0.4720 - val_acc: 0.8055\n",
      "Epoch 68/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4681 - acc: 0.8041 - val_loss: 0.4719 - val_acc: 0.8055\n",
      "Epoch 69/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4680 - acc: 0.8045 - val_loss: 0.4718 - val_acc: 0.8051\n",
      "Epoch 70/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4679 - acc: 0.8045 - val_loss: 0.4717 - val_acc: 0.8047\n",
      "Epoch 71/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4678 - acc: 0.8049 - val_loss: 0.4716 - val_acc: 0.8047\n",
      "Epoch 72/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4677 - acc: 0.8041 - val_loss: 0.4715 - val_acc: 0.8051\n",
      "Epoch 73/200\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4676 - acc: 0.8043 - val_loss: 0.4714 - val_acc: 0.8047\n",
      "Epoch 74/200\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4676 - acc: 0.8043 - val_loss: 0.4713 - val_acc: 0.8051\n",
      "Epoch 75/200\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4675 - acc: 0.8045 - val_loss: 0.4712 - val_acc: 0.8047\n",
      "Epoch 76/200\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4674 - acc: 0.8042 - val_loss: 0.4711 - val_acc: 0.8051\n",
      "Epoch 77/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4673 - acc: 0.8043 - val_loss: 0.4709 - val_acc: 0.8051\n",
      "Epoch 78/200\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4673 - acc: 0.8043 - val_loss: 0.4708 - val_acc: 0.8051\n",
      "Epoch 79/200\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4672 - acc: 0.8039 - val_loss: 0.4708 - val_acc: 0.8047\n",
      "Epoch 80/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4671 - acc: 0.8042 - val_loss: 0.4707 - val_acc: 0.8047\n",
      "Epoch 81/200\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4670 - acc: 0.8039 - val_loss: 0.4706 - val_acc: 0.8047\n",
      "Epoch 82/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4670 - acc: 0.8038 - val_loss: 0.4705 - val_acc: 0.8055\n",
      "Epoch 83/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4669 - acc: 0.8041 - val_loss: 0.4704 - val_acc: 0.8055\n",
      "Epoch 84/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4668 - acc: 0.8038 - val_loss: 0.4703 - val_acc: 0.8055\n",
      "Epoch 85/200\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4668 - acc: 0.8031 - val_loss: 0.4702 - val_acc: 0.8055\n",
      "Epoch 86/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4667 - acc: 0.8039 - val_loss: 0.4701 - val_acc: 0.8051\n",
      "Epoch 87/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4666 - acc: 0.8039 - val_loss: 0.4700 - val_acc: 0.8059\n",
      "Epoch 88/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4666 - acc: 0.8039 - val_loss: 0.4699 - val_acc: 0.8051\n",
      "Epoch 89/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4665 - acc: 0.8039 - val_loss: 0.4698 - val_acc: 0.8055\n",
      "Epoch 90/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4664 - acc: 0.8042 - val_loss: 0.4697 - val_acc: 0.8055\n",
      "Epoch 91/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4664 - acc: 0.8038 - val_loss: 0.4697 - val_acc: 0.8055\n",
      "Epoch 92/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4663 - acc: 0.8042 - val_loss: 0.4696 - val_acc: 0.8055\n",
      "Epoch 93/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4663 - acc: 0.8039 - val_loss: 0.4695 - val_acc: 0.8055\n",
      "Epoch 94/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4662 - acc: 0.8037 - val_loss: 0.4694 - val_acc: 0.8059\n",
      "Epoch 95/200\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4661 - acc: 0.8037 - val_loss: 0.4693 - val_acc: 0.8059\n",
      "Epoch 96/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4661 - acc: 0.8039 - val_loss: 0.4692 - val_acc: 0.8055\n",
      "Epoch 97/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4660 - acc: 0.8039 - val_loss: 0.4691 - val_acc: 0.8055\n",
      "Epoch 98/200\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4660 - acc: 0.8034 - val_loss: 0.4691 - val_acc: 0.8055\n",
      "Epoch 99/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4659 - acc: 0.8042 - val_loss: 0.4690 - val_acc: 0.8059\n",
      "Epoch 100/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4658 - acc: 0.8038 - val_loss: 0.4689 - val_acc: 0.8055\n",
      "Epoch 101/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4658 - acc: 0.8031 - val_loss: 0.4689 - val_acc: 0.8059\n",
      "Epoch 102/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4657 - acc: 0.8037 - val_loss: 0.4688 - val_acc: 0.8055\n",
      "Epoch 103/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4657 - acc: 0.8034 - val_loss: 0.4687 - val_acc: 0.8063\n",
      "Epoch 104/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4656 - acc: 0.8037 - val_loss: 0.4687 - val_acc: 0.8067\n",
      "Epoch 105/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4656 - acc: 0.8034 - val_loss: 0.4686 - val_acc: 0.8067\n",
      "Epoch 106/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4655 - acc: 0.8033 - val_loss: 0.4685 - val_acc: 0.8059\n",
      "Epoch 107/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4655 - acc: 0.8035 - val_loss: 0.4684 - val_acc: 0.8063\n",
      "Epoch 108/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4654 - acc: 0.8035 - val_loss: 0.4684 - val_acc: 0.8059\n",
      "Epoch 109/200\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4654 - acc: 0.8034 - val_loss: 0.4683 - val_acc: 0.8063\n",
      "Epoch 110/200\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4653 - acc: 0.8035 - val_loss: 0.4682 - val_acc: 0.8063\n",
      "Epoch 111/200\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4653 - acc: 0.8034 - val_loss: 0.4682 - val_acc: 0.8055\n",
      "Epoch 112/200\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4652 - acc: 0.8034 - val_loss: 0.4681 - val_acc: 0.8055\n",
      "Epoch 113/200\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4652 - acc: 0.8037 - val_loss: 0.4681 - val_acc: 0.8059\n",
      "Epoch 114/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4651 - acc: 0.8033 - val_loss: 0.4680 - val_acc: 0.8063\n",
      "Epoch 115/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4651 - acc: 0.8034 - val_loss: 0.4679 - val_acc: 0.8063\n",
      "Epoch 116/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4650 - acc: 0.8037 - val_loss: 0.4679 - val_acc: 0.8063\n",
      "Epoch 117/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4650 - acc: 0.8033 - val_loss: 0.4678 - val_acc: 0.8059\n",
      "Epoch 118/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4649 - acc: 0.8034 - val_loss: 0.4677 - val_acc: 0.8063\n",
      "Epoch 119/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4649 - acc: 0.8031 - val_loss: 0.4677 - val_acc: 0.8063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4648 - acc: 0.8033 - val_loss: 0.4676 - val_acc: 0.8063\n",
      "Epoch 121/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4648 - acc: 0.8034 - val_loss: 0.4675 - val_acc: 0.8063\n",
      "Epoch 122/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4648 - acc: 0.8031 - val_loss: 0.4675 - val_acc: 0.8067\n",
      "Epoch 123/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4647 - acc: 0.8033 - val_loss: 0.4674 - val_acc: 0.8067\n",
      "Epoch 124/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4647 - acc: 0.8034 - val_loss: 0.4674 - val_acc: 0.8063\n",
      "Epoch 125/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4646 - acc: 0.8034 - val_loss: 0.4674 - val_acc: 0.8063\n",
      "Epoch 126/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4646 - acc: 0.8035 - val_loss: 0.4673 - val_acc: 0.8063\n",
      "Epoch 127/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4646 - acc: 0.8031 - val_loss: 0.4672 - val_acc: 0.8063\n",
      "Epoch 128/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4645 - acc: 0.8033 - val_loss: 0.4672 - val_acc: 0.8067\n",
      "Epoch 129/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4645 - acc: 0.8034 - val_loss: 0.4671 - val_acc: 0.8063\n",
      "Epoch 130/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4644 - acc: 0.8034 - val_loss: 0.4671 - val_acc: 0.8063\n",
      "Epoch 131/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4644 - acc: 0.8039 - val_loss: 0.4670 - val_acc: 0.8063\n",
      "Epoch 132/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4643 - acc: 0.8037 - val_loss: 0.4670 - val_acc: 0.8063\n",
      "Epoch 133/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4643 - acc: 0.8041 - val_loss: 0.4669 - val_acc: 0.8063\n",
      "Epoch 134/200\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4643 - acc: 0.8038 - val_loss: 0.4669 - val_acc: 0.8063\n",
      "Epoch 135/200\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4642 - acc: 0.8041 - val_loss: 0.4668 - val_acc: 0.8063\n",
      "Epoch 136/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4642 - acc: 0.8035 - val_loss: 0.4668 - val_acc: 0.8059\n",
      "Epoch 137/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4642 - acc: 0.8041 - val_loss: 0.4667 - val_acc: 0.8059\n",
      "Epoch 138/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4641 - acc: 0.8039 - val_loss: 0.4667 - val_acc: 0.8059\n",
      "Epoch 139/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4641 - acc: 0.8041 - val_loss: 0.4666 - val_acc: 0.8063\n",
      "Epoch 140/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4641 - acc: 0.8042 - val_loss: 0.4666 - val_acc: 0.8063\n",
      "Epoch 141/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4640 - acc: 0.8041 - val_loss: 0.4666 - val_acc: 0.8063\n",
      "Epoch 142/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4640 - acc: 0.8037 - val_loss: 0.4665 - val_acc: 0.8059\n",
      "Epoch 143/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4639 - acc: 0.8038 - val_loss: 0.4664 - val_acc: 0.8059\n",
      "Epoch 144/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4639 - acc: 0.8038 - val_loss: 0.4664 - val_acc: 0.8063\n",
      "Epoch 145/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4639 - acc: 0.8034 - val_loss: 0.4664 - val_acc: 0.8063\n",
      "Epoch 146/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4639 - acc: 0.8038 - val_loss: 0.4663 - val_acc: 0.8063\n",
      "Epoch 147/200\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4638 - acc: 0.8039 - val_loss: 0.4663 - val_acc: 0.8063\n",
      "Epoch 148/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4638 - acc: 0.8043 - val_loss: 0.4663 - val_acc: 0.8067\n",
      "Epoch 149/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4637 - acc: 0.8037 - val_loss: 0.4662 - val_acc: 0.8059\n",
      "Epoch 150/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4637 - acc: 0.8037 - val_loss: 0.4662 - val_acc: 0.8059\n",
      "Epoch 151/200\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4637 - acc: 0.8042 - val_loss: 0.4661 - val_acc: 0.8059\n",
      "Epoch 152/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4636 - acc: 0.8041 - val_loss: 0.4661 - val_acc: 0.8063\n",
      "Epoch 153/200\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4636 - acc: 0.8041 - val_loss: 0.4661 - val_acc: 0.8063\n",
      "Epoch 154/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4636 - acc: 0.8041 - val_loss: 0.4661 - val_acc: 0.8067\n",
      "Epoch 155/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4635 - acc: 0.8037 - val_loss: 0.4661 - val_acc: 0.8067\n",
      "Epoch 156/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4635 - acc: 0.8043 - val_loss: 0.4660 - val_acc: 0.8063\n",
      "Epoch 157/200\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4635 - acc: 0.8037 - val_loss: 0.4660 - val_acc: 0.8059\n",
      "Epoch 158/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4634 - acc: 0.8042 - val_loss: 0.4659 - val_acc: 0.8055\n",
      "Epoch 159/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4634 - acc: 0.8041 - val_loss: 0.4659 - val_acc: 0.8055\n",
      "Epoch 160/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4634 - acc: 0.8038 - val_loss: 0.4659 - val_acc: 0.8055\n",
      "Epoch 161/200\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4634 - acc: 0.8041 - val_loss: 0.4658 - val_acc: 0.8055\n",
      "Epoch 162/200\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4633 - acc: 0.8037 - val_loss: 0.4658 - val_acc: 0.8055\n",
      "Epoch 163/200\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4633 - acc: 0.8037 - val_loss: 0.4658 - val_acc: 0.8051\n",
      "Epoch 164/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4633 - acc: 0.8042 - val_loss: 0.4657 - val_acc: 0.8051\n",
      "Epoch 165/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4633 - acc: 0.8039 - val_loss: 0.4657 - val_acc: 0.8055\n",
      "Epoch 166/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4632 - acc: 0.8035 - val_loss: 0.4657 - val_acc: 0.8055\n",
      "Epoch 167/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4632 - acc: 0.8039 - val_loss: 0.4657 - val_acc: 0.8055\n",
      "Epoch 168/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4632 - acc: 0.8038 - val_loss: 0.4657 - val_acc: 0.8055\n",
      "Epoch 169/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4631 - acc: 0.8037 - val_loss: 0.4656 - val_acc: 0.8055\n",
      "Epoch 170/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4631 - acc: 0.8038 - val_loss: 0.4656 - val_acc: 0.8055\n",
      "Epoch 171/200\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4631 - acc: 0.8038 - val_loss: 0.4656 - val_acc: 0.8055\n",
      "Epoch 172/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4631 - acc: 0.8039 - val_loss: 0.4655 - val_acc: 0.8055\n",
      "Epoch 173/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4630 - acc: 0.8039 - val_loss: 0.4655 - val_acc: 0.8055\n",
      "Epoch 174/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4630 - acc: 0.8038 - val_loss: 0.4655 - val_acc: 0.8059\n",
      "Epoch 175/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4630 - acc: 0.8038 - val_loss: 0.4654 - val_acc: 0.8055\n",
      "Epoch 176/200\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4630 - acc: 0.8041 - val_loss: 0.4654 - val_acc: 0.8055\n",
      "Epoch 177/200\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4629 - acc: 0.8041 - val_loss: 0.4654 - val_acc: 0.8055\n",
      "Epoch 178/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4629 - acc: 0.8039 - val_loss: 0.4653 - val_acc: 0.8055\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4629 - acc: 0.8042 - val_loss: 0.4653 - val_acc: 0.8055\n",
      "Epoch 180/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4629 - acc: 0.8038 - val_loss: 0.4653 - val_acc: 0.8055\n",
      "Epoch 181/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4629 - acc: 0.8037 - val_loss: 0.4653 - val_acc: 0.8055\n",
      "Epoch 182/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4628 - acc: 0.8039 - val_loss: 0.4652 - val_acc: 0.8055\n",
      "Epoch 183/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4628 - acc: 0.8037 - val_loss: 0.4652 - val_acc: 0.8055\n",
      "Epoch 184/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4628 - acc: 0.8039 - val_loss: 0.4652 - val_acc: 0.8055\n",
      "Epoch 185/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4627 - acc: 0.8038 - val_loss: 0.4652 - val_acc: 0.8059\n",
      "Epoch 186/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4627 - acc: 0.8038 - val_loss: 0.4651 - val_acc: 0.8059\n",
      "Epoch 187/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4627 - acc: 0.8041 - val_loss: 0.4651 - val_acc: 0.8055\n",
      "Epoch 188/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4627 - acc: 0.8041 - val_loss: 0.4651 - val_acc: 0.8055\n",
      "Epoch 189/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4627 - acc: 0.8041 - val_loss: 0.4650 - val_acc: 0.8055\n",
      "Epoch 190/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4626 - acc: 0.8042 - val_loss: 0.4650 - val_acc: 0.8059\n",
      "Epoch 191/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4626 - acc: 0.8039 - val_loss: 0.4650 - val_acc: 0.8059\n",
      "Epoch 192/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4626 - acc: 0.8041 - val_loss: 0.4649 - val_acc: 0.8051\n",
      "Epoch 193/200\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4626 - acc: 0.8045 - val_loss: 0.4649 - val_acc: 0.8051\n",
      "Epoch 194/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4626 - acc: 0.8041 - val_loss: 0.4649 - val_acc: 0.8055\n",
      "Epoch 195/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4625 - acc: 0.8042 - val_loss: 0.4649 - val_acc: 0.8055\n",
      "Epoch 196/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4625 - acc: 0.8035 - val_loss: 0.4649 - val_acc: 0.8059\n",
      "Epoch 197/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4625 - acc: 0.8042 - val_loss: 0.4648 - val_acc: 0.8059\n",
      "Epoch 198/200\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4625 - acc: 0.8039 - val_loss: 0.4648 - val_acc: 0.8059\n",
      "Epoch 199/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4625 - acc: 0.8042 - val_loss: 0.4648 - val_acc: 0.8063\n",
      "Epoch 200/200\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4624 - acc: 0.8039 - val_loss: 0.4647 - val_acc: 0.8059\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]\n",
    "y_pred_class_nn_1 #Resulting prediction for every single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15656613],\n",
       "       [0.11879377],\n",
       "       [0.17529505],\n",
       "       [0.24763307],\n",
       "       [0.13805325],\n",
       "       [0.36484885],\n",
       "       [0.06943215],\n",
       "       [0.02405155],\n",
       "       [0.21637224],\n",
       "       [0.06136973]], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions' results from the test casa\n",
    "y_pred_prob_nn_1[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.807\n",
      "roc-auc is 0.731\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f707c29fa58>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7477 samples, validate on 2493 samples\n",
      "Epoch 1/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4621 - acc: 0.8038 - val_loss: 0.4632 - val_acc: 0.8067\n",
      "Epoch 2/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4621 - acc: 0.8039 - val_loss: 0.4632 - val_acc: 0.8059\n",
      "Epoch 3/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4620 - acc: 0.8038 - val_loss: 0.4631 - val_acc: 0.8063\n",
      "Epoch 4/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4620 - acc: 0.8037 - val_loss: 0.4631 - val_acc: 0.8063\n",
      "Epoch 5/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4620 - acc: 0.8037 - val_loss: 0.4631 - val_acc: 0.8059\n",
      "Epoch 6/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4620 - acc: 0.8034 - val_loss: 0.4631 - val_acc: 0.8059\n",
      "Epoch 7/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4619 - acc: 0.8039 - val_loss: 0.4631 - val_acc: 0.8059\n",
      "Epoch 8/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4619 - acc: 0.8037 - val_loss: 0.4631 - val_acc: 0.8059\n",
      "Epoch 9/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4619 - acc: 0.8037 - val_loss: 0.4630 - val_acc: 0.8063\n",
      "Epoch 10/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4619 - acc: 0.8033 - val_loss: 0.4630 - val_acc: 0.8063\n",
      "Epoch 11/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4618 - acc: 0.8034 - val_loss: 0.4630 - val_acc: 0.8063\n",
      "Epoch 12/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4618 - acc: 0.8034 - val_loss: 0.4630 - val_acc: 0.8063\n",
      "Epoch 13/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4618 - acc: 0.8031 - val_loss: 0.4630 - val_acc: 0.8063\n",
      "Epoch 14/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4617 - acc: 0.8037 - val_loss: 0.4630 - val_acc: 0.8067\n",
      "Epoch 15/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4617 - acc: 0.8035 - val_loss: 0.4630 - val_acc: 0.8067\n",
      "Epoch 16/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4617 - acc: 0.8037 - val_loss: 0.4629 - val_acc: 0.8063\n",
      "Epoch 17/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4617 - acc: 0.8034 - val_loss: 0.4629 - val_acc: 0.8067\n",
      "Epoch 18/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4616 - acc: 0.8030 - val_loss: 0.4629 - val_acc: 0.8067\n",
      "Epoch 19/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4616 - acc: 0.8033 - val_loss: 0.4629 - val_acc: 0.8063\n",
      "Epoch 20/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4616 - acc: 0.8033 - val_loss: 0.4629 - val_acc: 0.8063\n",
      "Epoch 21/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4616 - acc: 0.8029 - val_loss: 0.4629 - val_acc: 0.8059\n",
      "Epoch 22/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4615 - acc: 0.8033 - val_loss: 0.4629 - val_acc: 0.8059\n",
      "Epoch 23/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4615 - acc: 0.8033 - val_loss: 0.4628 - val_acc: 0.8063\n",
      "Epoch 24/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4615 - acc: 0.8035 - val_loss: 0.4628 - val_acc: 0.8059\n",
      "Epoch 25/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4615 - acc: 0.8037 - val_loss: 0.4628 - val_acc: 0.8059\n",
      "Epoch 26/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4614 - acc: 0.8033 - val_loss: 0.4628 - val_acc: 0.8059\n",
      "Epoch 27/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4614 - acc: 0.8035 - val_loss: 0.4628 - val_acc: 0.8059\n",
      "Epoch 28/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4614 - acc: 0.8035 - val_loss: 0.4628 - val_acc: 0.8059\n",
      "Epoch 29/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4614 - acc: 0.8033 - val_loss: 0.4627 - val_acc: 0.8063\n",
      "Epoch 30/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4613 - acc: 0.8034 - val_loss: 0.4627 - val_acc: 0.8059\n",
      "Epoch 31/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4613 - acc: 0.8031 - val_loss: 0.4627 - val_acc: 0.8059\n",
      "Epoch 32/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4613 - acc: 0.8035 - val_loss: 0.4627 - val_acc: 0.8059\n",
      "Epoch 33/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4613 - acc: 0.8035 - val_loss: 0.4627 - val_acc: 0.8059\n",
      "Epoch 34/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4613 - acc: 0.8035 - val_loss: 0.4626 - val_acc: 0.8059\n",
      "Epoch 35/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4612 - acc: 0.8034 - val_loss: 0.4626 - val_acc: 0.8059\n",
      "Epoch 36/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4612 - acc: 0.8035 - val_loss: 0.4626 - val_acc: 0.8059\n",
      "Epoch 37/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4612 - acc: 0.8035 - val_loss: 0.4626 - val_acc: 0.8059\n",
      "Epoch 38/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4612 - acc: 0.8035 - val_loss: 0.4625 - val_acc: 0.8063\n",
      "Epoch 39/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4611 - acc: 0.8034 - val_loss: 0.4625 - val_acc: 0.8059\n",
      "Epoch 40/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4611 - acc: 0.8035 - val_loss: 0.4625 - val_acc: 0.8059\n",
      "Epoch 41/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4611 - acc: 0.8035 - val_loss: 0.4625 - val_acc: 0.8059\n",
      "Epoch 42/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4611 - acc: 0.8035 - val_loss: 0.4625 - val_acc: 0.8059\n",
      "Epoch 43/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4610 - acc: 0.8034 - val_loss: 0.4625 - val_acc: 0.8059\n",
      "Epoch 44/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4610 - acc: 0.8035 - val_loss: 0.4624 - val_acc: 0.8059\n",
      "Epoch 45/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4610 - acc: 0.8035 - val_loss: 0.4624 - val_acc: 0.8059\n",
      "Epoch 46/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4610 - acc: 0.8034 - val_loss: 0.4624 - val_acc: 0.8059\n",
      "Epoch 47/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4609 - acc: 0.8034 - val_loss: 0.4624 - val_acc: 0.8059\n",
      "Epoch 48/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4609 - acc: 0.8041 - val_loss: 0.4624 - val_acc: 0.8059\n",
      "Epoch 49/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4609 - acc: 0.8035 - val_loss: 0.4624 - val_acc: 0.8059\n",
      "Epoch 50/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4609 - acc: 0.8038 - val_loss: 0.4624 - val_acc: 0.8059\n",
      "Epoch 51/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4609 - acc: 0.8037 - val_loss: 0.4623 - val_acc: 0.8059\n",
      "Epoch 52/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4608 - acc: 0.8039 - val_loss: 0.4623 - val_acc: 0.8063\n",
      "Epoch 53/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4608 - acc: 0.8039 - val_loss: 0.4623 - val_acc: 0.8059\n",
      "Epoch 54/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4608 - acc: 0.8033 - val_loss: 0.4623 - val_acc: 0.8059\n",
      "Epoch 55/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4608 - acc: 0.8037 - val_loss: 0.4623 - val_acc: 0.8059\n",
      "Epoch 56/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4608 - acc: 0.8037 - val_loss: 0.4623 - val_acc: 0.8059\n",
      "Epoch 57/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4607 - acc: 0.8037 - val_loss: 0.4622 - val_acc: 0.8059\n",
      "Epoch 58/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4607 - acc: 0.8037 - val_loss: 0.4622 - val_acc: 0.8059\n",
      "Epoch 59/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4607 - acc: 0.8037 - val_loss: 0.4622 - val_acc: 0.8059\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4607 - acc: 0.8039 - val_loss: 0.4622 - val_acc: 0.8059\n",
      "Epoch 61/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4606 - acc: 0.8038 - val_loss: 0.4622 - val_acc: 0.8059\n",
      "Epoch 62/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4606 - acc: 0.8045 - val_loss: 0.4622 - val_acc: 0.8059\n",
      "Epoch 63/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4606 - acc: 0.8042 - val_loss: 0.4621 - val_acc: 0.8059\n",
      "Epoch 64/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4606 - acc: 0.8043 - val_loss: 0.4621 - val_acc: 0.8059\n",
      "Epoch 65/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4605 - acc: 0.8039 - val_loss: 0.4621 - val_acc: 0.8055\n",
      "Epoch 66/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4605 - acc: 0.8042 - val_loss: 0.4621 - val_acc: 0.8043\n",
      "Epoch 67/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4605 - acc: 0.8045 - val_loss: 0.4621 - val_acc: 0.8043\n",
      "Epoch 68/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4605 - acc: 0.8039 - val_loss: 0.4621 - val_acc: 0.8043\n",
      "Epoch 69/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4604 - acc: 0.8042 - val_loss: 0.4620 - val_acc: 0.8043\n",
      "Epoch 70/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4604 - acc: 0.8045 - val_loss: 0.4620 - val_acc: 0.8043\n",
      "Epoch 71/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4604 - acc: 0.8042 - val_loss: 0.4620 - val_acc: 0.8043\n",
      "Epoch 72/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4604 - acc: 0.8041 - val_loss: 0.4620 - val_acc: 0.8055\n",
      "Epoch 73/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4604 - acc: 0.8043 - val_loss: 0.4620 - val_acc: 0.8051\n",
      "Epoch 74/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4603 - acc: 0.8042 - val_loss: 0.4620 - val_acc: 0.8055\n",
      "Epoch 75/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4603 - acc: 0.8042 - val_loss: 0.4619 - val_acc: 0.8051\n",
      "Epoch 76/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4603 - acc: 0.8041 - val_loss: 0.4619 - val_acc: 0.8047\n",
      "Epoch 77/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4602 - acc: 0.8043 - val_loss: 0.4619 - val_acc: 0.8043\n",
      "Epoch 78/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4602 - acc: 0.8043 - val_loss: 0.4619 - val_acc: 0.8039\n",
      "Epoch 79/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4602 - acc: 0.8045 - val_loss: 0.4618 - val_acc: 0.8039\n",
      "Epoch 80/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4602 - acc: 0.8043 - val_loss: 0.4618 - val_acc: 0.8039\n",
      "Epoch 81/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4601 - acc: 0.8042 - val_loss: 0.4618 - val_acc: 0.8039\n",
      "Epoch 82/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4601 - acc: 0.8043 - val_loss: 0.4618 - val_acc: 0.8039\n",
      "Epoch 83/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4601 - acc: 0.8042 - val_loss: 0.4618 - val_acc: 0.8039\n",
      "Epoch 84/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4601 - acc: 0.8045 - val_loss: 0.4618 - val_acc: 0.8039\n",
      "Epoch 85/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4600 - acc: 0.8046 - val_loss: 0.4617 - val_acc: 0.8039\n",
      "Epoch 86/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4600 - acc: 0.8047 - val_loss: 0.4617 - val_acc: 0.8039\n",
      "Epoch 87/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4600 - acc: 0.8041 - val_loss: 0.4617 - val_acc: 0.8039\n",
      "Epoch 88/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4600 - acc: 0.8049 - val_loss: 0.4617 - val_acc: 0.8039\n",
      "Epoch 89/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4599 - acc: 0.8041 - val_loss: 0.4617 - val_acc: 0.8039\n",
      "Epoch 90/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4599 - acc: 0.8046 - val_loss: 0.4616 - val_acc: 0.8039\n",
      "Epoch 91/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4599 - acc: 0.8049 - val_loss: 0.4616 - val_acc: 0.8039\n",
      "Epoch 92/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4599 - acc: 0.8049 - val_loss: 0.4616 - val_acc: 0.8039\n",
      "Epoch 93/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4599 - acc: 0.8053 - val_loss: 0.4616 - val_acc: 0.8039\n",
      "Epoch 94/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4598 - acc: 0.8045 - val_loss: 0.4616 - val_acc: 0.8039\n",
      "Epoch 95/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4598 - acc: 0.8049 - val_loss: 0.4616 - val_acc: 0.8039\n",
      "Epoch 96/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4598 - acc: 0.8051 - val_loss: 0.4615 - val_acc: 0.8039\n",
      "Epoch 97/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4598 - acc: 0.8049 - val_loss: 0.4615 - val_acc: 0.8039\n",
      "Epoch 98/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4597 - acc: 0.8053 - val_loss: 0.4615 - val_acc: 0.8039\n",
      "Epoch 99/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4597 - acc: 0.8050 - val_loss: 0.4615 - val_acc: 0.8039\n",
      "Epoch 100/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4597 - acc: 0.8053 - val_loss: 0.4615 - val_acc: 0.8039\n",
      "Epoch 101/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4597 - acc: 0.8051 - val_loss: 0.4615 - val_acc: 0.8039\n",
      "Epoch 102/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4597 - acc: 0.8054 - val_loss: 0.4614 - val_acc: 0.8039\n",
      "Epoch 103/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4596 - acc: 0.8053 - val_loss: 0.4614 - val_acc: 0.8043\n",
      "Epoch 104/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4596 - acc: 0.8053 - val_loss: 0.4614 - val_acc: 0.8043\n",
      "Epoch 105/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4596 - acc: 0.8055 - val_loss: 0.4614 - val_acc: 0.8039\n",
      "Epoch 106/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4595 - acc: 0.8058 - val_loss: 0.4613 - val_acc: 0.8039\n",
      "Epoch 107/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4595 - acc: 0.8050 - val_loss: 0.4613 - val_acc: 0.8043\n",
      "Epoch 108/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4595 - acc: 0.8054 - val_loss: 0.4613 - val_acc: 0.8043\n",
      "Epoch 109/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4595 - acc: 0.8051 - val_loss: 0.4613 - val_acc: 0.8043\n",
      "Epoch 110/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4594 - acc: 0.8054 - val_loss: 0.4613 - val_acc: 0.8043\n",
      "Epoch 111/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4594 - acc: 0.8054 - val_loss: 0.4612 - val_acc: 0.8043\n",
      "Epoch 112/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4594 - acc: 0.8054 - val_loss: 0.4612 - val_acc: 0.8043\n",
      "Epoch 113/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4593 - acc: 0.8054 - val_loss: 0.4612 - val_acc: 0.8043\n",
      "Epoch 114/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4593 - acc: 0.8055 - val_loss: 0.4612 - val_acc: 0.8043\n",
      "Epoch 115/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4593 - acc: 0.8054 - val_loss: 0.4612 - val_acc: 0.8039\n",
      "Epoch 116/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4593 - acc: 0.8053 - val_loss: 0.4611 - val_acc: 0.8039\n",
      "Epoch 117/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4593 - acc: 0.8054 - val_loss: 0.4611 - val_acc: 0.8039\n",
      "Epoch 118/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4592 - acc: 0.8054 - val_loss: 0.4611 - val_acc: 0.8039\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4592 - acc: 0.8054 - val_loss: 0.4611 - val_acc: 0.8043\n",
      "Epoch 120/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4592 - acc: 0.8053 - val_loss: 0.4611 - val_acc: 0.8043\n",
      "Epoch 121/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4591 - acc: 0.8054 - val_loss: 0.4610 - val_acc: 0.8047\n",
      "Epoch 122/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4591 - acc: 0.8051 - val_loss: 0.4610 - val_acc: 0.8047\n",
      "Epoch 123/1000\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4591 - acc: 0.8053 - val_loss: 0.4610 - val_acc: 0.8047\n",
      "Epoch 124/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4591 - acc: 0.8054 - val_loss: 0.4610 - val_acc: 0.8047\n",
      "Epoch 125/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4590 - acc: 0.8054 - val_loss: 0.4610 - val_acc: 0.8047\n",
      "Epoch 126/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4590 - acc: 0.8054 - val_loss: 0.4610 - val_acc: 0.8047\n",
      "Epoch 127/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4590 - acc: 0.8055 - val_loss: 0.4609 - val_acc: 0.8047\n",
      "Epoch 128/1000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4589 - acc: 0.8059 - val_loss: 0.4609 - val_acc: 0.8047\n",
      "Epoch 129/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4589 - acc: 0.8051 - val_loss: 0.4609 - val_acc: 0.8047\n",
      "Epoch 130/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4589 - acc: 0.8053 - val_loss: 0.4609 - val_acc: 0.8051\n",
      "Epoch 131/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4589 - acc: 0.8054 - val_loss: 0.4608 - val_acc: 0.8051\n",
      "Epoch 132/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4589 - acc: 0.8051 - val_loss: 0.4608 - val_acc: 0.8051\n",
      "Epoch 133/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4588 - acc: 0.8053 - val_loss: 0.4608 - val_acc: 0.8047\n",
      "Epoch 134/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4588 - acc: 0.8055 - val_loss: 0.4608 - val_acc: 0.8047\n",
      "Epoch 135/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4588 - acc: 0.8055 - val_loss: 0.4608 - val_acc: 0.8047\n",
      "Epoch 136/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4588 - acc: 0.8054 - val_loss: 0.4607 - val_acc: 0.8047\n",
      "Epoch 137/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4587 - acc: 0.8051 - val_loss: 0.4607 - val_acc: 0.8047\n",
      "Epoch 138/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4587 - acc: 0.8053 - val_loss: 0.4607 - val_acc: 0.8047\n",
      "Epoch 139/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4587 - acc: 0.8054 - val_loss: 0.4607 - val_acc: 0.8047\n",
      "Epoch 140/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4587 - acc: 0.8051 - val_loss: 0.4606 - val_acc: 0.8051\n",
      "Epoch 141/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4586 - acc: 0.8053 - val_loss: 0.4606 - val_acc: 0.8055\n",
      "Epoch 142/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4586 - acc: 0.8057 - val_loss: 0.4606 - val_acc: 0.8051\n",
      "Epoch 143/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4586 - acc: 0.8051 - val_loss: 0.4606 - val_acc: 0.8051\n",
      "Epoch 144/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4586 - acc: 0.8054 - val_loss: 0.4605 - val_acc: 0.8047\n",
      "Epoch 145/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4585 - acc: 0.8055 - val_loss: 0.4605 - val_acc: 0.8047\n",
      "Epoch 146/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4585 - acc: 0.8053 - val_loss: 0.4605 - val_acc: 0.8047\n",
      "Epoch 147/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4585 - acc: 0.8057 - val_loss: 0.4605 - val_acc: 0.8047\n",
      "Epoch 148/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4585 - acc: 0.8055 - val_loss: 0.4604 - val_acc: 0.8051\n",
      "Epoch 149/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4584 - acc: 0.8055 - val_loss: 0.4604 - val_acc: 0.8051\n",
      "Epoch 150/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4584 - acc: 0.8057 - val_loss: 0.4604 - val_acc: 0.8051\n",
      "Epoch 151/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4584 - acc: 0.8059 - val_loss: 0.4604 - val_acc: 0.8047\n",
      "Epoch 152/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4583 - acc: 0.8058 - val_loss: 0.4604 - val_acc: 0.8051\n",
      "Epoch 153/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4583 - acc: 0.8057 - val_loss: 0.4604 - val_acc: 0.8051\n",
      "Epoch 154/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4583 - acc: 0.8054 - val_loss: 0.4603 - val_acc: 0.8059\n",
      "Epoch 155/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4583 - acc: 0.8055 - val_loss: 0.4603 - val_acc: 0.8059\n",
      "Epoch 156/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4583 - acc: 0.8058 - val_loss: 0.4603 - val_acc: 0.8059\n",
      "Epoch 157/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4582 - acc: 0.8057 - val_loss: 0.4603 - val_acc: 0.8059\n",
      "Epoch 158/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4582 - acc: 0.8058 - val_loss: 0.4603 - val_acc: 0.8051\n",
      "Epoch 159/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4582 - acc: 0.8057 - val_loss: 0.4602 - val_acc: 0.8047\n",
      "Epoch 160/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4582 - acc: 0.8054 - val_loss: 0.4602 - val_acc: 0.8059\n",
      "Epoch 161/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4581 - acc: 0.8057 - val_loss: 0.4602 - val_acc: 0.8055\n",
      "Epoch 162/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4581 - acc: 0.8057 - val_loss: 0.4602 - val_acc: 0.8051\n",
      "Epoch 163/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4581 - acc: 0.8057 - val_loss: 0.4602 - val_acc: 0.8051\n",
      "Epoch 164/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4581 - acc: 0.8057 - val_loss: 0.4601 - val_acc: 0.8055\n",
      "Epoch 165/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4580 - acc: 0.8057 - val_loss: 0.4601 - val_acc: 0.8059\n",
      "Epoch 166/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4580 - acc: 0.8058 - val_loss: 0.4601 - val_acc: 0.8055\n",
      "Epoch 167/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4580 - acc: 0.8055 - val_loss: 0.4601 - val_acc: 0.8055\n",
      "Epoch 168/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4580 - acc: 0.8057 - val_loss: 0.4601 - val_acc: 0.8055\n",
      "Epoch 169/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4580 - acc: 0.8057 - val_loss: 0.4601 - val_acc: 0.8059\n",
      "Epoch 170/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4579 - acc: 0.8058 - val_loss: 0.4600 - val_acc: 0.8059\n",
      "Epoch 171/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4579 - acc: 0.8057 - val_loss: 0.4600 - val_acc: 0.8059\n",
      "Epoch 172/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4579 - acc: 0.8057 - val_loss: 0.4600 - val_acc: 0.8063\n",
      "Epoch 173/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4579 - acc: 0.8058 - val_loss: 0.4600 - val_acc: 0.8063\n",
      "Epoch 174/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4579 - acc: 0.8055 - val_loss: 0.4600 - val_acc: 0.8063\n",
      "Epoch 175/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4578 - acc: 0.8057 - val_loss: 0.4600 - val_acc: 0.8063\n",
      "Epoch 176/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4578 - acc: 0.8055 - val_loss: 0.4600 - val_acc: 0.8063\n",
      "Epoch 177/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4578 - acc: 0.8058 - val_loss: 0.4600 - val_acc: 0.8063\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4578 - acc: 0.8058 - val_loss: 0.4600 - val_acc: 0.8063\n",
      "Epoch 179/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4577 - acc: 0.8058 - val_loss: 0.4600 - val_acc: 0.8063\n",
      "Epoch 180/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4577 - acc: 0.8055 - val_loss: 0.4600 - val_acc: 0.8063\n",
      "Epoch 181/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4577 - acc: 0.8058 - val_loss: 0.4599 - val_acc: 0.8063\n",
      "Epoch 182/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4577 - acc: 0.8058 - val_loss: 0.4599 - val_acc: 0.8063\n",
      "Epoch 183/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4577 - acc: 0.8053 - val_loss: 0.4599 - val_acc: 0.8059\n",
      "Epoch 184/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4576 - acc: 0.8055 - val_loss: 0.4599 - val_acc: 0.8059\n",
      "Epoch 185/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4576 - acc: 0.8058 - val_loss: 0.4599 - val_acc: 0.8059\n",
      "Epoch 186/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4576 - acc: 0.8055 - val_loss: 0.4599 - val_acc: 0.8063\n",
      "Epoch 187/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4576 - acc: 0.8059 - val_loss: 0.4598 - val_acc: 0.8063\n",
      "Epoch 188/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4576 - acc: 0.8059 - val_loss: 0.4598 - val_acc: 0.8067\n",
      "Epoch 189/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4575 - acc: 0.8058 - val_loss: 0.4598 - val_acc: 0.8063\n",
      "Epoch 190/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4575 - acc: 0.8058 - val_loss: 0.4598 - val_acc: 0.8059\n",
      "Epoch 191/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4575 - acc: 0.8057 - val_loss: 0.4598 - val_acc: 0.8059\n",
      "Epoch 192/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4575 - acc: 0.8057 - val_loss: 0.4598 - val_acc: 0.8059\n",
      "Epoch 193/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4575 - acc: 0.8059 - val_loss: 0.4598 - val_acc: 0.8067\n",
      "Epoch 194/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4575 - acc: 0.8058 - val_loss: 0.4597 - val_acc: 0.8063\n",
      "Epoch 195/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4574 - acc: 0.8058 - val_loss: 0.4597 - val_acc: 0.8063\n",
      "Epoch 196/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4574 - acc: 0.8057 - val_loss: 0.4597 - val_acc: 0.8063\n",
      "Epoch 197/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4574 - acc: 0.8054 - val_loss: 0.4597 - val_acc: 0.8063\n",
      "Epoch 198/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4574 - acc: 0.8057 - val_loss: 0.4597 - val_acc: 0.8067\n",
      "Epoch 199/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4573 - acc: 0.8059 - val_loss: 0.4597 - val_acc: 0.8067\n",
      "Epoch 200/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4573 - acc: 0.8057 - val_loss: 0.4597 - val_acc: 0.8063\n",
      "Epoch 201/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4573 - acc: 0.8057 - val_loss: 0.4597 - val_acc: 0.8063\n",
      "Epoch 202/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4573 - acc: 0.8058 - val_loss: 0.4596 - val_acc: 0.8063\n",
      "Epoch 203/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4573 - acc: 0.8057 - val_loss: 0.4596 - val_acc: 0.8063\n",
      "Epoch 204/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4573 - acc: 0.8058 - val_loss: 0.4596 - val_acc: 0.8071\n",
      "Epoch 205/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4572 - acc: 0.8057 - val_loss: 0.4596 - val_acc: 0.8071\n",
      "Epoch 206/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4572 - acc: 0.8057 - val_loss: 0.4596 - val_acc: 0.8067\n",
      "Epoch 207/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4572 - acc: 0.8057 - val_loss: 0.4596 - val_acc: 0.8067\n",
      "Epoch 208/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4572 - acc: 0.8055 - val_loss: 0.4595 - val_acc: 0.8071\n",
      "Epoch 209/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4572 - acc: 0.8054 - val_loss: 0.4595 - val_acc: 0.8071\n",
      "Epoch 210/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4571 - acc: 0.8055 - val_loss: 0.4595 - val_acc: 0.8067\n",
      "Epoch 211/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4571 - acc: 0.8059 - val_loss: 0.4595 - val_acc: 0.8067\n",
      "Epoch 212/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4571 - acc: 0.8055 - val_loss: 0.4595 - val_acc: 0.8067\n",
      "Epoch 213/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4571 - acc: 0.8055 - val_loss: 0.4595 - val_acc: 0.8067\n",
      "Epoch 214/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4571 - acc: 0.8055 - val_loss: 0.4595 - val_acc: 0.8067\n",
      "Epoch 215/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4570 - acc: 0.8054 - val_loss: 0.4595 - val_acc: 0.8067\n",
      "Epoch 216/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4570 - acc: 0.8054 - val_loss: 0.4594 - val_acc: 0.8067\n",
      "Epoch 217/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4570 - acc: 0.8053 - val_loss: 0.4594 - val_acc: 0.8067\n",
      "Epoch 218/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4570 - acc: 0.8053 - val_loss: 0.4594 - val_acc: 0.8071\n",
      "Epoch 219/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4570 - acc: 0.8057 - val_loss: 0.4594 - val_acc: 0.8071\n",
      "Epoch 220/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4569 - acc: 0.8051 - val_loss: 0.4594 - val_acc: 0.8071\n",
      "Epoch 221/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4569 - acc: 0.8055 - val_loss: 0.4594 - val_acc: 0.8075\n",
      "Epoch 222/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4569 - acc: 0.8055 - val_loss: 0.4594 - val_acc: 0.8075\n",
      "Epoch 223/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4569 - acc: 0.8054 - val_loss: 0.4593 - val_acc: 0.8071\n",
      "Epoch 224/1000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4568 - acc: 0.8055 - val_loss: 0.4593 - val_acc: 0.8071\n",
      "Epoch 225/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4568 - acc: 0.8057 - val_loss: 0.4593 - val_acc: 0.8071\n",
      "Epoch 226/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4568 - acc: 0.8057 - val_loss: 0.4593 - val_acc: 0.8071\n",
      "Epoch 227/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4568 - acc: 0.8057 - val_loss: 0.4592 - val_acc: 0.8071\n",
      "Epoch 228/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4568 - acc: 0.8057 - val_loss: 0.4592 - val_acc: 0.8067\n",
      "Epoch 229/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4567 - acc: 0.8058 - val_loss: 0.4592 - val_acc: 0.8067\n",
      "Epoch 230/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4567 - acc: 0.8058 - val_loss: 0.4592 - val_acc: 0.8067\n",
      "Epoch 231/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4567 - acc: 0.8061 - val_loss: 0.4592 - val_acc: 0.8071\n",
      "Epoch 232/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4567 - acc: 0.8058 - val_loss: 0.4592 - val_acc: 0.8071\n",
      "Epoch 233/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4566 - acc: 0.8054 - val_loss: 0.4591 - val_acc: 0.8067\n",
      "Epoch 234/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4566 - acc: 0.8059 - val_loss: 0.4591 - val_acc: 0.8067\n",
      "Epoch 235/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4566 - acc: 0.8059 - val_loss: 0.4591 - val_acc: 0.8063\n",
      "Epoch 236/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4566 - acc: 0.8059 - val_loss: 0.4591 - val_acc: 0.8071\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4566 - acc: 0.8059 - val_loss: 0.4591 - val_acc: 0.8067\n",
      "Epoch 238/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4565 - acc: 0.8058 - val_loss: 0.4591 - val_acc: 0.8067\n",
      "Epoch 239/1000\n",
      "7477/7477 [==============================] - 0s 43us/step - loss: 0.4565 - acc: 0.8058 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 240/1000\n",
      "7477/7477 [==============================] - 0s 46us/step - loss: 0.4565 - acc: 0.8058 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 241/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4565 - acc: 0.8057 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 242/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4565 - acc: 0.8059 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 243/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4564 - acc: 0.8059 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 244/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4564 - acc: 0.8055 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 245/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4564 - acc: 0.8062 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 246/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4564 - acc: 0.8059 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 247/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4563 - acc: 0.8059 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 248/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4563 - acc: 0.8065 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 249/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4563 - acc: 0.8063 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 250/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4563 - acc: 0.8063 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 251/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4563 - acc: 0.8062 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 252/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4562 - acc: 0.8063 - val_loss: 0.4588 - val_acc: 0.8067\n",
      "Epoch 253/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4562 - acc: 0.8065 - val_loss: 0.4588 - val_acc: 0.8067\n",
      "Epoch 254/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4562 - acc: 0.8062 - val_loss: 0.4588 - val_acc: 0.8067\n",
      "Epoch 255/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4562 - acc: 0.8062 - val_loss: 0.4588 - val_acc: 0.8071\n",
      "Epoch 256/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4562 - acc: 0.8063 - val_loss: 0.4587 - val_acc: 0.8071\n",
      "Epoch 257/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4561 - acc: 0.8063 - val_loss: 0.4587 - val_acc: 0.8071\n",
      "Epoch 258/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4561 - acc: 0.8065 - val_loss: 0.4587 - val_acc: 0.8071\n",
      "Epoch 259/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4561 - acc: 0.8061 - val_loss: 0.4587 - val_acc: 0.8075\n",
      "Epoch 260/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4561 - acc: 0.8063 - val_loss: 0.4587 - val_acc: 0.8075\n",
      "Epoch 261/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4561 - acc: 0.8063 - val_loss: 0.4587 - val_acc: 0.8075\n",
      "Epoch 262/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4561 - acc: 0.8066 - val_loss: 0.4587 - val_acc: 0.8075\n",
      "Epoch 263/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4560 - acc: 0.8062 - val_loss: 0.4586 - val_acc: 0.8075\n",
      "Epoch 264/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4560 - acc: 0.8062 - val_loss: 0.4586 - val_acc: 0.8075\n",
      "Epoch 265/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4560 - acc: 0.8063 - val_loss: 0.4586 - val_acc: 0.8079\n",
      "Epoch 266/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4560 - acc: 0.8063 - val_loss: 0.4586 - val_acc: 0.8075\n",
      "Epoch 267/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4560 - acc: 0.8061 - val_loss: 0.4586 - val_acc: 0.8079\n",
      "Epoch 268/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4559 - acc: 0.8066 - val_loss: 0.4586 - val_acc: 0.8075\n",
      "Epoch 269/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4559 - acc: 0.8065 - val_loss: 0.4586 - val_acc: 0.8079\n",
      "Epoch 270/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4559 - acc: 0.8065 - val_loss: 0.4586 - val_acc: 0.8079\n",
      "Epoch 271/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4559 - acc: 0.8067 - val_loss: 0.4586 - val_acc: 0.8079\n",
      "Epoch 272/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4559 - acc: 0.8066 - val_loss: 0.4586 - val_acc: 0.8079\n",
      "Epoch 273/1000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4559 - acc: 0.8065 - val_loss: 0.4586 - val_acc: 0.8079\n",
      "Epoch 274/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4559 - acc: 0.8066 - val_loss: 0.4585 - val_acc: 0.8079\n",
      "Epoch 275/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4558 - acc: 0.8062 - val_loss: 0.4585 - val_acc: 0.8079\n",
      "Epoch 276/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4558 - acc: 0.8065 - val_loss: 0.4585 - val_acc: 0.8079\n",
      "Epoch 277/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4558 - acc: 0.8067 - val_loss: 0.4585 - val_acc: 0.8079\n",
      "Epoch 278/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4558 - acc: 0.8066 - val_loss: 0.4585 - val_acc: 0.8079\n",
      "Epoch 279/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4558 - acc: 0.8067 - val_loss: 0.4585 - val_acc: 0.8079\n",
      "Epoch 280/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4558 - acc: 0.8066 - val_loss: 0.4585 - val_acc: 0.8079\n",
      "Epoch 281/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4558 - acc: 0.8067 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 282/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4557 - acc: 0.8065 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 283/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4557 - acc: 0.8067 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 284/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4557 - acc: 0.8066 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 285/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4557 - acc: 0.8066 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 286/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4557 - acc: 0.8069 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 287/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4557 - acc: 0.8062 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 288/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4557 - acc: 0.8065 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 289/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4557 - acc: 0.8066 - val_loss: 0.4583 - val_acc: 0.8079\n",
      "Epoch 290/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4556 - acc: 0.8066 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 291/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4556 - acc: 0.8067 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 292/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4556 - acc: 0.8066 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 293/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4556 - acc: 0.8066 - val_loss: 0.4583 - val_acc: 0.8079\n",
      "Epoch 294/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4556 - acc: 0.8067 - val_loss: 0.4584 - val_acc: 0.8079\n",
      "Epoch 295/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4556 - acc: 0.8065 - val_loss: 0.4583 - val_acc: 0.8079\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4556 - acc: 0.8065 - val_loss: 0.4583 - val_acc: 0.8079\n",
      "Epoch 297/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4555 - acc: 0.8065 - val_loss: 0.4583 - val_acc: 0.8079\n",
      "Epoch 298/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4555 - acc: 0.8065 - val_loss: 0.4583 - val_acc: 0.8079\n",
      "Epoch 299/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4555 - acc: 0.8065 - val_loss: 0.4583 - val_acc: 0.8079\n",
      "Epoch 300/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4555 - acc: 0.8063 - val_loss: 0.4583 - val_acc: 0.8079\n",
      "Epoch 301/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4555 - acc: 0.8066 - val_loss: 0.4583 - val_acc: 0.8079\n",
      "Epoch 302/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4555 - acc: 0.8065 - val_loss: 0.4583 - val_acc: 0.8083\n",
      "Epoch 303/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4555 - acc: 0.8063 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 304/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4555 - acc: 0.8065 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 305/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4554 - acc: 0.8065 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 306/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4554 - acc: 0.8065 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 307/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4554 - acc: 0.8063 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 308/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4554 - acc: 0.8065 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 309/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4554 - acc: 0.8065 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 310/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4554 - acc: 0.8066 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 311/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4554 - acc: 0.8063 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 312/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4553 - acc: 0.8059 - val_loss: 0.4582 - val_acc: 0.8083\n",
      "Epoch 313/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4553 - acc: 0.8067 - val_loss: 0.4581 - val_acc: 0.8079\n",
      "Epoch 314/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4553 - acc: 0.8063 - val_loss: 0.4581 - val_acc: 0.8087\n",
      "Epoch 315/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4553 - acc: 0.8066 - val_loss: 0.4582 - val_acc: 0.8087\n",
      "Epoch 316/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4553 - acc: 0.8063 - val_loss: 0.4581 - val_acc: 0.8083\n",
      "Epoch 317/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4553 - acc: 0.8065 - val_loss: 0.4581 - val_acc: 0.8083\n",
      "Epoch 318/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4553 - acc: 0.8067 - val_loss: 0.4581 - val_acc: 0.8083\n",
      "Epoch 319/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4553 - acc: 0.8062 - val_loss: 0.4581 - val_acc: 0.8083\n",
      "Epoch 320/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4552 - acc: 0.8067 - val_loss: 0.4581 - val_acc: 0.8079\n",
      "Epoch 321/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4552 - acc: 0.8063 - val_loss: 0.4581 - val_acc: 0.8079\n",
      "Epoch 322/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4552 - acc: 0.8062 - val_loss: 0.4580 - val_acc: 0.8083\n",
      "Epoch 323/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4552 - acc: 0.8065 - val_loss: 0.4581 - val_acc: 0.8079\n",
      "Epoch 324/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4552 - acc: 0.8065 - val_loss: 0.4580 - val_acc: 0.8087\n",
      "Epoch 325/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4552 - acc: 0.8065 - val_loss: 0.4580 - val_acc: 0.8087\n",
      "Epoch 326/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4552 - acc: 0.8065 - val_loss: 0.4580 - val_acc: 0.8075\n",
      "Epoch 327/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4551 - acc: 0.8065 - val_loss: 0.4580 - val_acc: 0.8079\n",
      "Epoch 328/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4551 - acc: 0.8066 - val_loss: 0.4580 - val_acc: 0.8075\n",
      "Epoch 329/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4551 - acc: 0.8062 - val_loss: 0.4580 - val_acc: 0.8079\n",
      "Epoch 330/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4551 - acc: 0.8063 - val_loss: 0.4580 - val_acc: 0.8079\n",
      "Epoch 331/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4551 - acc: 0.8070 - val_loss: 0.4580 - val_acc: 0.8079\n",
      "Epoch 332/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4551 - acc: 0.8065 - val_loss: 0.4579 - val_acc: 0.8079\n",
      "Epoch 333/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4551 - acc: 0.8069 - val_loss: 0.4579 - val_acc: 0.8079\n",
      "Epoch 334/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4551 - acc: 0.8066 - val_loss: 0.4579 - val_acc: 0.8075\n",
      "Epoch 335/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4550 - acc: 0.8071 - val_loss: 0.4579 - val_acc: 0.8079\n",
      "Epoch 336/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4550 - acc: 0.8066 - val_loss: 0.4579 - val_acc: 0.8079\n",
      "Epoch 337/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4550 - acc: 0.8063 - val_loss: 0.4579 - val_acc: 0.8075\n",
      "Epoch 338/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4550 - acc: 0.8067 - val_loss: 0.4579 - val_acc: 0.8075\n",
      "Epoch 339/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4550 - acc: 0.8070 - val_loss: 0.4579 - val_acc: 0.8083\n",
      "Epoch 340/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4550 - acc: 0.8069 - val_loss: 0.4579 - val_acc: 0.8079\n",
      "Epoch 341/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4550 - acc: 0.8067 - val_loss: 0.4579 - val_acc: 0.8079\n",
      "Epoch 342/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4550 - acc: 0.8067 - val_loss: 0.4578 - val_acc: 0.8075\n",
      "Epoch 343/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4550 - acc: 0.8063 - val_loss: 0.4578 - val_acc: 0.8075\n",
      "Epoch 344/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4549 - acc: 0.8065 - val_loss: 0.4579 - val_acc: 0.8075\n",
      "Epoch 345/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4549 - acc: 0.8066 - val_loss: 0.4578 - val_acc: 0.8071\n",
      "Epoch 346/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4549 - acc: 0.8070 - val_loss: 0.4578 - val_acc: 0.8075\n",
      "Epoch 347/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4549 - acc: 0.8066 - val_loss: 0.4578 - val_acc: 0.8075\n",
      "Epoch 348/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4549 - acc: 0.8066 - val_loss: 0.4578 - val_acc: 0.8075\n",
      "Epoch 349/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4549 - acc: 0.8067 - val_loss: 0.4577 - val_acc: 0.8071\n",
      "Epoch 350/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4549 - acc: 0.8067 - val_loss: 0.4577 - val_acc: 0.8071\n",
      "Epoch 351/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4549 - acc: 0.8067 - val_loss: 0.4578 - val_acc: 0.8071\n",
      "Epoch 352/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4548 - acc: 0.8066 - val_loss: 0.4577 - val_acc: 0.8067\n",
      "Epoch 353/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4548 - acc: 0.8070 - val_loss: 0.4577 - val_acc: 0.8067\n",
      "Epoch 354/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4548 - acc: 0.8065 - val_loss: 0.4577 - val_acc: 0.8067\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4548 - acc: 0.8065 - val_loss: 0.4577 - val_acc: 0.8067\n",
      "Epoch 356/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4548 - acc: 0.8069 - val_loss: 0.4577 - val_acc: 0.8063\n",
      "Epoch 357/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4548 - acc: 0.8069 - val_loss: 0.4577 - val_acc: 0.8063\n",
      "Epoch 358/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4548 - acc: 0.8069 - val_loss: 0.4576 - val_acc: 0.8067\n",
      "Epoch 359/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4548 - acc: 0.8067 - val_loss: 0.4576 - val_acc: 0.8063\n",
      "Epoch 360/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4548 - acc: 0.8067 - val_loss: 0.4576 - val_acc: 0.8063\n",
      "Epoch 361/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4548 - acc: 0.8067 - val_loss: 0.4576 - val_acc: 0.8063\n",
      "Epoch 362/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4547 - acc: 0.8070 - val_loss: 0.4576 - val_acc: 0.8059\n",
      "Epoch 363/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4547 - acc: 0.8070 - val_loss: 0.4576 - val_acc: 0.8059\n",
      "Epoch 364/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4547 - acc: 0.8067 - val_loss: 0.4576 - val_acc: 0.8059\n",
      "Epoch 365/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4547 - acc: 0.8067 - val_loss: 0.4576 - val_acc: 0.8059\n",
      "Epoch 366/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4547 - acc: 0.8069 - val_loss: 0.4576 - val_acc: 0.8063\n",
      "Epoch 367/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4547 - acc: 0.8069 - val_loss: 0.4575 - val_acc: 0.8063\n",
      "Epoch 368/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4547 - acc: 0.8065 - val_loss: 0.4575 - val_acc: 0.8063\n",
      "Epoch 369/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4547 - acc: 0.8069 - val_loss: 0.4575 - val_acc: 0.8063\n",
      "Epoch 370/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4547 - acc: 0.8066 - val_loss: 0.4575 - val_acc: 0.8059\n",
      "Epoch 371/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4547 - acc: 0.8065 - val_loss: 0.4575 - val_acc: 0.8059\n",
      "Epoch 372/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4547 - acc: 0.8067 - val_loss: 0.4575 - val_acc: 0.8059\n",
      "Epoch 373/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4546 - acc: 0.8067 - val_loss: 0.4575 - val_acc: 0.8059\n",
      "Epoch 374/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4546 - acc: 0.8065 - val_loss: 0.4575 - val_acc: 0.8059\n",
      "Epoch 375/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4546 - acc: 0.8070 - val_loss: 0.4575 - val_acc: 0.8059\n",
      "Epoch 376/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4546 - acc: 0.8067 - val_loss: 0.4574 - val_acc: 0.8059\n",
      "Epoch 377/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4546 - acc: 0.8070 - val_loss: 0.4574 - val_acc: 0.8055\n",
      "Epoch 378/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4546 - acc: 0.8067 - val_loss: 0.4574 - val_acc: 0.8051\n",
      "Epoch 379/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4546 - acc: 0.8070 - val_loss: 0.4574 - val_acc: 0.8051\n",
      "Epoch 380/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4545 - acc: 0.8066 - val_loss: 0.4573 - val_acc: 0.8051\n",
      "Epoch 381/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4546 - acc: 0.8069 - val_loss: 0.4574 - val_acc: 0.8051\n",
      "Epoch 382/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4546 - acc: 0.8070 - val_loss: 0.4574 - val_acc: 0.8051\n",
      "Epoch 383/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4546 - acc: 0.8065 - val_loss: 0.4574 - val_acc: 0.8051\n",
      "Epoch 384/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4545 - acc: 0.8071 - val_loss: 0.4573 - val_acc: 0.8047\n",
      "Epoch 385/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4545 - acc: 0.8071 - val_loss: 0.4573 - val_acc: 0.8051\n",
      "Epoch 386/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4545 - acc: 0.8069 - val_loss: 0.4574 - val_acc: 0.8051\n",
      "Epoch 387/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4545 - acc: 0.8070 - val_loss: 0.4573 - val_acc: 0.8047\n",
      "Epoch 388/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4545 - acc: 0.8069 - val_loss: 0.4573 - val_acc: 0.8043\n",
      "Epoch 389/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4545 - acc: 0.8070 - val_loss: 0.4573 - val_acc: 0.8043\n",
      "Epoch 390/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4545 - acc: 0.8067 - val_loss: 0.4573 - val_acc: 0.8043\n",
      "Epoch 391/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4545 - acc: 0.8067 - val_loss: 0.4573 - val_acc: 0.8043\n",
      "Epoch 392/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4545 - acc: 0.8070 - val_loss: 0.4572 - val_acc: 0.8043\n",
      "Epoch 393/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4545 - acc: 0.8067 - val_loss: 0.4572 - val_acc: 0.8047\n",
      "Epoch 394/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4545 - acc: 0.8069 - val_loss: 0.4572 - val_acc: 0.8043\n",
      "Epoch 395/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4544 - acc: 0.8070 - val_loss: 0.4572 - val_acc: 0.8039\n",
      "Epoch 396/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4544 - acc: 0.8069 - val_loss: 0.4572 - val_acc: 0.8039\n",
      "Epoch 397/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4544 - acc: 0.8074 - val_loss: 0.4572 - val_acc: 0.8039\n",
      "Epoch 398/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4544 - acc: 0.8070 - val_loss: 0.4572 - val_acc: 0.8034\n",
      "Epoch 399/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4544 - acc: 0.8069 - val_loss: 0.4572 - val_acc: 0.8030\n",
      "Epoch 400/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4544 - acc: 0.8073 - val_loss: 0.4572 - val_acc: 0.8034\n",
      "Epoch 401/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4544 - acc: 0.8070 - val_loss: 0.4572 - val_acc: 0.8034\n",
      "Epoch 402/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4544 - acc: 0.8066 - val_loss: 0.4572 - val_acc: 0.8039\n",
      "Epoch 403/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4544 - acc: 0.8067 - val_loss: 0.4572 - val_acc: 0.8043\n",
      "Epoch 404/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4544 - acc: 0.8075 - val_loss: 0.4572 - val_acc: 0.8034\n",
      "Epoch 405/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4543 - acc: 0.8071 - val_loss: 0.4572 - val_acc: 0.8034\n",
      "Epoch 406/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4543 - acc: 0.8069 - val_loss: 0.4571 - val_acc: 0.8034\n",
      "Epoch 407/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4543 - acc: 0.8070 - val_loss: 0.4571 - val_acc: 0.8034\n",
      "Epoch 408/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4543 - acc: 0.8067 - val_loss: 0.4571 - val_acc: 0.8034\n",
      "Epoch 409/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4543 - acc: 0.8065 - val_loss: 0.4571 - val_acc: 0.8030\n",
      "Epoch 410/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4543 - acc: 0.8070 - val_loss: 0.4571 - val_acc: 0.8034\n",
      "Epoch 411/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4543 - acc: 0.8066 - val_loss: 0.4571 - val_acc: 0.8039\n",
      "Epoch 412/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4543 - acc: 0.8069 - val_loss: 0.4571 - val_acc: 0.8039\n",
      "Epoch 413/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4543 - acc: 0.8071 - val_loss: 0.4571 - val_acc: 0.8039\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4543 - acc: 0.8066 - val_loss: 0.4571 - val_acc: 0.8030\n",
      "Epoch 415/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4543 - acc: 0.8070 - val_loss: 0.4571 - val_acc: 0.8030\n",
      "Epoch 416/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4543 - acc: 0.8070 - val_loss: 0.4571 - val_acc: 0.8034\n",
      "Epoch 417/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4543 - acc: 0.8070 - val_loss: 0.4570 - val_acc: 0.8034\n",
      "Epoch 418/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4542 - acc: 0.8069 - val_loss: 0.4570 - val_acc: 0.8039\n",
      "Epoch 419/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4542 - acc: 0.8069 - val_loss: 0.4570 - val_acc: 0.8034\n",
      "Epoch 420/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4542 - acc: 0.8066 - val_loss: 0.4570 - val_acc: 0.8039\n",
      "Epoch 421/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4542 - acc: 0.8070 - val_loss: 0.4569 - val_acc: 0.8034\n",
      "Epoch 422/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4542 - acc: 0.8067 - val_loss: 0.4569 - val_acc: 0.8034\n",
      "Epoch 423/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4542 - acc: 0.8067 - val_loss: 0.4569 - val_acc: 0.8034\n",
      "Epoch 424/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4542 - acc: 0.8070 - val_loss: 0.4570 - val_acc: 0.8030\n",
      "Epoch 425/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4542 - acc: 0.8069 - val_loss: 0.4569 - val_acc: 0.8030\n",
      "Epoch 426/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4542 - acc: 0.8066 - val_loss: 0.4569 - val_acc: 0.8034\n",
      "Epoch 427/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4542 - acc: 0.8066 - val_loss: 0.4569 - val_acc: 0.8034\n",
      "Epoch 428/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4542 - acc: 0.8070 - val_loss: 0.4569 - val_acc: 0.8030\n",
      "Epoch 429/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4542 - acc: 0.8066 - val_loss: 0.4569 - val_acc: 0.8034\n",
      "Epoch 430/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4542 - acc: 0.8066 - val_loss: 0.4569 - val_acc: 0.8034\n",
      "Epoch 431/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8067 - val_loss: 0.4569 - val_acc: 0.8034\n",
      "Epoch 432/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8063 - val_loss: 0.4569 - val_acc: 0.8030\n",
      "Epoch 433/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8066 - val_loss: 0.4569 - val_acc: 0.8030\n",
      "Epoch 434/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8070 - val_loss: 0.4568 - val_acc: 0.8034\n",
      "Epoch 435/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8065 - val_loss: 0.4568 - val_acc: 0.8030\n",
      "Epoch 436/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8069 - val_loss: 0.4569 - val_acc: 0.8030\n",
      "Epoch 437/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4541 - acc: 0.8066 - val_loss: 0.4568 - val_acc: 0.8030\n",
      "Epoch 438/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8070 - val_loss: 0.4568 - val_acc: 0.8030\n",
      "Epoch 439/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8070 - val_loss: 0.4568 - val_acc: 0.8034\n",
      "Epoch 440/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8069 - val_loss: 0.4568 - val_acc: 0.8026\n",
      "Epoch 441/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8063 - val_loss: 0.4568 - val_acc: 0.8030\n",
      "Epoch 442/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8067 - val_loss: 0.4568 - val_acc: 0.8026\n",
      "Epoch 443/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4540 - acc: 0.8069 - val_loss: 0.4568 - val_acc: 0.8030\n",
      "Epoch 444/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8070 - val_loss: 0.4568 - val_acc: 0.8026\n",
      "Epoch 445/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4541 - acc: 0.8070 - val_loss: 0.4568 - val_acc: 0.8026\n",
      "Epoch 446/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4540 - acc: 0.8074 - val_loss: 0.4568 - val_acc: 0.8026\n",
      "Epoch 447/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4540 - acc: 0.8071 - val_loss: 0.4568 - val_acc: 0.8026\n",
      "Epoch 448/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4540 - acc: 0.8069 - val_loss: 0.4568 - val_acc: 0.8026\n",
      "Epoch 449/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4540 - acc: 0.8069 - val_loss: 0.4568 - val_acc: 0.8022\n",
      "Epoch 450/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4540 - acc: 0.8069 - val_loss: 0.4567 - val_acc: 0.8026\n",
      "Epoch 451/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4540 - acc: 0.8071 - val_loss: 0.4567 - val_acc: 0.8026\n",
      "Epoch 452/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4540 - acc: 0.8071 - val_loss: 0.4567 - val_acc: 0.8026\n",
      "Epoch 453/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4540 - acc: 0.8069 - val_loss: 0.4567 - val_acc: 0.8022\n",
      "Epoch 454/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4540 - acc: 0.8073 - val_loss: 0.4567 - val_acc: 0.8026\n",
      "Epoch 455/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4540 - acc: 0.8069 - val_loss: 0.4567 - val_acc: 0.8026\n",
      "Epoch 456/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4540 - acc: 0.8074 - val_loss: 0.4566 - val_acc: 0.8022\n",
      "Epoch 457/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.8073 - val_loss: 0.4567 - val_acc: 0.8026\n",
      "Epoch 458/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4539 - acc: 0.8070 - val_loss: 0.4566 - val_acc: 0.8022\n",
      "Epoch 459/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.8069 - val_loss: 0.4566 - val_acc: 0.8030\n",
      "Epoch 460/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.8070 - val_loss: 0.4566 - val_acc: 0.8026\n",
      "Epoch 461/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.8074 - val_loss: 0.4566 - val_acc: 0.8030\n",
      "Epoch 462/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4539 - acc: 0.8075 - val_loss: 0.4566 - val_acc: 0.8026\n",
      "Epoch 463/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.8073 - val_loss: 0.4566 - val_acc: 0.8030\n",
      "Epoch 464/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4539 - acc: 0.8075 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 465/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.8069 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 466/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4539 - acc: 0.8070 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 467/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.8075 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 468/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.8075 - val_loss: 0.4565 - val_acc: 0.8034\n",
      "Epoch 469/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4538 - acc: 0.8077 - val_loss: 0.4565 - val_acc: 0.8034\n",
      "Epoch 470/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4538 - acc: 0.8073 - val_loss: 0.4565 - val_acc: 0.8034\n",
      "Epoch 471/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4539 - acc: 0.8069 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 472/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4538 - acc: 0.8073 - val_loss: 0.4565 - val_acc: 0.8034\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4538 - acc: 0.8074 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 474/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4538 - acc: 0.8077 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 475/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4538 - acc: 0.8074 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 476/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4538 - acc: 0.8074 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 477/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4538 - acc: 0.8075 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 478/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4538 - acc: 0.8075 - val_loss: 0.4564 - val_acc: 0.8026\n",
      "Epoch 479/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4538 - acc: 0.8075 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 480/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4538 - acc: 0.8074 - val_loss: 0.4565 - val_acc: 0.8030\n",
      "Epoch 481/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4538 - acc: 0.8073 - val_loss: 0.4565 - val_acc: 0.8026\n",
      "Epoch 482/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4538 - acc: 0.8075 - val_loss: 0.4564 - val_acc: 0.8030\n",
      "Epoch 483/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4538 - acc: 0.8077 - val_loss: 0.4565 - val_acc: 0.8034\n",
      "Epoch 484/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8073 - val_loss: 0.4564 - val_acc: 0.8034\n",
      "Epoch 485/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8077 - val_loss: 0.4564 - val_acc: 0.8030\n",
      "Epoch 486/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4537 - acc: 0.8073 - val_loss: 0.4564 - val_acc: 0.8030\n",
      "Epoch 487/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8078 - val_loss: 0.4564 - val_acc: 0.8030\n",
      "Epoch 488/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8075 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 489/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8074 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 490/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8075 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 491/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8078 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 492/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4537 - acc: 0.8074 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 493/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8078 - val_loss: 0.4563 - val_acc: 0.8034\n",
      "Epoch 494/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4537 - acc: 0.8077 - val_loss: 0.4563 - val_acc: 0.8034\n",
      "Epoch 495/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8071 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 496/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8078 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 497/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4537 - acc: 0.8074 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 498/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8075 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 499/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4537 - acc: 0.8071 - val_loss: 0.4563 - val_acc: 0.8030\n",
      "Epoch 500/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4536 - acc: 0.8075 - val_loss: 0.4563 - val_acc: 0.8034\n",
      "Epoch 501/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4536 - acc: 0.8074 - val_loss: 0.4562 - val_acc: 0.8034\n",
      "Epoch 502/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4536 - acc: 0.8077 - val_loss: 0.4562 - val_acc: 0.8034\n",
      "Epoch 503/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4536 - acc: 0.8074 - val_loss: 0.4562 - val_acc: 0.8034\n",
      "Epoch 504/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4536 - acc: 0.8074 - val_loss: 0.4562 - val_acc: 0.8034\n",
      "Epoch 505/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4536 - acc: 0.8073 - val_loss: 0.4562 - val_acc: 0.8034\n",
      "Epoch 506/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4536 - acc: 0.8071 - val_loss: 0.4561 - val_acc: 0.8039\n",
      "Epoch 507/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4536 - acc: 0.8077 - val_loss: 0.4561 - val_acc: 0.8039\n",
      "Epoch 508/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4536 - acc: 0.8071 - val_loss: 0.4561 - val_acc: 0.8034\n",
      "Epoch 509/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4536 - acc: 0.8075 - val_loss: 0.4561 - val_acc: 0.8034\n",
      "Epoch 510/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4536 - acc: 0.8077 - val_loss: 0.4561 - val_acc: 0.8039\n",
      "Epoch 511/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4536 - acc: 0.8073 - val_loss: 0.4561 - val_acc: 0.8034\n",
      "Epoch 512/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4536 - acc: 0.8075 - val_loss: 0.4561 - val_acc: 0.8039\n",
      "Epoch 513/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4535 - acc: 0.8078 - val_loss: 0.4561 - val_acc: 0.8039\n",
      "Epoch 514/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4535 - acc: 0.8073 - val_loss: 0.4561 - val_acc: 0.8034\n",
      "Epoch 515/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4535 - acc: 0.8074 - val_loss: 0.4561 - val_acc: 0.8039\n",
      "Epoch 516/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4535 - acc: 0.8074 - val_loss: 0.4561 - val_acc: 0.8043\n",
      "Epoch 517/1000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4535 - acc: 0.8070 - val_loss: 0.4561 - val_acc: 0.8043\n",
      "Epoch 518/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4535 - acc: 0.8079 - val_loss: 0.4561 - val_acc: 0.8047\n",
      "Epoch 519/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4535 - acc: 0.8077 - val_loss: 0.4561 - val_acc: 0.8043\n",
      "Epoch 520/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4535 - acc: 0.8073 - val_loss: 0.4561 - val_acc: 0.8043\n",
      "Epoch 521/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4535 - acc: 0.8077 - val_loss: 0.4561 - val_acc: 0.8047\n",
      "Epoch 522/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4535 - acc: 0.8077 - val_loss: 0.4561 - val_acc: 0.8043\n",
      "Epoch 523/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4535 - acc: 0.8077 - val_loss: 0.4561 - val_acc: 0.8051\n",
      "Epoch 524/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4534 - acc: 0.8077 - val_loss: 0.4560 - val_acc: 0.8043\n",
      "Epoch 525/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4535 - acc: 0.8074 - val_loss: 0.4560 - val_acc: 0.8043\n",
      "Epoch 526/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4534 - acc: 0.8077 - val_loss: 0.4560 - val_acc: 0.8051\n",
      "Epoch 527/1000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4534 - acc: 0.8078 - val_loss: 0.4560 - val_acc: 0.8047\n",
      "Epoch 528/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4534 - acc: 0.8075 - val_loss: 0.4560 - val_acc: 0.8047\n",
      "Epoch 529/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4534 - acc: 0.8078 - val_loss: 0.4560 - val_acc: 0.8043\n",
      "Epoch 530/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4534 - acc: 0.8071 - val_loss: 0.4560 - val_acc: 0.8034\n",
      "Epoch 531/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4534 - acc: 0.8073 - val_loss: 0.4560 - val_acc: 0.8039\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4534 - acc: 0.8083 - val_loss: 0.4560 - val_acc: 0.8055\n",
      "Epoch 533/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8081 - val_loss: 0.4560 - val_acc: 0.8055\n",
      "Epoch 534/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4534 - acc: 0.8085 - val_loss: 0.4560 - val_acc: 0.8055\n",
      "Epoch 535/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4534 - acc: 0.8079 - val_loss: 0.4560 - val_acc: 0.8059\n",
      "Epoch 536/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4534 - acc: 0.8081 - val_loss: 0.4560 - val_acc: 0.8059\n",
      "Epoch 537/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4534 - acc: 0.8078 - val_loss: 0.4560 - val_acc: 0.8059\n",
      "Epoch 538/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4534 - acc: 0.8077 - val_loss: 0.4559 - val_acc: 0.8055\n",
      "Epoch 539/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4534 - acc: 0.8079 - val_loss: 0.4559 - val_acc: 0.8059\n",
      "Epoch 540/1000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4534 - acc: 0.8079 - val_loss: 0.4559 - val_acc: 0.8059\n",
      "Epoch 541/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4533 - acc: 0.8081 - val_loss: 0.4559 - val_acc: 0.8059\n",
      "Epoch 542/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4533 - acc: 0.8079 - val_loss: 0.4559 - val_acc: 0.8051\n",
      "Epoch 543/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4533 - acc: 0.8083 - val_loss: 0.4559 - val_acc: 0.8059\n",
      "Epoch 544/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4533 - acc: 0.8079 - val_loss: 0.4559 - val_acc: 0.8063\n",
      "Epoch 545/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8081 - val_loss: 0.4559 - val_acc: 0.8063\n",
      "Epoch 546/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4533 - acc: 0.8077 - val_loss: 0.4559 - val_acc: 0.8055\n",
      "Epoch 547/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4533 - acc: 0.8079 - val_loss: 0.4560 - val_acc: 0.8063\n",
      "Epoch 548/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4533 - acc: 0.8081 - val_loss: 0.4559 - val_acc: 0.8063\n",
      "Epoch 549/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4533 - acc: 0.8087 - val_loss: 0.4559 - val_acc: 0.8063\n",
      "Epoch 550/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4533 - acc: 0.8086 - val_loss: 0.4559 - val_acc: 0.8063\n",
      "Epoch 551/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4533 - acc: 0.8085 - val_loss: 0.4559 - val_acc: 0.8055\n",
      "Epoch 552/1000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4533 - acc: 0.8083 - val_loss: 0.4559 - val_acc: 0.8055\n",
      "Epoch 553/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8082 - val_loss: 0.4559 - val_acc: 0.8055\n",
      "Epoch 554/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4532 - acc: 0.8087 - val_loss: 0.4559 - val_acc: 0.8055\n",
      "Epoch 555/1000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4532 - acc: 0.8085 - val_loss: 0.4559 - val_acc: 0.8055\n",
      "Epoch 556/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8082 - val_loss: 0.4558 - val_acc: 0.8063\n",
      "Epoch 557/1000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4532 - acc: 0.8083 - val_loss: 0.4558 - val_acc: 0.8055\n",
      "Epoch 558/1000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4532 - acc: 0.8082 - val_loss: 0.4558 - val_acc: 0.8063\n",
      "Epoch 559/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8085 - val_loss: 0.4558 - val_acc: 0.8059\n",
      "Epoch 560/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4532 - acc: 0.8085 - val_loss: 0.4558 - val_acc: 0.8059\n",
      "Epoch 561/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4532 - acc: 0.8086 - val_loss: 0.4558 - val_acc: 0.8059\n",
      "Epoch 562/1000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4532 - acc: 0.8081 - val_loss: 0.4558 - val_acc: 0.8059\n",
      "Epoch 563/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8082 - val_loss: 0.4558 - val_acc: 0.8059\n",
      "Epoch 564/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4532 - acc: 0.8083 - val_loss: 0.4558 - val_acc: 0.8059\n",
      "Epoch 565/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4532 - acc: 0.8082 - val_loss: 0.4558 - val_acc: 0.8059\n",
      "Epoch 566/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4532 - acc: 0.8083 - val_loss: 0.4558 - val_acc: 0.8063\n",
      "Epoch 567/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4532 - acc: 0.8083 - val_loss: 0.4558 - val_acc: 0.8059\n",
      "Epoch 568/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4532 - acc: 0.8085 - val_loss: 0.4558 - val_acc: 0.8063\n",
      "Epoch 569/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8081 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 570/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8085 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 571/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4531 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.8059\n",
      "Epoch 572/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4532 - acc: 0.8085 - val_loss: 0.4557 - val_acc: 0.8059\n",
      "Epoch 573/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8085 - val_loss: 0.4557 - val_acc: 0.8059\n",
      "Epoch 574/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4531 - acc: 0.8085 - val_loss: 0.4557 - val_acc: 0.8059\n",
      "Epoch 575/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4531 - acc: 0.8083 - val_loss: 0.4557 - val_acc: 0.8059\n",
      "Epoch 576/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4531 - acc: 0.8081 - val_loss: 0.4557 - val_acc: 0.8059\n",
      "Epoch 577/1000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4531 - acc: 0.8089 - val_loss: 0.4557 - val_acc: 0.8059\n",
      "Epoch 578/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8079 - val_loss: 0.4557 - val_acc: 0.8059\n",
      "Epoch 579/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8085 - val_loss: 0.4557 - val_acc: 0.8059\n",
      "Epoch 580/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 581/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4531 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.8067\n",
      "Epoch 582/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8081 - val_loss: 0.4556 - val_acc: 0.8067\n",
      "Epoch 583/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 584/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4531 - acc: 0.8083 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 585/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8078 - val_loss: 0.4557 - val_acc: 0.8067\n",
      "Epoch 586/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 587/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 588/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8082 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 589/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4530 - acc: 0.8081 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 590/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8083 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4530 - acc: 0.8077 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 592/1000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4530 - acc: 0.8078 - val_loss: 0.4556 - val_acc: 0.8063\n",
      "Epoch 593/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4530 - acc: 0.8079 - val_loss: 0.4557 - val_acc: 0.8063\n",
      "Epoch 594/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4530 - acc: 0.8078 - val_loss: 0.4556 - val_acc: 0.8063\n",
      "Epoch 595/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.8089 - val_loss: 0.4556 - val_acc: 0.8063\n",
      "Epoch 596/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.8082 - val_loss: 0.4556 - val_acc: 0.8063\n",
      "Epoch 597/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4530 - acc: 0.8082 - val_loss: 0.4556 - val_acc: 0.8063\n",
      "Epoch 598/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4530 - acc: 0.8085 - val_loss: 0.4556 - val_acc: 0.8063\n",
      "Epoch 599/1000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4530 - acc: 0.8083 - val_loss: 0.4556 - val_acc: 0.8063\n",
      "Epoch 600/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4530 - acc: 0.8081 - val_loss: 0.4556 - val_acc: 0.8059\n",
      "Epoch 601/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4530 - acc: 0.8082 - val_loss: 0.4555 - val_acc: 0.8059\n",
      "Epoch 602/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4529 - acc: 0.8083 - val_loss: 0.4556 - val_acc: 0.8059\n",
      "Epoch 603/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4530 - acc: 0.8086 - val_loss: 0.4556 - val_acc: 0.8059\n",
      "Epoch 604/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4530 - acc: 0.8087 - val_loss: 0.4556 - val_acc: 0.8059\n",
      "Epoch 605/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4530 - acc: 0.8082 - val_loss: 0.4555 - val_acc: 0.8059\n",
      "Epoch 606/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4530 - acc: 0.8087 - val_loss: 0.4555 - val_acc: 0.8063\n",
      "Epoch 607/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4529 - acc: 0.8085 - val_loss: 0.4555 - val_acc: 0.8063\n",
      "Epoch 608/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4529 - acc: 0.8091 - val_loss: 0.4555 - val_acc: 0.8059\n",
      "Epoch 609/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4530 - acc: 0.8085 - val_loss: 0.4555 - val_acc: 0.8059\n",
      "Epoch 610/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4529 - acc: 0.8089 - val_loss: 0.4555 - val_acc: 0.8059\n",
      "Epoch 611/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8093 - val_loss: 0.4555 - val_acc: 0.8063\n",
      "Epoch 612/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8086 - val_loss: 0.4555 - val_acc: 0.8067\n",
      "Epoch 613/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4529 - acc: 0.8083 - val_loss: 0.4555 - val_acc: 0.8063\n",
      "Epoch 614/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4529 - acc: 0.8090 - val_loss: 0.4555 - val_acc: 0.8063\n",
      "Epoch 615/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4529 - acc: 0.8085 - val_loss: 0.4555 - val_acc: 0.8063\n",
      "Epoch 616/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8087 - val_loss: 0.4555 - val_acc: 0.8063\n",
      "Epoch 617/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8081 - val_loss: 0.4555 - val_acc: 0.8063\n",
      "Epoch 618/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8086 - val_loss: 0.4555 - val_acc: 0.8063\n",
      "Epoch 619/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8087 - val_loss: 0.4554 - val_acc: 0.8067\n",
      "Epoch 620/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8087 - val_loss: 0.4554 - val_acc: 0.8063\n",
      "Epoch 621/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8090 - val_loss: 0.4554 - val_acc: 0.8063\n",
      "Epoch 622/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8089 - val_loss: 0.4554 - val_acc: 0.8067\n",
      "Epoch 623/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4529 - acc: 0.8083 - val_loss: 0.4554 - val_acc: 0.8063\n",
      "Epoch 624/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4528 - acc: 0.8087 - val_loss: 0.4554 - val_acc: 0.8063\n",
      "Epoch 625/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4529 - acc: 0.8085 - val_loss: 0.4554 - val_acc: 0.8067\n",
      "Epoch 626/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4529 - acc: 0.8087 - val_loss: 0.4554 - val_acc: 0.8067\n",
      "Epoch 627/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4528 - acc: 0.8079 - val_loss: 0.4554 - val_acc: 0.8067\n",
      "Epoch 628/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8090 - val_loss: 0.4554 - val_acc: 0.8067\n",
      "Epoch 629/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4529 - acc: 0.8087 - val_loss: 0.4554 - val_acc: 0.8059\n",
      "Epoch 630/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.8083 - val_loss: 0.4554 - val_acc: 0.8059\n",
      "Epoch 631/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.8087 - val_loss: 0.4554 - val_acc: 0.8059\n",
      "Epoch 632/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4528 - acc: 0.8087 - val_loss: 0.4554 - val_acc: 0.8059\n",
      "Epoch 633/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4528 - acc: 0.8085 - val_loss: 0.4554 - val_acc: 0.8059\n",
      "Epoch 634/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4528 - acc: 0.8090 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 635/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.8086 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 636/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4528 - acc: 0.8086 - val_loss: 0.4554 - val_acc: 0.8063\n",
      "Epoch 637/1000\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4528 - acc: 0.8083 - val_loss: 0.4553 - val_acc: 0.8063\n",
      "Epoch 638/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.8086 - val_loss: 0.4554 - val_acc: 0.8063\n",
      "Epoch 639/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4528 - acc: 0.8091 - val_loss: 0.4554 - val_acc: 0.8059\n",
      "Epoch 640/1000\n",
      "7477/7477 [==============================] - 0s 43us/step - loss: 0.4528 - acc: 0.8090 - val_loss: 0.4553 - val_acc: 0.8063\n",
      "Epoch 641/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4528 - acc: 0.8083 - val_loss: 0.4553 - val_acc: 0.8063\n",
      "Epoch 642/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4527 - acc: 0.8086 - val_loss: 0.4553 - val_acc: 0.8063\n",
      "Epoch 643/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4528 - acc: 0.8086 - val_loss: 0.4553 - val_acc: 0.8063\n",
      "Epoch 644/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4527 - acc: 0.8087 - val_loss: 0.4553 - val_acc: 0.8063\n",
      "Epoch 645/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4528 - acc: 0.8081 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 646/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4527 - acc: 0.8085 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 647/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4527 - acc: 0.8085 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 648/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4527 - acc: 0.8087 - val_loss: 0.4553 - val_acc: 0.8055\n",
      "Epoch 649/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4527 - acc: 0.8087 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4527 - acc: 0.8085 - val_loss: 0.4554 - val_acc: 0.8055\n",
      "Epoch 651/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4527 - acc: 0.8083 - val_loss: 0.4553 - val_acc: 0.8055\n",
      "Epoch 652/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8078 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 653/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4527 - acc: 0.8082 - val_loss: 0.4553 - val_acc: 0.8055\n",
      "Epoch 654/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8079 - val_loss: 0.4553 - val_acc: 0.8055\n",
      "Epoch 655/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4527 - acc: 0.8081 - val_loss: 0.4553 - val_acc: 0.8055\n",
      "Epoch 656/1000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4527 - acc: 0.8081 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 657/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4527 - acc: 0.8083 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 658/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8078 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 659/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8085 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 660/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4527 - acc: 0.8075 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 661/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8081 - val_loss: 0.4553 - val_acc: 0.8059\n",
      "Epoch 662/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8051\n",
      "Epoch 663/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8083 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 664/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 665/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8077 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 666/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4527 - acc: 0.8079 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 667/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8059\n",
      "Epoch 668/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8059\n",
      "Epoch 669/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4526 - acc: 0.8081 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 670/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8081 - val_loss: 0.4552 - val_acc: 0.8051\n",
      "Epoch 671/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8085 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 672/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8051\n",
      "Epoch 673/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8075 - val_loss: 0.4552 - val_acc: 0.8051\n",
      "Epoch 674/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8051\n",
      "Epoch 675/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8082 - val_loss: 0.4552 - val_acc: 0.8051\n",
      "Epoch 676/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8083 - val_loss: 0.4552 - val_acc: 0.8051\n",
      "Epoch 677/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8082 - val_loss: 0.4552 - val_acc: 0.8051\n",
      "Epoch 678/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 679/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8081 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 680/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 681/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 682/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8081 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 683/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8077 - val_loss: 0.4551 - val_acc: 0.8055\n",
      "Epoch 684/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4525 - acc: 0.8083 - val_loss: 0.4551 - val_acc: 0.8055\n",
      "Epoch 685/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8055\n",
      "Epoch 686/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4552 - val_acc: 0.8051\n",
      "Epoch 687/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4525 - acc: 0.8083 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 688/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4525 - acc: 0.8081 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 689/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4525 - acc: 0.8083 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 690/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4525 - acc: 0.8077 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 691/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4525 - acc: 0.8082 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 692/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4525 - acc: 0.8082 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 693/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4525 - acc: 0.8081 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 694/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4525 - acc: 0.8081 - val_loss: 0.4551 - val_acc: 0.8043\n",
      "Epoch 695/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4525 - acc: 0.8082 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 696/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4525 - acc: 0.8074 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 697/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4525 - acc: 0.8078 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 698/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4525 - acc: 0.8078 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 699/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4525 - acc: 0.8078 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 700/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4525 - acc: 0.8081 - val_loss: 0.4551 - val_acc: 0.8043\n",
      "Epoch 701/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4525 - acc: 0.8067 - val_loss: 0.4550 - val_acc: 0.8051\n",
      "Epoch 702/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4525 - acc: 0.8075 - val_loss: 0.4551 - val_acc: 0.8051\n",
      "Epoch 703/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4525 - acc: 0.8079 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 704/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4525 - acc: 0.8074 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 705/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4525 - acc: 0.8075 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 706/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4524 - acc: 0.8083 - val_loss: 0.4550 - val_acc: 0.8047\n",
      "Epoch 707/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4525 - acc: 0.8078 - val_loss: 0.4550 - val_acc: 0.8047\n",
      "Epoch 708/1000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4524 - acc: 0.8079 - val_loss: 0.4550 - val_acc: 0.8047\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4524 - acc: 0.8079 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 710/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4524 - acc: 0.8075 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 711/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4524 - acc: 0.8077 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 712/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8079 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 713/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4524 - acc: 0.8075 - val_loss: 0.4549 - val_acc: 0.8039\n",
      "Epoch 714/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4524 - acc: 0.8078 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 715/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8079 - val_loss: 0.4550 - val_acc: 0.8043\n",
      "Epoch 716/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4524 - acc: 0.8079 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 717/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4524 - acc: 0.8074 - val_loss: 0.4550 - val_acc: 0.8047\n",
      "Epoch 718/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4524 - acc: 0.8069 - val_loss: 0.4550 - val_acc: 0.8043\n",
      "Epoch 719/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4524 - acc: 0.8073 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 720/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4524 - acc: 0.8071 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 721/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4524 - acc: 0.8077 - val_loss: 0.4550 - val_acc: 0.8047\n",
      "Epoch 722/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4524 - acc: 0.8077 - val_loss: 0.4550 - val_acc: 0.8043\n",
      "Epoch 723/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4524 - acc: 0.8073 - val_loss: 0.4550 - val_acc: 0.8047\n",
      "Epoch 724/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8077 - val_loss: 0.4550 - val_acc: 0.8047\n",
      "Epoch 725/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4524 - acc: 0.8070 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 726/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4524 - acc: 0.8075 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 727/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8075 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 728/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4524 - acc: 0.8071 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 729/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4523 - acc: 0.8082 - val_loss: 0.4549 - val_acc: 0.8039\n",
      "Epoch 730/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4523 - acc: 0.8078 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 731/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8073 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 732/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8075 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 733/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8074 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 734/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8073 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 735/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8074 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 736/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8071 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 737/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8071 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 738/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8073 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 739/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8070 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 740/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8074 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 741/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8075 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 742/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8077 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 743/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8079 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 744/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8075 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 745/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8078 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 746/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4522 - acc: 0.8079 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 747/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4522 - acc: 0.8075 - val_loss: 0.4549 - val_acc: 0.8051\n",
      "Epoch 748/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4522 - acc: 0.8075 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 749/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8078 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 750/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4522 - acc: 0.8078 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 751/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4522 - acc: 0.8077 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 752/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4522 - acc: 0.8078 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 753/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4522 - acc: 0.8074 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 754/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4522 - acc: 0.8073 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 755/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4522 - acc: 0.8077 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 756/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4522 - acc: 0.8082 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 757/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4522 - acc: 0.8077 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 758/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4521 - acc: 0.8078 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 759/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8081 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 760/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4521 - acc: 0.8078 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 761/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4521 - acc: 0.8079 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 762/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4521 - acc: 0.8079 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 763/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8071 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 764/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4521 - acc: 0.8074 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 765/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8078 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 766/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4521 - acc: 0.8078 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 767/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4521 - acc: 0.8082 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4521 - acc: 0.8074 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 769/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8081 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 770/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8075 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 771/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4521 - acc: 0.8082 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 772/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4521 - acc: 0.8081 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 773/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4520 - acc: 0.8079 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 774/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8077 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 775/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4521 - acc: 0.8075 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 776/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4521 - acc: 0.8078 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 777/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4520 - acc: 0.8081 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 778/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4521 - acc: 0.8081 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 779/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8082 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 780/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4520 - acc: 0.8077 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 781/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8082 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 782/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4520 - acc: 0.8075 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 783/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8081 - val_loss: 0.4549 - val_acc: 0.8051\n",
      "Epoch 784/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8082 - val_loss: 0.4549 - val_acc: 0.8051\n",
      "Epoch 785/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4520 - acc: 0.8077 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 786/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8077 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 787/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4520 - acc: 0.8083 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 788/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4520 - acc: 0.8086 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 789/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8082 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 790/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8079 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 791/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4520 - acc: 0.8085 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 792/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4520 - acc: 0.8078 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 793/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4520 - acc: 0.8082 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 794/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4520 - acc: 0.8083 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 795/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8083 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 796/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8083 - val_loss: 0.4548 - val_acc: 0.8051\n",
      "Epoch 797/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4520 - acc: 0.8085 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 798/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8083 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 799/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4519 - acc: 0.8082 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 800/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4520 - acc: 0.8082 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 801/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4520 - acc: 0.8082 - val_loss: 0.4549 - val_acc: 0.8047\n",
      "Epoch 802/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4519 - acc: 0.8083 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 803/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4519 - acc: 0.8085 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 804/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4519 - acc: 0.8083 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 805/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4519 - acc: 0.8089 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 806/1000\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4519 - acc: 0.8082 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 807/1000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4519 - acc: 0.8083 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 808/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4519 - acc: 0.8083 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 809/1000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4519 - acc: 0.8089 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 810/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4519 - acc: 0.8086 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 811/1000\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4519 - acc: 0.8083 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 812/1000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4519 - acc: 0.8081 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 813/1000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4519 - acc: 0.8085 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 814/1000\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4519 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 815/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4519 - acc: 0.8087 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 816/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4519 - acc: 0.8086 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 817/1000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4519 - acc: 0.8083 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 818/1000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4519 - acc: 0.8087 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 819/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4519 - acc: 0.8089 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 820/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4519 - acc: 0.8090 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 821/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4518 - acc: 0.8082 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 822/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4519 - acc: 0.8091 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 823/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4519 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 824/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 825/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 826/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4519 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4518 - acc: 0.8095 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 828/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4518 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 829/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4518 - acc: 0.8085 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 830/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 831/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4518 - acc: 0.8086 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 832/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4518 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 833/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 834/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4518 - acc: 0.8082 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 835/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4518 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 836/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 837/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4518 - acc: 0.8094 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 838/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4518 - acc: 0.8093 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 839/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4518 - acc: 0.8090 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 840/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4518 - acc: 0.8095 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 841/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4518 - acc: 0.8090 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 842/1000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4518 - acc: 0.8090 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 843/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4518 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8051\n",
      "Epoch 844/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4518 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 845/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4518 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 846/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4518 - acc: 0.8085 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 847/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4518 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 848/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 849/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4518 - acc: 0.8086 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 850/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4517 - acc: 0.8091 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 851/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4518 - acc: 0.8086 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 852/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4517 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 853/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 854/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4517 - acc: 0.8097 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 855/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4517 - acc: 0.8091 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 856/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4517 - acc: 0.8095 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 857/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4517 - acc: 0.8091 - val_loss: 0.4547 - val_acc: 0.8034\n",
      "Epoch 858/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4517 - acc: 0.8086 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 859/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4517 - acc: 0.8098 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 860/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4517 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 861/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 862/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4517 - acc: 0.8090 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 863/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 864/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4517 - acc: 0.8094 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 865/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 866/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4517 - acc: 0.8094 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 867/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4517 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 868/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4516 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 869/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4517 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 870/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4517 - acc: 0.8093 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 871/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4517 - acc: 0.8086 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 872/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 873/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4516 - acc: 0.8090 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 874/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4516 - acc: 0.8094 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 875/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4516 - acc: 0.8085 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 876/1000\n",
      "7477/7477 [==============================] - 0s 22us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 877/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4516 - acc: 0.8090 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 878/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4516 - acc: 0.8090 - val_loss: 0.4549 - val_acc: 0.8043\n",
      "Epoch 879/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4516 - acc: 0.8090 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 880/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4516 - acc: 0.8091 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 881/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4548 - val_acc: 0.8043\n",
      "Epoch 882/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 883/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 884/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4548 - val_acc: 0.8047\n",
      "Epoch 885/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4516 - acc: 0.8086 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 887/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4516 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 888/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4516 - acc: 0.8091 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 889/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4516 - acc: 0.8086 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 890/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4516 - acc: 0.8086 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 891/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4516 - acc: 0.8086 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 892/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4515 - acc: 0.8093 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 893/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4515 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 894/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4515 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 895/1000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4515 - acc: 0.8086 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 896/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4515 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 897/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4515 - acc: 0.8097 - val_loss: 0.4547 - val_acc: 0.8051\n",
      "Epoch 898/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4515 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8051\n",
      "Epoch 899/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4515 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 900/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4515 - acc: 0.8085 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 901/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4515 - acc: 0.8085 - val_loss: 0.4547 - val_acc: 0.8051\n",
      "Epoch 902/1000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4515 - acc: 0.8093 - val_loss: 0.4547 - val_acc: 0.8051\n",
      "Epoch 903/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4515 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 904/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4515 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 905/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4515 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 906/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4515 - acc: 0.8091 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 907/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4515 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 908/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4515 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 909/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4515 - acc: 0.8094 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 910/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4515 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8051\n",
      "Epoch 911/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4514 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8051\n",
      "Epoch 912/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4515 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 913/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4515 - acc: 0.8089 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 914/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4514 - acc: 0.8090 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 915/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4515 - acc: 0.8089 - val_loss: 0.4547 - val_acc: 0.8047\n",
      "Epoch 916/1000\n",
      "7477/7477 [==============================] - 0s 22us/step - loss: 0.4514 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 917/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4514 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 918/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4514 - acc: 0.8089 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 919/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4514 - acc: 0.8089 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 920/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4514 - acc: 0.8094 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 921/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4514 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 922/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4514 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 923/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4514 - acc: 0.8094 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 924/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4514 - acc: 0.8086 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 925/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4514 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 926/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4514 - acc: 0.8089 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 927/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4514 - acc: 0.8089 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 928/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4514 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 929/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4514 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 930/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4514 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 931/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4514 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 932/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4514 - acc: 0.8095 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 933/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4514 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 934/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4514 - acc: 0.8089 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 935/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4514 - acc: 0.8089 - val_loss: 0.4546 - val_acc: 0.8051\n",
      "Epoch 936/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4514 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 937/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4514 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8051\n",
      "Epoch 938/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4513 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8051\n",
      "Epoch 939/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4513 - acc: 0.8089 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 940/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4513 - acc: 0.8094 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 941/1000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4514 - acc: 0.8093 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 942/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4513 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8051\n",
      "Epoch 943/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4513 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 944/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4513 - acc: 0.8097 - val_loss: 0.4546 - val_acc: 0.8051\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4513 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8051\n",
      "Epoch 946/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4513 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 947/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4513 - acc: 0.8089 - val_loss: 0.4545 - val_acc: 0.8051\n",
      "Epoch 948/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4513 - acc: 0.8093 - val_loss: 0.4545 - val_acc: 0.8051\n",
      "Epoch 949/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4513 - acc: 0.8089 - val_loss: 0.4545 - val_acc: 0.8051\n",
      "Epoch 950/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4513 - acc: 0.8095 - val_loss: 0.4545 - val_acc: 0.8051\n",
      "Epoch 951/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4512 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8051\n",
      "Epoch 952/1000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4513 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8055\n",
      "Epoch 953/1000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4513 - acc: 0.8090 - val_loss: 0.4546 - val_acc: 0.8055\n",
      "Epoch 954/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4513 - acc: 0.8090 - val_loss: 0.4545 - val_acc: 0.8055\n",
      "Epoch 955/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4513 - acc: 0.8091 - val_loss: 0.4545 - val_acc: 0.8055\n",
      "Epoch 956/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4513 - acc: 0.8093 - val_loss: 0.4545 - val_acc: 0.8051\n",
      "Epoch 957/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4513 - acc: 0.8090 - val_loss: 0.4545 - val_acc: 0.8051\n",
      "Epoch 958/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4513 - acc: 0.8091 - val_loss: 0.4546 - val_acc: 0.8051\n",
      "Epoch 959/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4513 - acc: 0.8094 - val_loss: 0.4545 - val_acc: 0.8055\n",
      "Epoch 960/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8094 - val_loss: 0.4545 - val_acc: 0.8047\n",
      "Epoch 961/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4513 - acc: 0.8091 - val_loss: 0.4545 - val_acc: 0.8047\n",
      "Epoch 962/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4513 - acc: 0.8095 - val_loss: 0.4545 - val_acc: 0.8055\n",
      "Epoch 963/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8091 - val_loss: 0.4545 - val_acc: 0.8055\n",
      "Epoch 964/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8090 - val_loss: 0.4545 - val_acc: 0.8051\n",
      "Epoch 965/1000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4513 - acc: 0.8090 - val_loss: 0.4545 - val_acc: 0.8055\n",
      "Epoch 966/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8094 - val_loss: 0.4545 - val_acc: 0.8055\n",
      "Epoch 967/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8091 - val_loss: 0.4545 - val_acc: 0.8055\n",
      "Epoch 968/1000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4512 - acc: 0.8093 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 969/1000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4512 - acc: 0.8091 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 970/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4512 - acc: 0.8094 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 971/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8091 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 972/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4512 - acc: 0.8093 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 973/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8091 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 974/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8089 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 975/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8091 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 976/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8095 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 977/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8093 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 978/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8093 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 979/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8094 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 980/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8091 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 981/1000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4512 - acc: 0.8090 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 982/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8093 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 983/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4511 - acc: 0.8091 - val_loss: 0.4545 - val_acc: 0.8055\n",
      "Epoch 984/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4512 - acc: 0.8093 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 985/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4511 - acc: 0.8091 - val_loss: 0.4544 - val_acc: 0.8059\n",
      "Epoch 986/1000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4512 - acc: 0.8094 - val_loss: 0.4544 - val_acc: 0.8059\n",
      "Epoch 987/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8095 - val_loss: 0.4544 - val_acc: 0.8055\n",
      "Epoch 988/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8094 - val_loss: 0.4543 - val_acc: 0.8055\n",
      "Epoch 989/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4511 - acc: 0.8090 - val_loss: 0.4544 - val_acc: 0.8059\n",
      "Epoch 990/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4512 - acc: 0.8090 - val_loss: 0.4543 - val_acc: 0.8059\n",
      "Epoch 991/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4512 - acc: 0.8093 - val_loss: 0.4543 - val_acc: 0.8059\n",
      "Epoch 992/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4512 - acc: 0.8091 - val_loss: 0.4543 - val_acc: 0.8059\n",
      "Epoch 993/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4511 - acc: 0.8090 - val_loss: 0.4543 - val_acc: 0.8059\n",
      "Epoch 994/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4511 - acc: 0.8089 - val_loss: 0.4543 - val_acc: 0.8059\n",
      "Epoch 995/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4511 - acc: 0.8090 - val_loss: 0.4543 - val_acc: 0.8059\n",
      "Epoch 996/1000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4511 - acc: 0.8090 - val_loss: 0.4543 - val_acc: 0.8063\n",
      "Epoch 997/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4511 - acc: 0.8089 - val_loss: 0.4543 - val_acc: 0.8059\n",
      "Epoch 998/1000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4511 - acc: 0.8090 - val_loss: 0.4543 - val_acc: 0.8063\n",
      "Epoch 999/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4511 - acc: 0.8091 - val_loss: 0.4543 - val_acc: 0.8059\n",
      "Epoch 1000/1000\n",
      "7477/7477 [==============================] - 0s 23us/step - loss: 0.4511 - acc: 0.8090 - val_loss: 0.4543 - val_acc: 0.8059\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f704cef7d30>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now it's your turn.  Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 6 hidden nodes, relu activation\n",
    "# 1 hidden layer, 6 hidden nodes, relu activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "\n",
    "model_2 = Sequential([\n",
    "    #hidden layers\n",
    "    Dense(6, input_shape=(9,), activation=\"relu\"),\n",
    "    Dense(6, activation=\"relu\"),\n",
    "    #final layer\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 6)                 60        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 109\n",
      "Trainable params: 109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7477 samples, validate on 2493 samples\n",
      "Epoch 1/1500\n",
      "7477/7477 [==============================] - 0s 49us/step - loss: 0.6164 - acc: 0.7018 - val_loss: 0.5950 - val_acc: 0.7497\n",
      "Epoch 2/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.5705 - acc: 0.7686 - val_loss: 0.5690 - val_acc: 0.7714\n",
      "Epoch 3/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.5510 - acc: 0.7781 - val_loss: 0.5551 - val_acc: 0.7746\n",
      "Epoch 4/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.5389 - acc: 0.7785 - val_loss: 0.5448 - val_acc: 0.7770\n",
      "Epoch 5/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.5297 - acc: 0.7789 - val_loss: 0.5366 - val_acc: 0.7766\n",
      "Epoch 6/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.5224 - acc: 0.7801 - val_loss: 0.5296 - val_acc: 0.7778\n",
      "Epoch 7/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.5162 - acc: 0.7813 - val_loss: 0.5235 - val_acc: 0.7798\n",
      "Epoch 8/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.5109 - acc: 0.7820 - val_loss: 0.5182 - val_acc: 0.7842\n",
      "Epoch 9/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.5062 - acc: 0.7852 - val_loss: 0.5135 - val_acc: 0.7878\n",
      "Epoch 10/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.5021 - acc: 0.7872 - val_loss: 0.5091 - val_acc: 0.7870\n",
      "Epoch 11/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4984 - acc: 0.7911 - val_loss: 0.5052 - val_acc: 0.7922\n",
      "Epoch 12/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4950 - acc: 0.7935 - val_loss: 0.5016 - val_acc: 0.7958\n",
      "Epoch 13/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4919 - acc: 0.7976 - val_loss: 0.4983 - val_acc: 0.8006\n",
      "Epoch 14/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4891 - acc: 0.8005 - val_loss: 0.4953 - val_acc: 0.8030\n",
      "Epoch 15/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4866 - acc: 0.8015 - val_loss: 0.4927 - val_acc: 0.8047\n",
      "Epoch 16/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4844 - acc: 0.8023 - val_loss: 0.4903 - val_acc: 0.8047\n",
      "Epoch 17/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4824 - acc: 0.8025 - val_loss: 0.4881 - val_acc: 0.8043\n",
      "Epoch 18/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4806 - acc: 0.8026 - val_loss: 0.4860 - val_acc: 0.8034\n",
      "Epoch 19/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4790 - acc: 0.8029 - val_loss: 0.4842 - val_acc: 0.8030\n",
      "Epoch 20/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4776 - acc: 0.8033 - val_loss: 0.4827 - val_acc: 0.8018\n",
      "Epoch 21/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4764 - acc: 0.8030 - val_loss: 0.4813 - val_acc: 0.8014\n",
      "Epoch 22/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4753 - acc: 0.8031 - val_loss: 0.4800 - val_acc: 0.8014\n",
      "Epoch 23/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4743 - acc: 0.8031 - val_loss: 0.4789 - val_acc: 0.8014\n",
      "Epoch 24/1500\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4735 - acc: 0.8033 - val_loss: 0.4779 - val_acc: 0.8014\n",
      "Epoch 25/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4727 - acc: 0.8031 - val_loss: 0.4770 - val_acc: 0.8014\n",
      "Epoch 26/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4721 - acc: 0.8034 - val_loss: 0.4763 - val_acc: 0.8014\n",
      "Epoch 27/1500\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4715 - acc: 0.8033 - val_loss: 0.4755 - val_acc: 0.8014\n",
      "Epoch 28/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4709 - acc: 0.8033 - val_loss: 0.4748 - val_acc: 0.8014\n",
      "Epoch 29/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4704 - acc: 0.8031 - val_loss: 0.4741 - val_acc: 0.8018\n",
      "Epoch 30/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4700 - acc: 0.8035 - val_loss: 0.4736 - val_acc: 0.8014\n",
      "Epoch 31/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4696 - acc: 0.8033 - val_loss: 0.4731 - val_acc: 0.8014\n",
      "Epoch 32/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4692 - acc: 0.8033 - val_loss: 0.4726 - val_acc: 0.8018\n",
      "Epoch 33/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4689 - acc: 0.8035 - val_loss: 0.4721 - val_acc: 0.8018\n",
      "Epoch 34/1500\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4686 - acc: 0.8034 - val_loss: 0.4717 - val_acc: 0.8022\n",
      "Epoch 35/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4684 - acc: 0.8033 - val_loss: 0.4714 - val_acc: 0.8018\n",
      "Epoch 36/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4681 - acc: 0.8034 - val_loss: 0.4710 - val_acc: 0.8018\n",
      "Epoch 37/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4679 - acc: 0.8035 - val_loss: 0.4707 - val_acc: 0.8010\n",
      "Epoch 38/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4677 - acc: 0.8034 - val_loss: 0.4704 - val_acc: 0.8022\n",
      "Epoch 39/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4675 - acc: 0.8038 - val_loss: 0.4701 - val_acc: 0.8014\n",
      "Epoch 40/1500\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4673 - acc: 0.8035 - val_loss: 0.4698 - val_acc: 0.8022\n",
      "Epoch 41/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4672 - acc: 0.8039 - val_loss: 0.4695 - val_acc: 0.8018\n",
      "Epoch 42/1500\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4670 - acc: 0.8033 - val_loss: 0.4693 - val_acc: 0.8026\n",
      "Epoch 43/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4669 - acc: 0.8038 - val_loss: 0.4691 - val_acc: 0.8026\n",
      "Epoch 44/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4667 - acc: 0.8034 - val_loss: 0.4688 - val_acc: 0.8026\n",
      "Epoch 45/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4666 - acc: 0.8041 - val_loss: 0.4687 - val_acc: 0.8022\n",
      "Epoch 46/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4664 - acc: 0.8039 - val_loss: 0.4686 - val_acc: 0.8018\n",
      "Epoch 47/1500\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4664 - acc: 0.8037 - val_loss: 0.4683 - val_acc: 0.8022\n",
      "Epoch 48/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4663 - acc: 0.8037 - val_loss: 0.4681 - val_acc: 0.8022\n",
      "Epoch 49/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4661 - acc: 0.8037 - val_loss: 0.4679 - val_acc: 0.8026\n",
      "Epoch 50/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4660 - acc: 0.8035 - val_loss: 0.4678 - val_acc: 0.8022\n",
      "Epoch 51/1500\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4659 - acc: 0.8034 - val_loss: 0.4676 - val_acc: 0.8022\n",
      "Epoch 52/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4659 - acc: 0.8037 - val_loss: 0.4675 - val_acc: 0.8026\n",
      "Epoch 53/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4658 - acc: 0.8031 - val_loss: 0.4674 - val_acc: 0.8022\n",
      "Epoch 54/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4657 - acc: 0.8034 - val_loss: 0.4673 - val_acc: 0.8026\n",
      "Epoch 55/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4656 - acc: 0.8035 - val_loss: 0.4671 - val_acc: 0.8026\n",
      "Epoch 56/1500\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4655 - acc: 0.8037 - val_loss: 0.4670 - val_acc: 0.8026\n",
      "Epoch 57/1500\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4654 - acc: 0.8038 - val_loss: 0.4669 - val_acc: 0.8026\n",
      "Epoch 58/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4654 - acc: 0.8038 - val_loss: 0.4668 - val_acc: 0.8026\n",
      "Epoch 59/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4653 - acc: 0.8037 - val_loss: 0.4667 - val_acc: 0.8026\n",
      "Epoch 60/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4652 - acc: 0.8041 - val_loss: 0.4666 - val_acc: 0.8026\n",
      "Epoch 61/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4651 - acc: 0.8041 - val_loss: 0.4664 - val_acc: 0.8030\n",
      "Epoch 62/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4651 - acc: 0.8037 - val_loss: 0.4663 - val_acc: 0.8030\n",
      "Epoch 63/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4650 - acc: 0.8037 - val_loss: 0.4663 - val_acc: 0.8030\n",
      "Epoch 64/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4650 - acc: 0.8041 - val_loss: 0.4662 - val_acc: 0.8026\n",
      "Epoch 65/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4649 - acc: 0.8037 - val_loss: 0.4661 - val_acc: 0.8030\n",
      "Epoch 66/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4648 - acc: 0.8035 - val_loss: 0.4660 - val_acc: 0.8026\n",
      "Epoch 67/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4648 - acc: 0.8034 - val_loss: 0.4659 - val_acc: 0.8030\n",
      "Epoch 68/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4647 - acc: 0.8034 - val_loss: 0.4658 - val_acc: 0.8030\n",
      "Epoch 69/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4646 - acc: 0.8033 - val_loss: 0.4657 - val_acc: 0.8030\n",
      "Epoch 70/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4646 - acc: 0.8033 - val_loss: 0.4656 - val_acc: 0.8030\n",
      "Epoch 71/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4645 - acc: 0.8033 - val_loss: 0.4656 - val_acc: 0.8034\n",
      "Epoch 72/1500\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4644 - acc: 0.8031 - val_loss: 0.4655 - val_acc: 0.8026\n",
      "Epoch 73/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4644 - acc: 0.8033 - val_loss: 0.4654 - val_acc: 0.8030\n",
      "Epoch 74/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4643 - acc: 0.8031 - val_loss: 0.4654 - val_acc: 0.8030\n",
      "Epoch 75/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4643 - acc: 0.8030 - val_loss: 0.4653 - val_acc: 0.8026\n",
      "Epoch 76/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4642 - acc: 0.8033 - val_loss: 0.4652 - val_acc: 0.8026\n",
      "Epoch 77/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4642 - acc: 0.8033 - val_loss: 0.4651 - val_acc: 0.8030\n",
      "Epoch 78/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4641 - acc: 0.8031 - val_loss: 0.4651 - val_acc: 0.8034\n",
      "Epoch 79/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4641 - acc: 0.8031 - val_loss: 0.4650 - val_acc: 0.8034\n",
      "Epoch 80/1500\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4640 - acc: 0.8038 - val_loss: 0.4650 - val_acc: 0.8030\n",
      "Epoch 81/1500\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4640 - acc: 0.8033 - val_loss: 0.4649 - val_acc: 0.8030\n",
      "Epoch 82/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4639 - acc: 0.8035 - val_loss: 0.4649 - val_acc: 0.8030\n",
      "Epoch 83/1500\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4639 - acc: 0.8030 - val_loss: 0.4649 - val_acc: 0.8030\n",
      "Epoch 84/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4638 - acc: 0.8031 - val_loss: 0.4648 - val_acc: 0.8030\n",
      "Epoch 85/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4638 - acc: 0.8034 - val_loss: 0.4647 - val_acc: 0.8043\n",
      "Epoch 86/1500\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4637 - acc: 0.8039 - val_loss: 0.4648 - val_acc: 0.8034\n",
      "Epoch 87/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4637 - acc: 0.8034 - val_loss: 0.4646 - val_acc: 0.8043\n",
      "Epoch 88/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4636 - acc: 0.8035 - val_loss: 0.4646 - val_acc: 0.8039\n",
      "Epoch 89/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4636 - acc: 0.8037 - val_loss: 0.4646 - val_acc: 0.8039\n",
      "Epoch 90/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4635 - acc: 0.8035 - val_loss: 0.4645 - val_acc: 0.8043\n",
      "Epoch 91/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4635 - acc: 0.8034 - val_loss: 0.4645 - val_acc: 0.8039\n",
      "Epoch 92/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4634 - acc: 0.8033 - val_loss: 0.4645 - val_acc: 0.8047\n",
      "Epoch 93/1500\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4634 - acc: 0.8031 - val_loss: 0.4645 - val_acc: 0.8047\n",
      "Epoch 94/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4634 - acc: 0.8030 - val_loss: 0.4644 - val_acc: 0.8047\n",
      "Epoch 95/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4633 - acc: 0.8034 - val_loss: 0.4644 - val_acc: 0.8047\n",
      "Epoch 96/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4633 - acc: 0.8037 - val_loss: 0.4643 - val_acc: 0.8047\n",
      "Epoch 97/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4632 - acc: 0.8030 - val_loss: 0.4643 - val_acc: 0.8047\n",
      "Epoch 98/1500\n",
      "7477/7477 [==============================] - 0s 47us/step - loss: 0.4632 - acc: 0.8030 - val_loss: 0.4642 - val_acc: 0.8051\n",
      "Epoch 99/1500\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4631 - acc: 0.8031 - val_loss: 0.4642 - val_acc: 0.8051\n",
      "Epoch 100/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4631 - acc: 0.8030 - val_loss: 0.4641 - val_acc: 0.8051\n",
      "Epoch 101/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4630 - acc: 0.8031 - val_loss: 0.4641 - val_acc: 0.8051\n",
      "Epoch 102/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4630 - acc: 0.8030 - val_loss: 0.4641 - val_acc: 0.8051\n",
      "Epoch 103/1500\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4629 - acc: 0.8033 - val_loss: 0.4640 - val_acc: 0.8051\n",
      "Epoch 104/1500\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4629 - acc: 0.8031 - val_loss: 0.4640 - val_acc: 0.8051\n",
      "Epoch 105/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4628 - acc: 0.8033 - val_loss: 0.4639 - val_acc: 0.8051\n",
      "Epoch 106/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4628 - acc: 0.8034 - val_loss: 0.4639 - val_acc: 0.8051\n",
      "Epoch 107/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4628 - acc: 0.8034 - val_loss: 0.4638 - val_acc: 0.8051\n",
      "Epoch 108/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4627 - acc: 0.8037 - val_loss: 0.4638 - val_acc: 0.8051\n",
      "Epoch 109/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4626 - acc: 0.8031 - val_loss: 0.4638 - val_acc: 0.8051\n",
      "Epoch 110/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4626 - acc: 0.8035 - val_loss: 0.4638 - val_acc: 0.8051\n",
      "Epoch 111/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4626 - acc: 0.8035 - val_loss: 0.4637 - val_acc: 0.8051\n",
      "Epoch 112/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4625 - acc: 0.8030 - val_loss: 0.4637 - val_acc: 0.8051\n",
      "Epoch 113/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4625 - acc: 0.8038 - val_loss: 0.4637 - val_acc: 0.8051\n",
      "Epoch 114/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4625 - acc: 0.8030 - val_loss: 0.4636 - val_acc: 0.8043\n",
      "Epoch 115/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4624 - acc: 0.8035 - val_loss: 0.4636 - val_acc: 0.8043\n",
      "Epoch 116/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4623 - acc: 0.8031 - val_loss: 0.4635 - val_acc: 0.8043\n",
      "Epoch 117/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4623 - acc: 0.8041 - val_loss: 0.4635 - val_acc: 0.8043\n",
      "Epoch 118/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4623 - acc: 0.8033 - val_loss: 0.4634 - val_acc: 0.8043\n",
      "Epoch 119/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4623 - acc: 0.8031 - val_loss: 0.4634 - val_acc: 0.8043\n",
      "Epoch 120/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4622 - acc: 0.8037 - val_loss: 0.4634 - val_acc: 0.8043\n",
      "Epoch 121/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4622 - acc: 0.8034 - val_loss: 0.4634 - val_acc: 0.8043\n",
      "Epoch 122/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4622 - acc: 0.8034 - val_loss: 0.4633 - val_acc: 0.8039\n",
      "Epoch 123/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4621 - acc: 0.8033 - val_loss: 0.4633 - val_acc: 0.8039\n",
      "Epoch 124/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4621 - acc: 0.8033 - val_loss: 0.4633 - val_acc: 0.8043\n",
      "Epoch 125/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4621 - acc: 0.8035 - val_loss: 0.4632 - val_acc: 0.8039\n",
      "Epoch 126/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4620 - acc: 0.8033 - val_loss: 0.4632 - val_acc: 0.8039\n",
      "Epoch 127/1500\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4620 - acc: 0.8033 - val_loss: 0.4631 - val_acc: 0.8039\n",
      "Epoch 128/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4620 - acc: 0.8030 - val_loss: 0.4631 - val_acc: 0.8039\n",
      "Epoch 129/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4619 - acc: 0.8031 - val_loss: 0.4631 - val_acc: 0.8039\n",
      "Epoch 130/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4619 - acc: 0.8031 - val_loss: 0.4631 - val_acc: 0.8039\n",
      "Epoch 131/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4619 - acc: 0.8037 - val_loss: 0.4630 - val_acc: 0.8039\n",
      "Epoch 132/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4619 - acc: 0.8034 - val_loss: 0.4630 - val_acc: 0.8039\n",
      "Epoch 133/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4619 - acc: 0.8035 - val_loss: 0.4630 - val_acc: 0.8039\n",
      "Epoch 134/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4618 - acc: 0.8033 - val_loss: 0.4629 - val_acc: 0.8043\n",
      "Epoch 135/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4618 - acc: 0.8039 - val_loss: 0.4629 - val_acc: 0.8039\n",
      "Epoch 136/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4618 - acc: 0.8033 - val_loss: 0.4629 - val_acc: 0.8039\n",
      "Epoch 137/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4617 - acc: 0.8033 - val_loss: 0.4628 - val_acc: 0.8039\n",
      "Epoch 138/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4617 - acc: 0.8027 - val_loss: 0.4628 - val_acc: 0.8051\n",
      "Epoch 139/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4617 - acc: 0.8029 - val_loss: 0.4628 - val_acc: 0.8047\n",
      "Epoch 140/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4616 - acc: 0.8038 - val_loss: 0.4627 - val_acc: 0.8047\n",
      "Epoch 141/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4616 - acc: 0.8035 - val_loss: 0.4627 - val_acc: 0.8039\n",
      "Epoch 142/1500\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4616 - acc: 0.8031 - val_loss: 0.4627 - val_acc: 0.8047\n",
      "Epoch 143/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4616 - acc: 0.8034 - val_loss: 0.4626 - val_acc: 0.8047\n",
      "Epoch 144/1500\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4616 - acc: 0.8033 - val_loss: 0.4626 - val_acc: 0.8043\n",
      "Epoch 145/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4615 - acc: 0.8034 - val_loss: 0.4626 - val_acc: 0.8043\n",
      "Epoch 146/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4615 - acc: 0.8031 - val_loss: 0.4626 - val_acc: 0.8043\n",
      "Epoch 147/1500\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4615 - acc: 0.8035 - val_loss: 0.4625 - val_acc: 0.8047\n",
      "Epoch 148/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4615 - acc: 0.8033 - val_loss: 0.4625 - val_acc: 0.8047\n",
      "Epoch 149/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4614 - acc: 0.8034 - val_loss: 0.4625 - val_acc: 0.8047\n",
      "Epoch 150/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4614 - acc: 0.8033 - val_loss: 0.4624 - val_acc: 0.8047\n",
      "Epoch 151/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4614 - acc: 0.8035 - val_loss: 0.4624 - val_acc: 0.8051\n",
      "Epoch 152/1500\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4614 - acc: 0.8029 - val_loss: 0.4623 - val_acc: 0.8051\n",
      "Epoch 153/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4613 - acc: 0.8037 - val_loss: 0.4623 - val_acc: 0.8051\n",
      "Epoch 154/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4613 - acc: 0.8026 - val_loss: 0.4623 - val_acc: 0.8055\n",
      "Epoch 155/1500\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4613 - acc: 0.8039 - val_loss: 0.4623 - val_acc: 0.8055\n",
      "Epoch 156/1500\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4613 - acc: 0.8033 - val_loss: 0.4623 - val_acc: 0.8051\n",
      "Epoch 157/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4612 - acc: 0.8038 - val_loss: 0.4622 - val_acc: 0.8051\n",
      "Epoch 158/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4612 - acc: 0.8034 - val_loss: 0.4622 - val_acc: 0.8051\n",
      "Epoch 159/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4612 - acc: 0.8043 - val_loss: 0.4622 - val_acc: 0.8051\n",
      "Epoch 160/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4612 - acc: 0.8033 - val_loss: 0.4622 - val_acc: 0.8047\n",
      "Epoch 161/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4611 - acc: 0.8035 - val_loss: 0.4622 - val_acc: 0.8051\n",
      "Epoch 162/1500\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4611 - acc: 0.8038 - val_loss: 0.4622 - val_acc: 0.8047\n",
      "Epoch 163/1500\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4611 - acc: 0.8038 - val_loss: 0.4621 - val_acc: 0.8051\n",
      "Epoch 164/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4611 - acc: 0.8039 - val_loss: 0.4621 - val_acc: 0.8047\n",
      "Epoch 165/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4610 - acc: 0.8029 - val_loss: 0.4621 - val_acc: 0.8051\n",
      "Epoch 166/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4610 - acc: 0.8033 - val_loss: 0.4620 - val_acc: 0.8051\n",
      "Epoch 167/1500\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4610 - acc: 0.8041 - val_loss: 0.4620 - val_acc: 0.8051\n",
      "Epoch 168/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4610 - acc: 0.8035 - val_loss: 0.4620 - val_acc: 0.8055\n",
      "Epoch 169/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4610 - acc: 0.8038 - val_loss: 0.4619 - val_acc: 0.8051\n",
      "Epoch 170/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4609 - acc: 0.8037 - val_loss: 0.4619 - val_acc: 0.8047\n",
      "Epoch 171/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4609 - acc: 0.8033 - val_loss: 0.4619 - val_acc: 0.8051\n",
      "Epoch 172/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4609 - acc: 0.8039 - val_loss: 0.4618 - val_acc: 0.8047\n",
      "Epoch 173/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4609 - acc: 0.8041 - val_loss: 0.4618 - val_acc: 0.8047\n",
      "Epoch 174/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4608 - acc: 0.8042 - val_loss: 0.4618 - val_acc: 0.8051\n",
      "Epoch 175/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4608 - acc: 0.8039 - val_loss: 0.4618 - val_acc: 0.8047\n",
      "Epoch 176/1500\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4608 - acc: 0.8035 - val_loss: 0.4618 - val_acc: 0.8047\n",
      "Epoch 177/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4608 - acc: 0.8034 - val_loss: 0.4617 - val_acc: 0.8047\n",
      "Epoch 178/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4608 - acc: 0.8037 - val_loss: 0.4617 - val_acc: 0.8047\n",
      "Epoch 179/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4607 - acc: 0.8037 - val_loss: 0.4617 - val_acc: 0.8047\n",
      "Epoch 180/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4607 - acc: 0.8033 - val_loss: 0.4617 - val_acc: 0.8047\n",
      "Epoch 181/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4607 - acc: 0.8037 - val_loss: 0.4617 - val_acc: 0.8051\n",
      "Epoch 182/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4607 - acc: 0.8042 - val_loss: 0.4616 - val_acc: 0.8047\n",
      "Epoch 183/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4607 - acc: 0.8035 - val_loss: 0.4616 - val_acc: 0.8051\n",
      "Epoch 184/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4607 - acc: 0.8038 - val_loss: 0.4616 - val_acc: 0.8047\n",
      "Epoch 185/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4606 - acc: 0.8037 - val_loss: 0.4615 - val_acc: 0.8051\n",
      "Epoch 186/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4606 - acc: 0.8039 - val_loss: 0.4615 - val_acc: 0.8055\n",
      "Epoch 187/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4606 - acc: 0.8037 - val_loss: 0.4615 - val_acc: 0.8051\n",
      "Epoch 188/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4606 - acc: 0.8038 - val_loss: 0.4615 - val_acc: 0.8051\n",
      "Epoch 189/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4605 - acc: 0.8037 - val_loss: 0.4614 - val_acc: 0.8059\n",
      "Epoch 190/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4605 - acc: 0.8037 - val_loss: 0.4614 - val_acc: 0.8059\n",
      "Epoch 191/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4605 - acc: 0.8034 - val_loss: 0.4614 - val_acc: 0.8059\n",
      "Epoch 192/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4605 - acc: 0.8034 - val_loss: 0.4613 - val_acc: 0.8059\n",
      "Epoch 193/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4604 - acc: 0.8037 - val_loss: 0.4613 - val_acc: 0.8059\n",
      "Epoch 194/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4604 - acc: 0.8037 - val_loss: 0.4613 - val_acc: 0.8051\n",
      "Epoch 195/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4604 - acc: 0.8039 - val_loss: 0.4613 - val_acc: 0.8055\n",
      "Epoch 196/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4604 - acc: 0.8038 - val_loss: 0.4613 - val_acc: 0.8051\n",
      "Epoch 197/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4604 - acc: 0.8038 - val_loss: 0.4612 - val_acc: 0.8051\n",
      "Epoch 198/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4603 - acc: 0.8038 - val_loss: 0.4612 - val_acc: 0.8059\n",
      "Epoch 199/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4603 - acc: 0.8038 - val_loss: 0.4612 - val_acc: 0.8055\n",
      "Epoch 200/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4603 - acc: 0.8037 - val_loss: 0.4611 - val_acc: 0.8059\n",
      "Epoch 201/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4603 - acc: 0.8034 - val_loss: 0.4611 - val_acc: 0.8059\n",
      "Epoch 202/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4603 - acc: 0.8038 - val_loss: 0.4611 - val_acc: 0.8059\n",
      "Epoch 203/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4602 - acc: 0.8034 - val_loss: 0.4610 - val_acc: 0.8055\n",
      "Epoch 204/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4602 - acc: 0.8038 - val_loss: 0.4610 - val_acc: 0.8059\n",
      "Epoch 205/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4602 - acc: 0.8041 - val_loss: 0.4610 - val_acc: 0.8059\n",
      "Epoch 206/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4602 - acc: 0.8041 - val_loss: 0.4610 - val_acc: 0.8055\n",
      "Epoch 207/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4601 - acc: 0.8035 - val_loss: 0.4609 - val_acc: 0.8059\n",
      "Epoch 208/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4601 - acc: 0.8035 - val_loss: 0.4609 - val_acc: 0.8059\n",
      "Epoch 209/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4601 - acc: 0.8038 - val_loss: 0.4609 - val_acc: 0.8059\n",
      "Epoch 210/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4601 - acc: 0.8033 - val_loss: 0.4609 - val_acc: 0.8055\n",
      "Epoch 211/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4600 - acc: 0.8034 - val_loss: 0.4609 - val_acc: 0.8059\n",
      "Epoch 212/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4600 - acc: 0.8035 - val_loss: 0.4608 - val_acc: 0.8055\n",
      "Epoch 213/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4600 - acc: 0.8042 - val_loss: 0.4608 - val_acc: 0.8051\n",
      "Epoch 214/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4600 - acc: 0.8038 - val_loss: 0.4608 - val_acc: 0.8051\n",
      "Epoch 215/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4600 - acc: 0.8041 - val_loss: 0.4608 - val_acc: 0.8055\n",
      "Epoch 216/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4600 - acc: 0.8035 - val_loss: 0.4608 - val_acc: 0.8055\n",
      "Epoch 217/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4599 - acc: 0.8035 - val_loss: 0.4607 - val_acc: 0.8055\n",
      "Epoch 218/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4599 - acc: 0.8039 - val_loss: 0.4607 - val_acc: 0.8051\n",
      "Epoch 219/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4599 - acc: 0.8041 - val_loss: 0.4607 - val_acc: 0.8047\n",
      "Epoch 220/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4599 - acc: 0.8038 - val_loss: 0.4607 - val_acc: 0.8047\n",
      "Epoch 221/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4599 - acc: 0.8037 - val_loss: 0.4606 - val_acc: 0.8043\n",
      "Epoch 222/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4598 - acc: 0.8035 - val_loss: 0.4606 - val_acc: 0.8047\n",
      "Epoch 223/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4598 - acc: 0.8042 - val_loss: 0.4606 - val_acc: 0.8047\n",
      "Epoch 224/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4598 - acc: 0.8041 - val_loss: 0.4606 - val_acc: 0.8043\n",
      "Epoch 225/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4598 - acc: 0.8043 - val_loss: 0.4606 - val_acc: 0.8043\n",
      "Epoch 226/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4597 - acc: 0.8043 - val_loss: 0.4606 - val_acc: 0.8043\n",
      "Epoch 227/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4598 - acc: 0.8039 - val_loss: 0.4605 - val_acc: 0.8043\n",
      "Epoch 228/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4597 - acc: 0.8041 - val_loss: 0.4605 - val_acc: 0.8043\n",
      "Epoch 229/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4597 - acc: 0.8039 - val_loss: 0.4605 - val_acc: 0.8055\n",
      "Epoch 230/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4597 - acc: 0.8039 - val_loss: 0.4605 - val_acc: 0.8043\n",
      "Epoch 231/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4597 - acc: 0.8038 - val_loss: 0.4604 - val_acc: 0.8051\n",
      "Epoch 232/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4596 - acc: 0.8039 - val_loss: 0.4604 - val_acc: 0.8051\n",
      "Epoch 233/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4596 - acc: 0.8038 - val_loss: 0.4604 - val_acc: 0.8055\n",
      "Epoch 234/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4596 - acc: 0.8042 - val_loss: 0.4604 - val_acc: 0.8051\n",
      "Epoch 235/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4596 - acc: 0.8038 - val_loss: 0.4604 - val_acc: 0.8047\n",
      "Epoch 236/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4596 - acc: 0.8038 - val_loss: 0.4604 - val_acc: 0.8051\n",
      "Epoch 237/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4595 - acc: 0.8041 - val_loss: 0.4603 - val_acc: 0.8047\n",
      "Epoch 238/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4595 - acc: 0.8041 - val_loss: 0.4603 - val_acc: 0.8047\n",
      "Epoch 239/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4595 - acc: 0.8039 - val_loss: 0.4603 - val_acc: 0.8051\n",
      "Epoch 240/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4595 - acc: 0.8039 - val_loss: 0.4603 - val_acc: 0.8047\n",
      "Epoch 241/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4595 - acc: 0.8037 - val_loss: 0.4603 - val_acc: 0.8047\n",
      "Epoch 242/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4594 - acc: 0.8035 - val_loss: 0.4602 - val_acc: 0.8047\n",
      "Epoch 243/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4594 - acc: 0.8037 - val_loss: 0.4602 - val_acc: 0.8043\n",
      "Epoch 244/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4594 - acc: 0.8042 - val_loss: 0.4602 - val_acc: 0.8047\n",
      "Epoch 245/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4594 - acc: 0.8035 - val_loss: 0.4602 - val_acc: 0.8047\n",
      "Epoch 246/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4593 - acc: 0.8039 - val_loss: 0.4601 - val_acc: 0.8059\n",
      "Epoch 247/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4593 - acc: 0.8045 - val_loss: 0.4601 - val_acc: 0.8063\n",
      "Epoch 248/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4593 - acc: 0.8034 - val_loss: 0.4601 - val_acc: 0.8047\n",
      "Epoch 249/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4593 - acc: 0.8035 - val_loss: 0.4601 - val_acc: 0.8059\n",
      "Epoch 250/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4593 - acc: 0.8038 - val_loss: 0.4600 - val_acc: 0.8071\n",
      "Epoch 251/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4593 - acc: 0.8038 - val_loss: 0.4600 - val_acc: 0.8051\n",
      "Epoch 252/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4593 - acc: 0.8038 - val_loss: 0.4600 - val_acc: 0.8055\n",
      "Epoch 253/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4593 - acc: 0.8037 - val_loss: 0.4600 - val_acc: 0.8055\n",
      "Epoch 254/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4592 - acc: 0.8038 - val_loss: 0.4599 - val_acc: 0.8055\n",
      "Epoch 255/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4592 - acc: 0.8037 - val_loss: 0.4599 - val_acc: 0.8055\n",
      "Epoch 256/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4592 - acc: 0.8034 - val_loss: 0.4599 - val_acc: 0.8055\n",
      "Epoch 257/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4592 - acc: 0.8039 - val_loss: 0.4599 - val_acc: 0.8055\n",
      "Epoch 258/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4592 - acc: 0.8039 - val_loss: 0.4599 - val_acc: 0.8055\n",
      "Epoch 259/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4592 - acc: 0.8031 - val_loss: 0.4598 - val_acc: 0.8055\n",
      "Epoch 260/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4592 - acc: 0.8035 - val_loss: 0.4598 - val_acc: 0.8055\n",
      "Epoch 261/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4591 - acc: 0.8034 - val_loss: 0.4598 - val_acc: 0.8063\n",
      "Epoch 262/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4591 - acc: 0.8041 - val_loss: 0.4598 - val_acc: 0.8055\n",
      "Epoch 263/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4591 - acc: 0.8037 - val_loss: 0.4597 - val_acc: 0.8055\n",
      "Epoch 264/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4591 - acc: 0.8034 - val_loss: 0.4597 - val_acc: 0.8055\n",
      "Epoch 265/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4590 - acc: 0.8037 - val_loss: 0.4597 - val_acc: 0.8071\n",
      "Epoch 266/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4591 - acc: 0.8038 - val_loss: 0.4596 - val_acc: 0.8055\n",
      "Epoch 267/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4590 - acc: 0.8034 - val_loss: 0.4596 - val_acc: 0.8051\n",
      "Epoch 268/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4590 - acc: 0.8041 - val_loss: 0.4596 - val_acc: 0.8055\n",
      "Epoch 269/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4590 - acc: 0.8031 - val_loss: 0.4596 - val_acc: 0.8051\n",
      "Epoch 270/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4590 - acc: 0.8034 - val_loss: 0.4596 - val_acc: 0.8051\n",
      "Epoch 271/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4590 - acc: 0.8038 - val_loss: 0.4595 - val_acc: 0.8055\n",
      "Epoch 272/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4590 - acc: 0.8037 - val_loss: 0.4595 - val_acc: 0.8055\n",
      "Epoch 273/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4589 - acc: 0.8037 - val_loss: 0.4595 - val_acc: 0.8055\n",
      "Epoch 274/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4589 - acc: 0.8035 - val_loss: 0.4595 - val_acc: 0.8055\n",
      "Epoch 275/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4589 - acc: 0.8038 - val_loss: 0.4595 - val_acc: 0.8059\n",
      "Epoch 276/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4589 - acc: 0.8035 - val_loss: 0.4594 - val_acc: 0.8059\n",
      "Epoch 277/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4589 - acc: 0.8035 - val_loss: 0.4594 - val_acc: 0.8055\n",
      "Epoch 278/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4589 - acc: 0.8035 - val_loss: 0.4594 - val_acc: 0.8051\n",
      "Epoch 279/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4588 - acc: 0.8037 - val_loss: 0.4594 - val_acc: 0.8059\n",
      "Epoch 280/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4588 - acc: 0.8038 - val_loss: 0.4594 - val_acc: 0.8059\n",
      "Epoch 281/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4588 - acc: 0.8037 - val_loss: 0.4593 - val_acc: 0.8059\n",
      "Epoch 282/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4588 - acc: 0.8039 - val_loss: 0.4593 - val_acc: 0.8059\n",
      "Epoch 283/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4588 - acc: 0.8038 - val_loss: 0.4593 - val_acc: 0.8055\n",
      "Epoch 284/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4588 - acc: 0.8035 - val_loss: 0.4593 - val_acc: 0.8059\n",
      "Epoch 285/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4587 - acc: 0.8039 - val_loss: 0.4593 - val_acc: 0.8059\n",
      "Epoch 286/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4587 - acc: 0.8043 - val_loss: 0.4592 - val_acc: 0.8059\n",
      "Epoch 287/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4587 - acc: 0.8041 - val_loss: 0.4592 - val_acc: 0.8059\n",
      "Epoch 288/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4587 - acc: 0.8038 - val_loss: 0.4592 - val_acc: 0.8059\n",
      "Epoch 289/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4587 - acc: 0.8041 - val_loss: 0.4592 - val_acc: 0.8059\n",
      "Epoch 290/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4586 - acc: 0.8034 - val_loss: 0.4591 - val_acc: 0.8071\n",
      "Epoch 291/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4586 - acc: 0.8041 - val_loss: 0.4591 - val_acc: 0.8075\n",
      "Epoch 292/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4586 - acc: 0.8041 - val_loss: 0.4591 - val_acc: 0.8059\n",
      "Epoch 293/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4586 - acc: 0.8041 - val_loss: 0.4591 - val_acc: 0.8063\n",
      "Epoch 294/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4586 - acc: 0.8038 - val_loss: 0.4591 - val_acc: 0.8063\n",
      "Epoch 295/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4586 - acc: 0.8038 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 296/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4585 - acc: 0.8038 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 297/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4585 - acc: 0.8038 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 298/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4585 - acc: 0.8034 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 299/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4585 - acc: 0.8042 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 300/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4585 - acc: 0.8041 - val_loss: 0.4590 - val_acc: 0.8055\n",
      "Epoch 301/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4585 - acc: 0.8037 - val_loss: 0.4590 - val_acc: 0.8067\n",
      "Epoch 302/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4584 - acc: 0.8042 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 303/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4584 - acc: 0.8041 - val_loss: 0.4589 - val_acc: 0.8059\n",
      "Epoch 304/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4584 - acc: 0.8041 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 305/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4584 - acc: 0.8042 - val_loss: 0.4589 - val_acc: 0.8067\n",
      "Epoch 306/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4584 - acc: 0.8038 - val_loss: 0.4588 - val_acc: 0.8067\n",
      "Epoch 307/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4583 - acc: 0.8041 - val_loss: 0.4588 - val_acc: 0.8059\n",
      "Epoch 308/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4584 - acc: 0.8043 - val_loss: 0.4588 - val_acc: 0.8059\n",
      "Epoch 309/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4583 - acc: 0.8038 - val_loss: 0.4588 - val_acc: 0.8063\n",
      "Epoch 310/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4583 - acc: 0.8038 - val_loss: 0.4587 - val_acc: 0.8059\n",
      "Epoch 311/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4583 - acc: 0.8045 - val_loss: 0.4587 - val_acc: 0.8071\n",
      "Epoch 312/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4583 - acc: 0.8043 - val_loss: 0.4587 - val_acc: 0.8067\n",
      "Epoch 313/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4582 - acc: 0.8039 - val_loss: 0.4587 - val_acc: 0.8067\n",
      "Epoch 314/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4582 - acc: 0.8037 - val_loss: 0.4586 - val_acc: 0.8063\n",
      "Epoch 315/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4582 - acc: 0.8039 - val_loss: 0.4586 - val_acc: 0.8063\n",
      "Epoch 316/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4582 - acc: 0.8039 - val_loss: 0.4586 - val_acc: 0.8067\n",
      "Epoch 317/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4582 - acc: 0.8041 - val_loss: 0.4585 - val_acc: 0.8071\n",
      "Epoch 318/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4582 - acc: 0.8038 - val_loss: 0.4585 - val_acc: 0.8063\n",
      "Epoch 319/1500\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4581 - acc: 0.8043 - val_loss: 0.4585 - val_acc: 0.8067\n",
      "Epoch 320/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4581 - acc: 0.8039 - val_loss: 0.4585 - val_acc: 0.8071\n",
      "Epoch 321/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4581 - acc: 0.8043 - val_loss: 0.4584 - val_acc: 0.8067\n",
      "Epoch 322/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4581 - acc: 0.8042 - val_loss: 0.4584 - val_acc: 0.8071\n",
      "Epoch 323/1500\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4581 - acc: 0.8041 - val_loss: 0.4583 - val_acc: 0.8071\n",
      "Epoch 324/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4581 - acc: 0.8047 - val_loss: 0.4583 - val_acc: 0.8059\n",
      "Epoch 325/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4581 - acc: 0.8039 - val_loss: 0.4583 - val_acc: 0.8059\n",
      "Epoch 326/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4580 - acc: 0.8041 - val_loss: 0.4583 - val_acc: 0.8059\n",
      "Epoch 327/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4580 - acc: 0.8043 - val_loss: 0.4583 - val_acc: 0.8071\n",
      "Epoch 328/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4580 - acc: 0.8042 - val_loss: 0.4582 - val_acc: 0.8059\n",
      "Epoch 329/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4580 - acc: 0.8037 - val_loss: 0.4582 - val_acc: 0.8071\n",
      "Epoch 330/1500\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4580 - acc: 0.8042 - val_loss: 0.4582 - val_acc: 0.8071\n",
      "Epoch 331/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4580 - acc: 0.8038 - val_loss: 0.4582 - val_acc: 0.8075\n",
      "Epoch 332/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4579 - acc: 0.8051 - val_loss: 0.4582 - val_acc: 0.8063\n",
      "Epoch 333/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4579 - acc: 0.8045 - val_loss: 0.4582 - val_acc: 0.8063\n",
      "Epoch 334/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4579 - acc: 0.8043 - val_loss: 0.4581 - val_acc: 0.8059\n",
      "Epoch 335/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4579 - acc: 0.8045 - val_loss: 0.4581 - val_acc: 0.8059\n",
      "Epoch 336/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4579 - acc: 0.8042 - val_loss: 0.4581 - val_acc: 0.8079\n",
      "Epoch 337/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4578 - acc: 0.8038 - val_loss: 0.4580 - val_acc: 0.8063\n",
      "Epoch 338/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4578 - acc: 0.8043 - val_loss: 0.4580 - val_acc: 0.8059\n",
      "Epoch 339/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4578 - acc: 0.8039 - val_loss: 0.4580 - val_acc: 0.8071\n",
      "Epoch 340/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4578 - acc: 0.8041 - val_loss: 0.4579 - val_acc: 0.8071\n",
      "Epoch 341/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4578 - acc: 0.8043 - val_loss: 0.4579 - val_acc: 0.8071\n",
      "Epoch 342/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4578 - acc: 0.8051 - val_loss: 0.4579 - val_acc: 0.8071\n",
      "Epoch 343/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4577 - acc: 0.8043 - val_loss: 0.4578 - val_acc: 0.8067\n",
      "Epoch 344/1500\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4577 - acc: 0.8045 - val_loss: 0.4578 - val_acc: 0.8071\n",
      "Epoch 345/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4577 - acc: 0.8039 - val_loss: 0.4578 - val_acc: 0.8071\n",
      "Epoch 346/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4577 - acc: 0.8043 - val_loss: 0.4577 - val_acc: 0.8071\n",
      "Epoch 347/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4577 - acc: 0.8046 - val_loss: 0.4577 - val_acc: 0.8071\n",
      "Epoch 348/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4577 - acc: 0.8042 - val_loss: 0.4576 - val_acc: 0.8071\n",
      "Epoch 349/1500\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4576 - acc: 0.8045 - val_loss: 0.4576 - val_acc: 0.8067\n",
      "Epoch 350/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4576 - acc: 0.8047 - val_loss: 0.4576 - val_acc: 0.8071\n",
      "Epoch 351/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4576 - acc: 0.8041 - val_loss: 0.4576 - val_acc: 0.8071\n",
      "Epoch 352/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4576 - acc: 0.8046 - val_loss: 0.4575 - val_acc: 0.8071\n",
      "Epoch 353/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4576 - acc: 0.8049 - val_loss: 0.4575 - val_acc: 0.8071\n",
      "Epoch 354/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4575 - acc: 0.8049 - val_loss: 0.4575 - val_acc: 0.8067\n",
      "Epoch 355/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4575 - acc: 0.8046 - val_loss: 0.4574 - val_acc: 0.8071\n",
      "Epoch 356/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4575 - acc: 0.8047 - val_loss: 0.4574 - val_acc: 0.8071\n",
      "Epoch 357/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4575 - acc: 0.8047 - val_loss: 0.4573 - val_acc: 0.8071\n",
      "Epoch 358/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4575 - acc: 0.8049 - val_loss: 0.4573 - val_acc: 0.8071\n",
      "Epoch 359/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4574 - acc: 0.8049 - val_loss: 0.4573 - val_acc: 0.8067\n",
      "Epoch 360/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4574 - acc: 0.8050 - val_loss: 0.4572 - val_acc: 0.8063\n",
      "Epoch 361/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4574 - acc: 0.8049 - val_loss: 0.4572 - val_acc: 0.8059\n",
      "Epoch 362/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4574 - acc: 0.8053 - val_loss: 0.4571 - val_acc: 0.8059\n",
      "Epoch 363/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4573 - acc: 0.8050 - val_loss: 0.4571 - val_acc: 0.8059\n",
      "Epoch 364/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4573 - acc: 0.8049 - val_loss: 0.4570 - val_acc: 0.8055\n",
      "Epoch 365/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4573 - acc: 0.8049 - val_loss: 0.4570 - val_acc: 0.8059\n",
      "Epoch 366/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4572 - acc: 0.8049 - val_loss: 0.4570 - val_acc: 0.8055\n",
      "Epoch 367/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4572 - acc: 0.8050 - val_loss: 0.4569 - val_acc: 0.8047\n",
      "Epoch 368/1500\n",
      "7477/7477 [==============================] - 0s 43us/step - loss: 0.4572 - acc: 0.8050 - val_loss: 0.4569 - val_acc: 0.8055\n",
      "Epoch 369/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4572 - acc: 0.8049 - val_loss: 0.4569 - val_acc: 0.8055\n",
      "Epoch 370/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4572 - acc: 0.8051 - val_loss: 0.4568 - val_acc: 0.8047\n",
      "Epoch 371/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4572 - acc: 0.8051 - val_loss: 0.4568 - val_acc: 0.8055\n",
      "Epoch 372/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4571 - acc: 0.8049 - val_loss: 0.4568 - val_acc: 0.8051\n",
      "Epoch 373/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4571 - acc: 0.8047 - val_loss: 0.4568 - val_acc: 0.8051\n",
      "Epoch 374/1500\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4571 - acc: 0.8049 - val_loss: 0.4567 - val_acc: 0.8055\n",
      "Epoch 375/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4571 - acc: 0.8051 - val_loss: 0.4567 - val_acc: 0.8055\n",
      "Epoch 376/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4571 - acc: 0.8051 - val_loss: 0.4567 - val_acc: 0.8051\n",
      "Epoch 377/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4570 - acc: 0.8051 - val_loss: 0.4566 - val_acc: 0.8051\n",
      "Epoch 378/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4570 - acc: 0.8053 - val_loss: 0.4566 - val_acc: 0.8051\n",
      "Epoch 379/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4570 - acc: 0.8049 - val_loss: 0.4566 - val_acc: 0.8055\n",
      "Epoch 380/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4570 - acc: 0.8045 - val_loss: 0.4565 - val_acc: 0.8051\n",
      "Epoch 381/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4570 - acc: 0.8045 - val_loss: 0.4565 - val_acc: 0.8055\n",
      "Epoch 382/1500\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4570 - acc: 0.8050 - val_loss: 0.4565 - val_acc: 0.8051\n",
      "Epoch 383/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4569 - acc: 0.8049 - val_loss: 0.4565 - val_acc: 0.8051\n",
      "Epoch 384/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4569 - acc: 0.8047 - val_loss: 0.4564 - val_acc: 0.8055\n",
      "Epoch 385/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4569 - acc: 0.8043 - val_loss: 0.4564 - val_acc: 0.8051\n",
      "Epoch 386/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4569 - acc: 0.8049 - val_loss: 0.4564 - val_acc: 0.8051\n",
      "Epoch 387/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4569 - acc: 0.8050 - val_loss: 0.4563 - val_acc: 0.8051\n",
      "Epoch 388/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4569 - acc: 0.8047 - val_loss: 0.4563 - val_acc: 0.8051\n",
      "Epoch 389/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4569 - acc: 0.8045 - val_loss: 0.4563 - val_acc: 0.8047\n",
      "Epoch 390/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4568 - acc: 0.8046 - val_loss: 0.4562 - val_acc: 0.8047\n",
      "Epoch 391/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4568 - acc: 0.8047 - val_loss: 0.4562 - val_acc: 0.8043\n",
      "Epoch 392/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4568 - acc: 0.8041 - val_loss: 0.4562 - val_acc: 0.8051\n",
      "Epoch 393/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4568 - acc: 0.8046 - val_loss: 0.4562 - val_acc: 0.8043\n",
      "Epoch 394/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4568 - acc: 0.8046 - val_loss: 0.4561 - val_acc: 0.8051\n",
      "Epoch 395/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4567 - acc: 0.8043 - val_loss: 0.4561 - val_acc: 0.8043\n",
      "Epoch 396/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4567 - acc: 0.8047 - val_loss: 0.4561 - val_acc: 0.8055\n",
      "Epoch 397/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4567 - acc: 0.8045 - val_loss: 0.4560 - val_acc: 0.8051\n",
      "Epoch 398/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4567 - acc: 0.8046 - val_loss: 0.4560 - val_acc: 0.8043\n",
      "Epoch 399/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4566 - acc: 0.8039 - val_loss: 0.4560 - val_acc: 0.8051\n",
      "Epoch 400/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4567 - acc: 0.8043 - val_loss: 0.4560 - val_acc: 0.8051\n",
      "Epoch 401/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4566 - acc: 0.8043 - val_loss: 0.4560 - val_acc: 0.8051\n",
      "Epoch 402/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4566 - acc: 0.8043 - val_loss: 0.4559 - val_acc: 0.8051\n",
      "Epoch 403/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4566 - acc: 0.8037 - val_loss: 0.4559 - val_acc: 0.8051\n",
      "Epoch 404/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4566 - acc: 0.8045 - val_loss: 0.4559 - val_acc: 0.8055\n",
      "Epoch 405/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4566 - acc: 0.8045 - val_loss: 0.4559 - val_acc: 0.8043\n",
      "Epoch 406/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4565 - acc: 0.8045 - val_loss: 0.4559 - val_acc: 0.8055\n",
      "Epoch 407/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4565 - acc: 0.8043 - val_loss: 0.4558 - val_acc: 0.8051\n",
      "Epoch 408/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4565 - acc: 0.8045 - val_loss: 0.4558 - val_acc: 0.8043\n",
      "Epoch 409/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4565 - acc: 0.8042 - val_loss: 0.4558 - val_acc: 0.8043\n",
      "Epoch 410/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4565 - acc: 0.8043 - val_loss: 0.4558 - val_acc: 0.8039\n",
      "Epoch 411/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4565 - acc: 0.8042 - val_loss: 0.4557 - val_acc: 0.8043\n",
      "Epoch 412/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4565 - acc: 0.8043 - val_loss: 0.4557 - val_acc: 0.8039\n",
      "Epoch 413/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4565 - acc: 0.8042 - val_loss: 0.4557 - val_acc: 0.8043\n",
      "Epoch 414/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4564 - acc: 0.8042 - val_loss: 0.4557 - val_acc: 0.8047\n",
      "Epoch 415/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4564 - acc: 0.8046 - val_loss: 0.4556 - val_acc: 0.8043\n",
      "Epoch 416/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4564 - acc: 0.8045 - val_loss: 0.4556 - val_acc: 0.8051\n",
      "Epoch 417/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4564 - acc: 0.8045 - val_loss: 0.4556 - val_acc: 0.8047\n",
      "Epoch 418/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4564 - acc: 0.8047 - val_loss: 0.4556 - val_acc: 0.8043\n",
      "Epoch 419/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4563 - acc: 0.8051 - val_loss: 0.4556 - val_acc: 0.8051\n",
      "Epoch 420/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4563 - acc: 0.8046 - val_loss: 0.4556 - val_acc: 0.8051\n",
      "Epoch 421/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4563 - acc: 0.8045 - val_loss: 0.4555 - val_acc: 0.8051\n",
      "Epoch 422/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4563 - acc: 0.8049 - val_loss: 0.4555 - val_acc: 0.8051\n",
      "Epoch 423/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4563 - acc: 0.8046 - val_loss: 0.4555 - val_acc: 0.8043\n",
      "Epoch 424/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4563 - acc: 0.8054 - val_loss: 0.4555 - val_acc: 0.8051\n",
      "Epoch 425/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4563 - acc: 0.8043 - val_loss: 0.4555 - val_acc: 0.8047\n",
      "Epoch 426/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4562 - acc: 0.8051 - val_loss: 0.4554 - val_acc: 0.8047\n",
      "Epoch 427/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4562 - acc: 0.8046 - val_loss: 0.4554 - val_acc: 0.8047\n",
      "Epoch 428/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4562 - acc: 0.8046 - val_loss: 0.4554 - val_acc: 0.8051\n",
      "Epoch 429/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4562 - acc: 0.8049 - val_loss: 0.4554 - val_acc: 0.8051\n",
      "Epoch 430/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4562 - acc: 0.8049 - val_loss: 0.4554 - val_acc: 0.8047\n",
      "Epoch 431/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4562 - acc: 0.8050 - val_loss: 0.4554 - val_acc: 0.8051\n",
      "Epoch 432/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4561 - acc: 0.8057 - val_loss: 0.4554 - val_acc: 0.8043\n",
      "Epoch 433/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4561 - acc: 0.8051 - val_loss: 0.4554 - val_acc: 0.8039\n",
      "Epoch 434/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4561 - acc: 0.8050 - val_loss: 0.4554 - val_acc: 0.8039\n",
      "Epoch 435/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4561 - acc: 0.8049 - val_loss: 0.4554 - val_acc: 0.8043\n",
      "Epoch 436/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4561 - acc: 0.8050 - val_loss: 0.4553 - val_acc: 0.8043\n",
      "Epoch 437/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4561 - acc: 0.8055 - val_loss: 0.4553 - val_acc: 0.8039\n",
      "Epoch 438/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4561 - acc: 0.8051 - val_loss: 0.4553 - val_acc: 0.8039\n",
      "Epoch 439/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4560 - acc: 0.8053 - val_loss: 0.4553 - val_acc: 0.8039\n",
      "Epoch 440/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4560 - acc: 0.8050 - val_loss: 0.4553 - val_acc: 0.8047\n",
      "Epoch 441/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4560 - acc: 0.8046 - val_loss: 0.4553 - val_acc: 0.8047\n",
      "Epoch 442/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4560 - acc: 0.8046 - val_loss: 0.4553 - val_acc: 0.8047\n",
      "Epoch 443/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4560 - acc: 0.8053 - val_loss: 0.4552 - val_acc: 0.8043\n",
      "Epoch 444/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4560 - acc: 0.8050 - val_loss: 0.4552 - val_acc: 0.8043\n",
      "Epoch 445/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4560 - acc: 0.8054 - val_loss: 0.4552 - val_acc: 0.8047\n",
      "Epoch 446/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4560 - acc: 0.8049 - val_loss: 0.4552 - val_acc: 0.8034\n",
      "Epoch 447/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4559 - acc: 0.8050 - val_loss: 0.4552 - val_acc: 0.8047\n",
      "Epoch 448/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4559 - acc: 0.8053 - val_loss: 0.4551 - val_acc: 0.8043\n",
      "Epoch 449/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4559 - acc: 0.8053 - val_loss: 0.4552 - val_acc: 0.8034\n",
      "Epoch 450/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4559 - acc: 0.8057 - val_loss: 0.4552 - val_acc: 0.8034\n",
      "Epoch 451/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4559 - acc: 0.8047 - val_loss: 0.4552 - val_acc: 0.8034\n",
      "Epoch 452/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4559 - acc: 0.8043 - val_loss: 0.4552 - val_acc: 0.8039\n",
      "Epoch 453/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4559 - acc: 0.8046 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 454/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4559 - acc: 0.8050 - val_loss: 0.4551 - val_acc: 0.8039\n",
      "Epoch 455/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4559 - acc: 0.8047 - val_loss: 0.4551 - val_acc: 0.8043\n",
      "Epoch 456/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4558 - acc: 0.8047 - val_loss: 0.4551 - val_acc: 0.8039\n",
      "Epoch 457/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4558 - acc: 0.8057 - val_loss: 0.4551 - val_acc: 0.8039\n",
      "Epoch 458/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4558 - acc: 0.8043 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 459/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4558 - acc: 0.8050 - val_loss: 0.4551 - val_acc: 0.8047\n",
      "Epoch 460/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4558 - acc: 0.8047 - val_loss: 0.4550 - val_acc: 0.8047\n",
      "Epoch 461/1500\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4558 - acc: 0.8047 - val_loss: 0.4550 - val_acc: 0.8043\n",
      "Epoch 462/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4558 - acc: 0.8050 - val_loss: 0.4550 - val_acc: 0.8043\n",
      "Epoch 463/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4558 - acc: 0.8053 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 464/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4557 - acc: 0.8054 - val_loss: 0.4550 - val_acc: 0.8043\n",
      "Epoch 465/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4557 - acc: 0.8047 - val_loss: 0.4550 - val_acc: 0.8043\n",
      "Epoch 466/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4557 - acc: 0.8049 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 467/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4557 - acc: 0.8050 - val_loss: 0.4550 - val_acc: 0.8039\n",
      "Epoch 468/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4557 - acc: 0.8053 - val_loss: 0.4549 - val_acc: 0.8034\n",
      "Epoch 469/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4557 - acc: 0.8047 - val_loss: 0.4549 - val_acc: 0.8039\n",
      "Epoch 470/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4557 - acc: 0.8051 - val_loss: 0.4549 - val_acc: 0.8039\n",
      "Epoch 471/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4557 - acc: 0.8047 - val_loss: 0.4549 - val_acc: 0.8039\n",
      "Epoch 472/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4556 - acc: 0.8054 - val_loss: 0.4549 - val_acc: 0.8039\n",
      "Epoch 473/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4556 - acc: 0.8050 - val_loss: 0.4549 - val_acc: 0.8034\n",
      "Epoch 474/1500\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4556 - acc: 0.8049 - val_loss: 0.4549 - val_acc: 0.8034\n",
      "Epoch 475/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4556 - acc: 0.8054 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 476/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4556 - acc: 0.8047 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 477/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4556 - acc: 0.8053 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 478/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4556 - acc: 0.8049 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 479/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4555 - acc: 0.8055 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 480/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4556 - acc: 0.8047 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 481/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4556 - acc: 0.8046 - val_loss: 0.4548 - val_acc: 0.8039\n",
      "Epoch 482/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4556 - acc: 0.8051 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 483/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4555 - acc: 0.8051 - val_loss: 0.4548 - val_acc: 0.8030\n",
      "Epoch 484/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4555 - acc: 0.8046 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 485/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4555 - acc: 0.8049 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 486/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4555 - acc: 0.8043 - val_loss: 0.4548 - val_acc: 0.8034\n",
      "Epoch 487/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4555 - acc: 0.8049 - val_loss: 0.4547 - val_acc: 0.8034\n",
      "Epoch 488/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4555 - acc: 0.8049 - val_loss: 0.4547 - val_acc: 0.8034\n",
      "Epoch 489/1500\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4555 - acc: 0.8046 - val_loss: 0.4547 - val_acc: 0.8034\n",
      "Epoch 490/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4555 - acc: 0.8042 - val_loss: 0.4547 - val_acc: 0.8034\n",
      "Epoch 491/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4555 - acc: 0.8049 - val_loss: 0.4547 - val_acc: 0.8034\n",
      "Epoch 492/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4555 - acc: 0.8047 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 493/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4555 - acc: 0.8043 - val_loss: 0.4547 - val_acc: 0.8034\n",
      "Epoch 494/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4554 - acc: 0.8047 - val_loss: 0.4547 - val_acc: 0.8034\n",
      "Epoch 495/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4554 - acc: 0.8055 - val_loss: 0.4547 - val_acc: 0.8043\n",
      "Epoch 496/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4554 - acc: 0.8047 - val_loss: 0.4546 - val_acc: 0.8030\n",
      "Epoch 497/1500\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4554 - acc: 0.8046 - val_loss: 0.4546 - val_acc: 0.8039\n",
      "Epoch 498/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4554 - acc: 0.8051 - val_loss: 0.4546 - val_acc: 0.8030\n",
      "Epoch 499/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4554 - acc: 0.8051 - val_loss: 0.4547 - val_acc: 0.8039\n",
      "Epoch 500/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4554 - acc: 0.8050 - val_loss: 0.4546 - val_acc: 0.8034\n",
      "Epoch 501/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4554 - acc: 0.8050 - val_loss: 0.4546 - val_acc: 0.8034\n",
      "Epoch 502/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4554 - acc: 0.8047 - val_loss: 0.4546 - val_acc: 0.8030\n",
      "Epoch 503/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4554 - acc: 0.8049 - val_loss: 0.4546 - val_acc: 0.8039\n",
      "Epoch 504/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4554 - acc: 0.8049 - val_loss: 0.4546 - val_acc: 0.8047\n",
      "Epoch 505/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4553 - acc: 0.8053 - val_loss: 0.4546 - val_acc: 0.8030\n",
      "Epoch 506/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4553 - acc: 0.8050 - val_loss: 0.4546 - val_acc: 0.8039\n",
      "Epoch 507/1500\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4553 - acc: 0.8045 - val_loss: 0.4546 - val_acc: 0.8043\n",
      "Epoch 508/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4553 - acc: 0.8053 - val_loss: 0.4546 - val_acc: 0.8043\n",
      "Epoch 509/1500\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4553 - acc: 0.8053 - val_loss: 0.4546 - val_acc: 0.8043\n",
      "Epoch 510/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4553 - acc: 0.8046 - val_loss: 0.4546 - val_acc: 0.8043\n",
      "Epoch 511/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4553 - acc: 0.8051 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 512/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4553 - acc: 0.8049 - val_loss: 0.4545 - val_acc: 0.8034\n",
      "Epoch 513/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4553 - acc: 0.8043 - val_loss: 0.4545 - val_acc: 0.8034\n",
      "Epoch 514/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4553 - acc: 0.8051 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 515/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4552 - acc: 0.8046 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 516/1500\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4553 - acc: 0.8047 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 517/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4552 - acc: 0.8053 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 518/1500\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4552 - acc: 0.8051 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 519/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4552 - acc: 0.8049 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 520/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4552 - acc: 0.8058 - val_loss: 0.4545 - val_acc: 0.8039\n",
      "Epoch 521/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4552 - acc: 0.8046 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 522/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4552 - acc: 0.8053 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 523/1500\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4552 - acc: 0.8054 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 524/1500\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4552 - acc: 0.8050 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 525/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4552 - acc: 0.8053 - val_loss: 0.4545 - val_acc: 0.8043\n",
      "Epoch 526/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4552 - acc: 0.8054 - val_loss: 0.4544 - val_acc: 0.8043\n",
      "Epoch 527/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4552 - acc: 0.8054 - val_loss: 0.4544 - val_acc: 0.8043\n",
      "Epoch 528/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4551 - acc: 0.8046 - val_loss: 0.4544 - val_acc: 0.8043\n",
      "Epoch 529/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4551 - acc: 0.8055 - val_loss: 0.4544 - val_acc: 0.8043\n",
      "Epoch 530/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4551 - acc: 0.8053 - val_loss: 0.4544 - val_acc: 0.8047\n",
      "Epoch 531/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4551 - acc: 0.8053 - val_loss: 0.4544 - val_acc: 0.8043\n",
      "Epoch 532/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4551 - acc: 0.8053 - val_loss: 0.4544 - val_acc: 0.8043\n",
      "Epoch 533/1500\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4551 - acc: 0.8049 - val_loss: 0.4544 - val_acc: 0.8047\n",
      "Epoch 534/1500\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4551 - acc: 0.8050 - val_loss: 0.4544 - val_acc: 0.8043\n",
      "Epoch 535/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4551 - acc: 0.8051 - val_loss: 0.4544 - val_acc: 0.8043\n",
      "Epoch 536/1500\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4551 - acc: 0.8045 - val_loss: 0.4544 - val_acc: 0.8051\n",
      "Epoch 537/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4551 - acc: 0.8054 - val_loss: 0.4543 - val_acc: 0.8051\n",
      "Epoch 538/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4550 - acc: 0.8053 - val_loss: 0.4543 - val_acc: 0.8051\n",
      "Epoch 539/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4550 - acc: 0.8061 - val_loss: 0.4543 - val_acc: 0.8043\n",
      "Epoch 540/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4550 - acc: 0.8046 - val_loss: 0.4543 - val_acc: 0.8051\n",
      "Epoch 541/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4550 - acc: 0.8053 - val_loss: 0.4543 - val_acc: 0.8051\n",
      "Epoch 542/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4550 - acc: 0.8050 - val_loss: 0.4543 - val_acc: 0.8047\n",
      "Epoch 543/1500\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4550 - acc: 0.8043 - val_loss: 0.4543 - val_acc: 0.8051\n",
      "Epoch 544/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4550 - acc: 0.8057 - val_loss: 0.4543 - val_acc: 0.8047\n",
      "Epoch 545/1500\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4550 - acc: 0.8053 - val_loss: 0.4543 - val_acc: 0.8043\n",
      "Epoch 546/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4550 - acc: 0.8050 - val_loss: 0.4543 - val_acc: 0.8051\n",
      "Epoch 547/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4550 - acc: 0.8055 - val_loss: 0.4543 - val_acc: 0.8047\n",
      "Epoch 548/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4550 - acc: 0.8061 - val_loss: 0.4543 - val_acc: 0.8043\n",
      "Epoch 549/1500\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4550 - acc: 0.8051 - val_loss: 0.4543 - val_acc: 0.8043\n",
      "Epoch 550/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4550 - acc: 0.8050 - val_loss: 0.4543 - val_acc: 0.8051\n",
      "Epoch 551/1500\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4550 - acc: 0.8058 - val_loss: 0.4543 - val_acc: 0.8047\n",
      "Epoch 552/1500\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4549 - acc: 0.8057 - val_loss: 0.4543 - val_acc: 0.8051\n",
      "Epoch 553/1500\n",
      "1984/7477 [======>.......................] - ETA: 0s - loss: 0.4520 - acc: 0.8039"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-30a1b7d4e454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train function!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_hist_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# the fit function returns the run history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# It is very convenient, as it contains information about the model fit, iterations etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    117\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   4117\u001b[0m     \"\"\"\n\u001b[1;32m   4118\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 4119\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   4120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   4006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4007\u001b[0m     \"\"\"\n\u001b[0;32m-> 4008\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4009\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4010\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train function!!\n",
    "run_hist_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)\n",
    "\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7477 samples, validate on 2493 samples\n",
      "Epoch 1/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4550 - acc: 0.8049 - val_loss: 0.4543 - val_acc: 0.8043\n",
      "Epoch 2/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4549 - acc: 0.8055 - val_loss: 0.4542 - val_acc: 0.8047\n",
      "Epoch 3/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4549 - acc: 0.8053 - val_loss: 0.4543 - val_acc: 0.8047\n",
      "Epoch 4/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4549 - acc: 0.8051 - val_loss: 0.4543 - val_acc: 0.8039\n",
      "Epoch 5/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4549 - acc: 0.8049 - val_loss: 0.4543 - val_acc: 0.8047\n",
      "Epoch 6/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4549 - acc: 0.8053 - val_loss: 0.4543 - val_acc: 0.8051\n",
      "Epoch 7/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4549 - acc: 0.8059 - val_loss: 0.4542 - val_acc: 0.8039\n",
      "Epoch 8/3000\n",
      "7477/7477 [==============================] - 0s 44us/step - loss: 0.4549 - acc: 0.8050 - val_loss: 0.4542 - val_acc: 0.8051\n",
      "Epoch 9/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4549 - acc: 0.8055 - val_loss: 0.4542 - val_acc: 0.8051\n",
      "Epoch 10/3000\n",
      "7477/7477 [==============================] - 0s 46us/step - loss: 0.4549 - acc: 0.8050 - val_loss: 0.4542 - val_acc: 0.8051\n",
      "Epoch 11/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4549 - acc: 0.8055 - val_loss: 0.4542 - val_acc: 0.8055\n",
      "Epoch 12/3000\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4548 - acc: 0.8057 - val_loss: 0.4542 - val_acc: 0.8059\n",
      "Epoch 13/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4548 - acc: 0.8057 - val_loss: 0.4542 - val_acc: 0.8051\n",
      "Epoch 14/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4548 - acc: 0.8053 - val_loss: 0.4542 - val_acc: 0.8055\n",
      "Epoch 15/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4548 - acc: 0.8057 - val_loss: 0.4542 - val_acc: 0.8051\n",
      "Epoch 16/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4548 - acc: 0.8050 - val_loss: 0.4542 - val_acc: 0.8067\n",
      "Epoch 17/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4548 - acc: 0.8054 - val_loss: 0.4542 - val_acc: 0.8067\n",
      "Epoch 18/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4548 - acc: 0.8045 - val_loss: 0.4542 - val_acc: 0.8067\n",
      "Epoch 19/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4548 - acc: 0.8059 - val_loss: 0.4542 - val_acc: 0.8071\n",
      "Epoch 20/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4548 - acc: 0.8055 - val_loss: 0.4542 - val_acc: 0.8067\n",
      "Epoch 21/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4548 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8067\n",
      "Epoch 22/3000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4548 - acc: 0.8047 - val_loss: 0.4542 - val_acc: 0.8071\n",
      "Epoch 23/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4548 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8071\n",
      "Epoch 24/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4548 - acc: 0.8059 - val_loss: 0.4541 - val_acc: 0.8071\n",
      "Epoch 25/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4547 - acc: 0.8058 - val_loss: 0.4541 - val_acc: 0.8047\n",
      "Epoch 26/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4548 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8039\n",
      "Epoch 27/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4547 - acc: 0.8065 - val_loss: 0.4541 - val_acc: 0.8043\n",
      "Epoch 28/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4547 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8047\n",
      "Epoch 29/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4547 - acc: 0.8053 - val_loss: 0.4541 - val_acc: 0.8071\n",
      "Epoch 30/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4547 - acc: 0.8058 - val_loss: 0.4541 - val_acc: 0.8047\n",
      "Epoch 31/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4547 - acc: 0.8053 - val_loss: 0.4541 - val_acc: 0.8067\n",
      "Epoch 32/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4547 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8067\n",
      "Epoch 33/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4547 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8067\n",
      "Epoch 34/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4547 - acc: 0.8050 - val_loss: 0.4541 - val_acc: 0.8067\n",
      "Epoch 35/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4547 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8067\n",
      "Epoch 36/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4547 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8071\n",
      "Epoch 37/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4547 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8063\n",
      "Epoch 38/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4547 - acc: 0.8058 - val_loss: 0.4541 - val_acc: 0.8067\n",
      "Epoch 39/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4547 - acc: 0.8058 - val_loss: 0.4541 - val_acc: 0.8063\n",
      "Epoch 40/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4547 - acc: 0.8057 - val_loss: 0.4540 - val_acc: 0.8071\n",
      "Epoch 41/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4547 - acc: 0.8053 - val_loss: 0.4540 - val_acc: 0.8067\n",
      "Epoch 42/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4546 - acc: 0.8051 - val_loss: 0.4540 - val_acc: 0.8067\n",
      "Epoch 43/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4546 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8063\n",
      "Epoch 44/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4546 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8063\n",
      "Epoch 45/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4546 - acc: 0.8061 - val_loss: 0.4540 - val_acc: 0.8063\n",
      "Epoch 46/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4546 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8067\n",
      "Epoch 47/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4546 - acc: 0.8058 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 48/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4546 - acc: 0.8058 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 49/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4546 - acc: 0.8057 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 50/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4546 - acc: 0.8057 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 51/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4546 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 52/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4546 - acc: 0.8057 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 53/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4546 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 54/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4546 - acc: 0.8055 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 55/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4546 - acc: 0.8057 - val_loss: 0.4540 - val_acc: 0.8063\n",
      "Epoch 56/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4545 - acc: 0.8059 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 57/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4546 - acc: 0.8053 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 58/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4545 - acc: 0.8054 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 59/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4545 - acc: 0.8055 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 60/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4545 - acc: 0.8055 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 61/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4545 - acc: 0.8057 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 62/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4545 - acc: 0.8050 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 63/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4545 - acc: 0.8051 - val_loss: 0.4539 - val_acc: 0.8063\n",
      "Epoch 64/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4545 - acc: 0.8058 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 65/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4545 - acc: 0.8057 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 66/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4545 - acc: 0.8050 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 67/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4545 - acc: 0.8053 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 68/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4545 - acc: 0.8057 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 69/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4545 - acc: 0.8053 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 70/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4544 - acc: 0.8059 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 71/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4544 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 72/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4544 - acc: 0.8053 - val_loss: 0.4539 - val_acc: 0.8063\n",
      "Epoch 73/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4544 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 74/3000\n",
      "7477/7477 [==============================] - 0s 52us/step - loss: 0.4544 - acc: 0.8055 - val_loss: 0.4539 - val_acc: 0.8063\n",
      "Epoch 75/3000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4544 - acc: 0.8057 - val_loss: 0.4539 - val_acc: 0.8063\n",
      "Epoch 76/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4544 - acc: 0.8051 - val_loss: 0.4539 - val_acc: 0.8063\n",
      "Epoch 77/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4544 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8063\n",
      "Epoch 78/3000\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4544 - acc: 0.8053 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 79/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4544 - acc: 0.8053 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 80/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4544 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8063\n",
      "Epoch 81/3000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4543 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8063\n",
      "Epoch 82/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4543 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 83/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4544 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 84/3000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4543 - acc: 0.8054 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 85/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4543 - acc: 0.8061 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 86/3000\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4543 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 87/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4543 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 88/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4543 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 89/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4543 - acc: 0.8053 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 90/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4543 - acc: 0.8058 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 91/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4543 - acc: 0.8058 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 92/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4543 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 93/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4543 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 94/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4543 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 95/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4542 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 96/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4543 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 97/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4543 - acc: 0.8058 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 98/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4543 - acc: 0.8051 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 99/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4542 - acc: 0.8053 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 100/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4542 - acc: 0.8050 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 101/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4542 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 102/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4542 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 103/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4542 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 104/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4542 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 105/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4542 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 106/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4542 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 107/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4542 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 108/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4542 - acc: 0.8058 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 109/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4542 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8063\n",
      "Epoch 110/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4542 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 111/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4542 - acc: 0.8059 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 112/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4541 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 113/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4542 - acc: 0.8059 - val_loss: 0.4540 - val_acc: 0.8063\n",
      "Epoch 114/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4541 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8063\n",
      "Epoch 115/3000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4541 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8063\n",
      "Epoch 116/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4541 - acc: 0.8053 - val_loss: 0.4541 - val_acc: 0.8063\n",
      "Epoch 117/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4541 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 118/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4541 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8063\n",
      "Epoch 119/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4541 - acc: 0.8058 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 120/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4541 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8063\n",
      "Epoch 121/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4541 - acc: 0.8061 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 122/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4541 - acc: 0.8059 - val_loss: 0.4541 - val_acc: 0.8051\n",
      "Epoch 123/3000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4541 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 124/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4541 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8051\n",
      "Epoch 125/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4541 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 126/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4541 - acc: 0.8050 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 127/3000\n",
      "7477/7477 [==============================] - 0s 53us/step - loss: 0.4541 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 128/3000\n",
      "7477/7477 [==============================] - 0s 44us/step - loss: 0.4540 - acc: 0.8053 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 129/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4540 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 130/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4540 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 131/3000\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4540 - acc: 0.8051 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 132/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4540 - acc: 0.8049 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 133/3000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4540 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8047\n",
      "Epoch 134/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4540 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 135/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4540 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8059\n",
      "Epoch 136/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4540 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8055\n",
      "Epoch 137/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4540 - acc: 0.8053 - val_loss: 0.4541 - val_acc: 0.8051\n",
      "Epoch 138/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4540 - acc: 0.8062 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 139/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4540 - acc: 0.8051 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 140/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4539 - acc: 0.8058 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 141/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4540 - acc: 0.8051 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 142/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4540 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8059\n",
      "Epoch 143/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4539 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8051\n",
      "Epoch 144/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4539 - acc: 0.8063 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 145/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4539 - acc: 0.8053 - val_loss: 0.4541 - val_acc: 0.8051\n",
      "Epoch 146/3000\n",
      "7477/7477 [==============================] - 0s 51us/step - loss: 0.4539 - acc: 0.8059 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 147/3000\n",
      "7477/7477 [==============================] - 0s 46us/step - loss: 0.4539 - acc: 0.8059 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 148/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4539 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 149/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4539 - acc: 0.8057 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 150/3000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4539 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 151/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4539 - acc: 0.8062 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 152/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4539 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8051\n",
      "Epoch 153/3000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4539 - acc: 0.8055 - val_loss: 0.4541 - val_acc: 0.8051\n",
      "Epoch 154/3000\n",
      "7477/7477 [==============================] - 0s 48us/step - loss: 0.4539 - acc: 0.8058 - val_loss: 0.4541 - val_acc: 0.8047\n",
      "Epoch 155/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4539 - acc: 0.8062 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 156/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4539 - acc: 0.8054 - val_loss: 0.4541 - val_acc: 0.8043\n",
      "Epoch 157/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4539 - acc: 0.8057 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 158/3000\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4539 - acc: 0.8062 - val_loss: 0.4540 - val_acc: 0.8047\n",
      "Epoch 159/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4539 - acc: 0.8057 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 160/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4538 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8047\n",
      "Epoch 161/3000\n",
      "7477/7477 [==============================] - 0s 44us/step - loss: 0.4538 - acc: 0.8065 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 162/3000\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4539 - acc: 0.8059 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 163/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4538 - acc: 0.8058 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 164/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4538 - acc: 0.8062 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 165/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4538 - acc: 0.8062 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 166/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4538 - acc: 0.8063 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 167/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4538 - acc: 0.8055 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 168/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4538 - acc: 0.8059 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 169/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4538 - acc: 0.8061 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 170/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4538 - acc: 0.8058 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 171/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4538 - acc: 0.8058 - val_loss: 0.4539 - val_acc: 0.8047\n",
      "Epoch 172/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4538 - acc: 0.8055 - val_loss: 0.4539 - val_acc: 0.8051\n",
      "Epoch 173/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4538 - acc: 0.8061 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 174/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4538 - acc: 0.8062 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 175/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4538 - acc: 0.8054 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 176/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4538 - acc: 0.8070 - val_loss: 0.4540 - val_acc: 0.8055\n",
      "Epoch 177/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4537 - acc: 0.8062 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 178/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4538 - acc: 0.8055 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 179/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4537 - acc: 0.8058 - val_loss: 0.4539 - val_acc: 0.8051\n",
      "Epoch 180/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4538 - acc: 0.8065 - val_loss: 0.4539 - val_acc: 0.8047\n",
      "Epoch 181/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4537 - acc: 0.8054 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 182/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4538 - acc: 0.8063 - val_loss: 0.4539 - val_acc: 0.8047\n",
      "Epoch 183/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4537 - acc: 0.8061 - val_loss: 0.4539 - val_acc: 0.8051\n",
      "Epoch 184/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4537 - acc: 0.8065 - val_loss: 0.4539 - val_acc: 0.8047\n",
      "Epoch 185/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4538 - acc: 0.8067 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 186/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4537 - acc: 0.8059 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 187/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8059 - val_loss: 0.4540 - val_acc: 0.8051\n",
      "Epoch 188/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4537 - acc: 0.8066 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 189/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8067 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 190/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4537 - acc: 0.8059 - val_loss: 0.4539 - val_acc: 0.8051\n",
      "Epoch 191/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8063 - val_loss: 0.4539 - val_acc: 0.8051\n",
      "Epoch 192/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8065 - val_loss: 0.4539 - val_acc: 0.8051\n",
      "Epoch 193/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4537 - acc: 0.8063 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 194/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4537 - acc: 0.8073 - val_loss: 0.4539 - val_acc: 0.8051\n",
      "Epoch 195/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4537 - acc: 0.8067 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 196/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8055 - val_loss: 0.4538 - val_acc: 0.8055\n",
      "Epoch 197/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8067 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 198/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4537 - acc: 0.8059 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 199/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 200/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8061 - val_loss: 0.4538 - val_acc: 0.8055\n",
      "Epoch 201/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4537 - acc: 0.8065 - val_loss: 0.4538 - val_acc: 0.8051\n",
      "Epoch 202/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4537 - acc: 0.8069 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 203/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8062 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 204/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4536 - acc: 0.8066 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 205/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4537 - acc: 0.8061 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 206/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4537 - acc: 0.8062 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 207/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8061 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 208/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4537 - acc: 0.8059 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 209/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 210/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4536 - acc: 0.8057 - val_loss: 0.4538 - val_acc: 0.8055\n",
      "Epoch 211/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4536 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8055\n",
      "Epoch 212/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4536 - acc: 0.8065 - val_loss: 0.4538 - val_acc: 0.8055\n",
      "Epoch 213/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4537 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 214/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4536 - acc: 0.8073 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 215/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4536 - acc: 0.8066 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 216/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4536 - acc: 0.8066 - val_loss: 0.4539 - val_acc: 0.8063\n",
      "Epoch 217/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4536 - acc: 0.8066 - val_loss: 0.4539 - val_acc: 0.8055\n",
      "Epoch 218/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4536 - acc: 0.8070 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 219/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4536 - acc: 0.8066 - val_loss: 0.4539 - val_acc: 0.8059\n",
      "Epoch 220/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4536 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 221/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4536 - acc: 0.8066 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 222/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4536 - acc: 0.8075 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 223/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4536 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 224/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4536 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 225/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4536 - acc: 0.8061 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 226/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4536 - acc: 0.8066 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 227/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4536 - acc: 0.8063 - val_loss: 0.4539 - val_acc: 0.8063\n",
      "Epoch 228/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4536 - acc: 0.8066 - val_loss: 0.4539 - val_acc: 0.8063\n",
      "Epoch 229/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4536 - acc: 0.8070 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 230/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4536 - acc: 0.8069 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 231/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4535 - acc: 0.8059 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 232/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4535 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 233/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4535 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 234/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4535 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 235/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4535 - acc: 0.8073 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 236/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4535 - acc: 0.8069 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 237/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4535 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 238/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4535 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 239/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4535 - acc: 0.8073 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 240/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4535 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 241/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4535 - acc: 0.8067 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 242/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4535 - acc: 0.8067 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 243/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4535 - acc: 0.8070 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 244/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4535 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 245/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4535 - acc: 0.8066 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 246/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4535 - acc: 0.8067 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 247/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4535 - acc: 0.8070 - val_loss: 0.4539 - val_acc: 0.8067\n",
      "Epoch 248/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8066 - val_loss: 0.4539 - val_acc: 0.8071\n",
      "Epoch 249/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4535 - acc: 0.8074 - val_loss: 0.4538 - val_acc: 0.8071\n",
      "Epoch 250/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4535 - acc: 0.8066 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 251/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4535 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 252/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4535 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 253/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4535 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 254/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4535 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 255/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4534 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 256/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4535 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 257/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4534 - acc: 0.8063 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 258/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 259/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4534 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 260/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4534 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 261/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 262/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4534 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 263/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 264/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 265/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 266/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 267/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 268/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 269/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4534 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 270/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4534 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 271/3000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4534 - acc: 0.8062 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 272/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4534 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 273/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 274/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4534 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 275/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4534 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 276/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4534 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 277/3000\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4533 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 278/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4533 - acc: 0.8061 - val_loss: 0.4538 - val_acc: 0.8071\n",
      "Epoch 279/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4533 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 280/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 281/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8066 - val_loss: 0.4538 - val_acc: 0.8071\n",
      "Epoch 282/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8063 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 283/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 284/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 285/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 286/3000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4533 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 287/3000\n",
      "7477/7477 [==============================] - 0s 57us/step - loss: 0.4533 - acc: 0.8063 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 288/3000\n",
      "7477/7477 [==============================] - 0s 44us/step - loss: 0.4533 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 289/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4533 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 290/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4533 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 291/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4533 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 292/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4533 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 293/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4533 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 294/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4533 - acc: 0.8066 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 295/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4533 - acc: 0.8075 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 296/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4533 - acc: 0.8069 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 297/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 298/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4533 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 299/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4533 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 300/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4532 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 301/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4533 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 302/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4532 - acc: 0.8066 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 303/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 304/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8065 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 305/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 306/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 307/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4532 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 308/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8062 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 309/3000\n",
      "7477/7477 [==============================] - 0s 43us/step - loss: 0.4532 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 310/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 311/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 312/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8070 - val_loss: 0.4538 - val_acc: 0.8071\n",
      "Epoch 313/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 314/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8066 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 315/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4532 - acc: 0.8065 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 316/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 317/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 318/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 319/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 320/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8066 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 321/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4532 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 322/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 323/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4532 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 324/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 325/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4532 - acc: 0.8070 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 326/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8073 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 327/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4531 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 328/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4532 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 329/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 330/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 331/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8073 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 332/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 333/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4531 - acc: 0.8063 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 334/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 335/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 336/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8063 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 337/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 338/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8066 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 339/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8065 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 340/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8066 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 341/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4531 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 342/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 343/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8070 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 344/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8065 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 345/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8074 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 346/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 347/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8059 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 348/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 349/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 350/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8070 - val_loss: 0.4535 - val_acc: 0.8067\n",
      "Epoch 351/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8066 - val_loss: 0.4535 - val_acc: 0.8071\n",
      "Epoch 352/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8070 - val_loss: 0.4535 - val_acc: 0.8063\n",
      "Epoch 353/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8069 - val_loss: 0.4535 - val_acc: 0.8063\n",
      "Epoch 354/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.8065 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 355/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.8058 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 356/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8073 - val_loss: 0.4535 - val_acc: 0.8059\n",
      "Epoch 357/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4530 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 358/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 359/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4531 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 360/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8063 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 361/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4530 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 362/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4531 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8055\n",
      "Epoch 363/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 364/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.8070 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 365/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4530 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 366/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8067 - val_loss: 0.4538 - val_acc: 0.8071\n",
      "Epoch 367/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 368/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4530 - acc: 0.8066 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 369/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 370/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 371/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 372/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8066 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 373/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4530 - acc: 0.8070 - val_loss: 0.4536 - val_acc: 0.8055\n",
      "Epoch 374/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 375/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 376/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8073 - val_loss: 0.4535 - val_acc: 0.8055\n",
      "Epoch 377/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8063 - val_loss: 0.4536 - val_acc: 0.8055\n",
      "Epoch 378/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4530 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 379/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8066 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 380/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 381/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8075 - val_loss: 0.4535 - val_acc: 0.8055\n",
      "Epoch 382/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4530 - acc: 0.8073 - val_loss: 0.4536 - val_acc: 0.8055\n",
      "Epoch 383/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4530 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8055\n",
      "Epoch 384/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4530 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8055\n",
      "Epoch 385/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 386/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8073 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 387/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4530 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8055\n",
      "Epoch 388/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4530 - acc: 0.8070 - val_loss: 0.4536 - val_acc: 0.8055\n",
      "Epoch 389/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4530 - acc: 0.8070 - val_loss: 0.4536 - val_acc: 0.8055\n",
      "Epoch 390/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 391/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8063 - val_loss: 0.4537 - val_acc: 0.8055\n",
      "Epoch 392/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8073 - val_loss: 0.4536 - val_acc: 0.8055\n",
      "Epoch 393/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 394/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 395/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8059 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 396/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 397/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 398/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 399/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 400/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8065 - val_loss: 0.4537 - val_acc: 0.8055\n",
      "Epoch 401/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8067 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 402/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8066 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 403/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 404/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 405/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8055\n",
      "Epoch 406/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8070 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 407/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 408/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8055\n",
      "Epoch 409/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4529 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 410/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4529 - acc: 0.8066 - val_loss: 0.4537 - val_acc: 0.8055\n",
      "Epoch 411/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4529 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 412/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4528 - acc: 0.8070 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 413/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4528 - acc: 0.8077 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 414/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4528 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 415/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 416/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.8069 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 417/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4529 - acc: 0.8074 - val_loss: 0.4538 - val_acc: 0.8059\n",
      "Epoch 418/3000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4529 - acc: 0.8070 - val_loss: 0.4538 - val_acc: 0.8063\n",
      "Epoch 419/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4529 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 420/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4529 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 421/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4528 - acc: 0.8067 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 422/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4528 - acc: 0.8070 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 423/3000\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4528 - acc: 0.8069 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 424/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4528 - acc: 0.8066 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 425/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4528 - acc: 0.8062 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 426/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4528 - acc: 0.8066 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 427/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4528 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 428/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 429/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4528 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 430/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4528 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 431/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4528 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 432/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4528 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 433/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4528 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 434/3000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4528 - acc: 0.8063 - val_loss: 0.4538 - val_acc: 0.8067\n",
      "Epoch 435/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4528 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 436/3000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4528 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 437/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4528 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 438/3000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4528 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 439/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4528 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 440/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4528 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 441/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4528 - acc: 0.8067 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 442/3000\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4528 - acc: 0.8075 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 443/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4528 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 444/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4528 - acc: 0.8073 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 445/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4527 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 446/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4527 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 447/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4528 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 448/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4527 - acc: 0.8079 - val_loss: 0.4538 - val_acc: 0.8071\n",
      "Epoch 449/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4527 - acc: 0.8073 - val_loss: 0.4538 - val_acc: 0.8071\n",
      "Epoch 450/3000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4527 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 451/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4527 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 452/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4527 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 453/3000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4527 - acc: 0.8070 - val_loss: 0.4538 - val_acc: 0.8071\n",
      "Epoch 454/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8075 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 455/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4527 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 456/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4527 - acc: 0.8081 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 457/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4527 - acc: 0.8069 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 458/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4527 - acc: 0.8075 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 459/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8073 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 460/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4527 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 461/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4527 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 462/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4527 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 463/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4527 - acc: 0.8079 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 464/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 465/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4527 - acc: 0.8070 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 466/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8078 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 467/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8067\n",
      "Epoch 468/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8077 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 469/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 470/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4527 - acc: 0.8070 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 471/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4527 - acc: 0.8081 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 472/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4527 - acc: 0.8075 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 473/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4526 - acc: 0.8079 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 474/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4526 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 475/3000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4526 - acc: 0.8075 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 476/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4527 - acc: 0.8075 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 477/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4526 - acc: 0.8071 - val_loss: 0.4536 - val_acc: 0.8059\n",
      "Epoch 478/3000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4526 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8059\n",
      "Epoch 479/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8074 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 480/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4526 - acc: 0.8073 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 481/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4526 - acc: 0.8071 - val_loss: 0.4537 - val_acc: 0.8063\n",
      "Epoch 482/3000\n",
      "7477/7477 [==============================] - 0s 39us/step - loss: 0.4526 - acc: 0.8079 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 483/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4526 - acc: 0.8082 - val_loss: 0.4538 - val_acc: 0.8083\n",
      "Epoch 484/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4526 - acc: 0.8083 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 485/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4526 - acc: 0.8074 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 486/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4526 - acc: 0.8071 - val_loss: 0.4538 - val_acc: 0.8071\n",
      "Epoch 487/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4526 - acc: 0.8077 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 488/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4526 - acc: 0.8079 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 489/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4526 - acc: 0.8079 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 490/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4526 - acc: 0.8078 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 491/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8081 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 492/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4526 - acc: 0.8073 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 493/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4526 - acc: 0.8075 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 494/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4525 - acc: 0.8075 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 495/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4525 - acc: 0.8074 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 496/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4525 - acc: 0.8079 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 497/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4525 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 498/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4525 - acc: 0.8078 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 499/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4525 - acc: 0.8075 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 500/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4525 - acc: 0.8081 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 501/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4525 - acc: 0.8073 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 502/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4525 - acc: 0.8073 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 503/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4525 - acc: 0.8082 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 504/3000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4525 - acc: 0.8077 - val_loss: 0.4538 - val_acc: 0.8083\n",
      "Epoch 505/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4525 - acc: 0.8079 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 506/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4525 - acc: 0.8083 - val_loss: 0.4539 - val_acc: 0.8079\n",
      "Epoch 507/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4525 - acc: 0.8079 - val_loss: 0.4538 - val_acc: 0.8083\n",
      "Epoch 508/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8083 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 509/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4524 - acc: 0.8077 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 510/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4524 - acc: 0.8082 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 511/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8081 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 512/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4524 - acc: 0.8081 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 513/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4524 - acc: 0.8079 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 514/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4524 - acc: 0.8081 - val_loss: 0.4537 - val_acc: 0.8087\n",
      "Epoch 515/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4524 - acc: 0.8078 - val_loss: 0.4538 - val_acc: 0.8083\n",
      "Epoch 516/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4524 - acc: 0.8090 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 517/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4524 - acc: 0.8083 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 518/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4524 - acc: 0.8081 - val_loss: 0.4538 - val_acc: 0.8087\n",
      "Epoch 519/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4524 - acc: 0.8083 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 520/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8081 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 521/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8089 - val_loss: 0.4538 - val_acc: 0.8075\n",
      "Epoch 522/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8086 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 523/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8086 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 524/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4523 - acc: 0.8087 - val_loss: 0.4536 - val_acc: 0.8063\n",
      "Epoch 525/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8087 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 526/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4523 - acc: 0.8085 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 527/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4523 - acc: 0.8079 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 528/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4524 - acc: 0.8086 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 529/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4523 - acc: 0.8089 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 530/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4523 - acc: 0.8086 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 531/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4523 - acc: 0.8085 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 532/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4523 - acc: 0.8081 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 533/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8089 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 534/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4523 - acc: 0.8083 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 535/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4523 - acc: 0.8087 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 536/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8090 - val_loss: 0.4536 - val_acc: 0.8087\n",
      "Epoch 537/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4523 - acc: 0.8086 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 538/3000\n",
      "7477/7477 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8086 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 539/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4523 - acc: 0.8087 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 540/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8085 - val_loss: 0.4537 - val_acc: 0.8091\n",
      "Epoch 541/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8085 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 542/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8085 - val_loss: 0.4536 - val_acc: 0.8067\n",
      "Epoch 543/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8087 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 544/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8085 - val_loss: 0.4538 - val_acc: 0.8079\n",
      "Epoch 545/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4523 - acc: 0.8087 - val_loss: 0.4537 - val_acc: 0.8071\n",
      "Epoch 546/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8089 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 547/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4522 - acc: 0.8093 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 548/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8089 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 549/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8085 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 550/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4522 - acc: 0.8089 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 551/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8090 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 552/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4522 - acc: 0.8090 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 553/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4522 - acc: 0.8089 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 554/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8093 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 555/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8086 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 556/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8089 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 557/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8082 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 558/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 559/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4522 - acc: 0.8089 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 560/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4522 - acc: 0.8090 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 561/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8086 - val_loss: 0.4538 - val_acc: 0.8083\n",
      "Epoch 562/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4522 - acc: 0.8087 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 563/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4522 - acc: 0.8094 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 564/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4522 - acc: 0.8090 - val_loss: 0.4535 - val_acc: 0.8071\n",
      "Epoch 565/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8090 - val_loss: 0.4537 - val_acc: 0.8075\n",
      "Epoch 566/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4522 - acc: 0.8090 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 567/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8091 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 568/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8090 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 569/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4521 - acc: 0.8089 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 570/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8075\n",
      "Epoch 571/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8087 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 572/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8087 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 573/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4521 - acc: 0.8089 - val_loss: 0.4535 - val_acc: 0.8071\n",
      "Epoch 574/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4521 - acc: 0.8097 - val_loss: 0.4535 - val_acc: 0.8079\n",
      "Epoch 575/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8090 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 576/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8094 - val_loss: 0.4537 - val_acc: 0.8083\n",
      "Epoch 577/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4521 - acc: 0.8095 - val_loss: 0.4536 - val_acc: 0.8083\n",
      "Epoch 578/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8090 - val_loss: 0.4536 - val_acc: 0.8083\n",
      "Epoch 579/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4521 - acc: 0.8094 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 580/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4521 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 581/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8087 - val_loss: 0.4536 - val_acc: 0.8071\n",
      "Epoch 582/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4521 - acc: 0.8086 - val_loss: 0.4537 - val_acc: 0.8079\n",
      "Epoch 583/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8090 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 584/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8089 - val_loss: 0.4537 - val_acc: 0.8087\n",
      "Epoch 585/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8094 - val_loss: 0.4535 - val_acc: 0.8079\n",
      "Epoch 586/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8090 - val_loss: 0.4536 - val_acc: 0.8091\n",
      "Epoch 587/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4521 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 588/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4521 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 589/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4520 - acc: 0.8090 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 590/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8094 - val_loss: 0.4535 - val_acc: 0.8079\n",
      "Epoch 591/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8087\n",
      "Epoch 592/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8085 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 593/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8087\n",
      "Epoch 594/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4521 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8083\n",
      "Epoch 595/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8093 - val_loss: 0.4535 - val_acc: 0.8071\n",
      "Epoch 596/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8094 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 597/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8091 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 598/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8095 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 599/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8100 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 600/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4520 - acc: 0.8094 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 601/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4520 - acc: 0.8091 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 602/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8094 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 603/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8091 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 604/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8098 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 605/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8087\n",
      "Epoch 606/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8086 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 607/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8093 - val_loss: 0.4536 - val_acc: 0.8079\n",
      "Epoch 608/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8091 - val_loss: 0.4536 - val_acc: 0.8091\n",
      "Epoch 609/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8094 - val_loss: 0.4536 - val_acc: 0.8083\n",
      "Epoch 610/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8091 - val_loss: 0.4535 - val_acc: 0.8079\n",
      "Epoch 611/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8094 - val_loss: 0.4535 - val_acc: 0.8079\n",
      "Epoch 612/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8090 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 613/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4519 - acc: 0.8091 - val_loss: 0.4534 - val_acc: 0.8075\n",
      "Epoch 614/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4519 - acc: 0.8094 - val_loss: 0.4534 - val_acc: 0.8071\n",
      "Epoch 615/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4520 - acc: 0.8091 - val_loss: 0.4534 - val_acc: 0.8075\n",
      "Epoch 616/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8097 - val_loss: 0.4535 - val_acc: 0.8083\n",
      "Epoch 617/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4519 - acc: 0.8090 - val_loss: 0.4535 - val_acc: 0.8091\n",
      "Epoch 618/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4519 - acc: 0.8097 - val_loss: 0.4534 - val_acc: 0.8075\n",
      "Epoch 619/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4519 - acc: 0.8091 - val_loss: 0.4535 - val_acc: 0.8083\n",
      "Epoch 620/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4519 - acc: 0.8094 - val_loss: 0.4534 - val_acc: 0.8083\n",
      "Epoch 621/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4520 - acc: 0.8100 - val_loss: 0.4534 - val_acc: 0.8083\n",
      "Epoch 622/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4519 - acc: 0.8094 - val_loss: 0.4534 - val_acc: 0.8075\n",
      "Epoch 623/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4519 - acc: 0.8090 - val_loss: 0.4534 - val_acc: 0.8075\n",
      "Epoch 624/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4519 - acc: 0.8093 - val_loss: 0.4534 - val_acc: 0.8075\n",
      "Epoch 625/3000\n",
      "7477/7477 [==============================] - 0s 25us/step - loss: 0.4519 - acc: 0.8094 - val_loss: 0.4534 - val_acc: 0.8079\n",
      "Epoch 626/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4519 - acc: 0.8091 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 627/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4519 - acc: 0.8097 - val_loss: 0.4535 - val_acc: 0.8091\n",
      "Epoch 628/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4519 - acc: 0.8097 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 629/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4519 - acc: 0.8098 - val_loss: 0.4534 - val_acc: 0.8075\n",
      "Epoch 630/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4519 - acc: 0.8097 - val_loss: 0.4534 - val_acc: 0.8075\n",
      "Epoch 631/3000\n",
      "7477/7477 [==============================] - 0s 31us/step - loss: 0.4519 - acc: 0.8091 - val_loss: 0.4535 - val_acc: 0.8091\n",
      "Epoch 632/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4519 - acc: 0.8093 - val_loss: 0.4534 - val_acc: 0.8079\n",
      "Epoch 633/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4519 - acc: 0.8090 - val_loss: 0.4534 - val_acc: 0.8075\n",
      "Epoch 634/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4519 - acc: 0.8094 - val_loss: 0.4534 - val_acc: 0.8079\n",
      "Epoch 635/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4518 - acc: 0.8093 - val_loss: 0.4535 - val_acc: 0.8087\n",
      "Epoch 636/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4519 - acc: 0.8094 - val_loss: 0.4535 - val_acc: 0.8087\n",
      "Epoch 637/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4518 - acc: 0.8095 - val_loss: 0.4535 - val_acc: 0.8075\n",
      "Epoch 638/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4518 - acc: 0.8094 - val_loss: 0.4536 - val_acc: 0.8087\n",
      "Epoch 639/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4519 - acc: 0.8095 - val_loss: 0.4535 - val_acc: 0.8091\n",
      "Epoch 640/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4519 - acc: 0.8095 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 641/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4534 - val_acc: 0.8079\n",
      "Epoch 642/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4535 - val_acc: 0.8083\n",
      "Epoch 643/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4534 - val_acc: 0.8079\n",
      "Epoch 644/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4518 - acc: 0.8087 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 645/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4518 - acc: 0.8090 - val_loss: 0.4534 - val_acc: 0.8091\n",
      "Epoch 646/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4518 - acc: 0.8093 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 647/3000\n",
      "7477/7477 [==============================] - 0s 40us/step - loss: 0.4518 - acc: 0.8087 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 648/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4518 - acc: 0.8094 - val_loss: 0.4534 - val_acc: 0.8083\n",
      "Epoch 649/3000\n",
      "7477/7477 [==============================] - 0s 38us/step - loss: 0.4518 - acc: 0.8094 - val_loss: 0.4534 - val_acc: 0.8083\n",
      "Epoch 650/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7477/7477 [==============================] - 0s 44us/step - loss: 0.4518 - acc: 0.8093 - val_loss: 0.4533 - val_acc: 0.8083\n",
      "Epoch 651/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4518 - acc: 0.8093 - val_loss: 0.4534 - val_acc: 0.8091\n",
      "Epoch 652/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4518 - acc: 0.8095 - val_loss: 0.4535 - val_acc: 0.8095\n",
      "Epoch 653/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4518 - acc: 0.8093 - val_loss: 0.4534 - val_acc: 0.8091\n",
      "Epoch 654/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4518 - acc: 0.8087 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 655/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4518 - acc: 0.8097 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 656/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4518 - acc: 0.8093 - val_loss: 0.4533 - val_acc: 0.8091\n",
      "Epoch 657/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4518 - acc: 0.8091 - val_loss: 0.4533 - val_acc: 0.8083\n",
      "Epoch 658/3000\n",
      "7477/7477 [==============================] - 0s 33us/step - loss: 0.4517 - acc: 0.8090 - val_loss: 0.4534 - val_acc: 0.8091\n",
      "Epoch 659/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4517 - acc: 0.8087 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 660/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.4533 - val_acc: 0.8075\n",
      "Epoch 661/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4517 - acc: 0.8085 - val_loss: 0.4534 - val_acc: 0.8091\n",
      "Epoch 662/3000\n",
      "7477/7477 [==============================] - 0s 36us/step - loss: 0.4517 - acc: 0.8087 - val_loss: 0.4534 - val_acc: 0.8091\n",
      "Epoch 663/3000\n",
      "7477/7477 [==============================] - 0s 26us/step - loss: 0.4517 - acc: 0.8093 - val_loss: 0.4534 - val_acc: 0.8091\n",
      "Epoch 664/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4517 - acc: 0.8091 - val_loss: 0.4534 - val_acc: 0.8091\n",
      "Epoch 665/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4517 - acc: 0.8094 - val_loss: 0.4535 - val_acc: 0.8087\n",
      "Epoch 666/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4517 - acc: 0.8091 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 667/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.4535 - val_acc: 0.8095\n",
      "Epoch 668/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4517 - acc: 0.8093 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 669/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4517 - acc: 0.8091 - val_loss: 0.4533 - val_acc: 0.8079\n",
      "Epoch 670/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.4533 - val_acc: 0.8079\n",
      "Epoch 671/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 672/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4517 - acc: 0.8097 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 673/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4517 - acc: 0.8089 - val_loss: 0.4533 - val_acc: 0.8091\n",
      "Epoch 674/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4517 - acc: 0.8095 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 675/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4533 - val_acc: 0.8079\n",
      "Epoch 676/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4516 - acc: 0.8083 - val_loss: 0.4533 - val_acc: 0.8083\n",
      "Epoch 677/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4516 - acc: 0.8086 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 678/3000\n",
      "7477/7477 [==============================] - 0s 41us/step - loss: 0.4516 - acc: 0.8095 - val_loss: 0.4532 - val_acc: 0.8083\n",
      "Epoch 679/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4517 - acc: 0.8085 - val_loss: 0.4533 - val_acc: 0.8083\n",
      "Epoch 680/3000\n",
      "7477/7477 [==============================] - 0s 34us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4533 - val_acc: 0.8083\n",
      "Epoch 681/3000\n",
      "7477/7477 [==============================] - 0s 35us/step - loss: 0.4516 - acc: 0.8082 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 682/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4517 - acc: 0.8090 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 683/3000\n",
      "7477/7477 [==============================] - 0s 37us/step - loss: 0.4516 - acc: 0.8085 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 684/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4533 - val_acc: 0.8083\n",
      "Epoch 685/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4516 - acc: 0.8085 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 686/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4516 - acc: 0.8091 - val_loss: 0.4533 - val_acc: 0.8083\n",
      "Epoch 687/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4516 - acc: 0.8091 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 688/3000\n",
      "7477/7477 [==============================] - 0s 27us/step - loss: 0.4516 - acc: 0.8085 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 689/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4516 - acc: 0.8083 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 690/3000\n",
      "7477/7477 [==============================] - 0s 42us/step - loss: 0.4516 - acc: 0.8089 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 691/3000\n",
      "7477/7477 [==============================] - 0s 32us/step - loss: 0.4516 - acc: 0.8090 - val_loss: 0.4532 - val_acc: 0.8075\n",
      "Epoch 692/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4532 - val_acc: 0.8075\n",
      "Epoch 693/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4516 - acc: 0.8087 - val_loss: 0.4533 - val_acc: 0.8075\n",
      "Epoch 694/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4516 - acc: 0.8085 - val_loss: 0.4532 - val_acc: 0.8079\n",
      "Epoch 695/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4516 - acc: 0.8081 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 696/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4516 - acc: 0.8083 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 697/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4516 - acc: 0.8086 - val_loss: 0.4533 - val_acc: 0.8087\n",
      "Epoch 698/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4516 - acc: 0.8085 - val_loss: 0.4534 - val_acc: 0.8095\n",
      "Epoch 699/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4516 - acc: 0.8091 - val_loss: 0.4534 - val_acc: 0.8095\n",
      "Epoch 700/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4515 - acc: 0.8087 - val_loss: 0.4533 - val_acc: 0.8079\n",
      "Epoch 701/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4515 - acc: 0.8083 - val_loss: 0.4532 - val_acc: 0.8075\n",
      "Epoch 702/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4516 - acc: 0.8083 - val_loss: 0.4533 - val_acc: 0.8083\n",
      "Epoch 703/3000\n",
      "7477/7477 [==============================] - 0s 30us/step - loss: 0.4515 - acc: 0.8085 - val_loss: 0.4532 - val_acc: 0.8079\n",
      "Epoch 704/3000\n",
      "7477/7477 [==============================] - 0s 29us/step - loss: 0.4515 - acc: 0.8087 - val_loss: 0.4534 - val_acc: 0.8083\n",
      "Epoch 705/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4515 - acc: 0.8087 - val_loss: 0.4533 - val_acc: 0.8079\n",
      "Epoch 706/3000\n",
      "7477/7477 [==============================] - 0s 28us/step - loss: 0.4515 - acc: 0.8090 - val_loss: 0.4534 - val_acc: 0.8087\n",
      "Epoch 707/3000\n",
      "3680/7477 [=============>................] - ETA: 0s - loss: 0.4443 - acc: 0.8133"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-43ba39ea6977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#There is not a big improvement when training with 2x as many epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_hist_2b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#There is not a big improvement when training with 2x as many epochs\n",
    "run_hist_2b = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_2.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_2.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_2.history[\"acc\"],'r', marker='.', label=\"Train Accuracy\")\n",
    "ax.plot(run_hist_2.history[\"val_acc\"],'b', marker='.', label=\"Validation Accuracy\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_2b.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_2b.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_2b.history[\"acc\"],'r', marker='.', label=\"Train Accuracy\")\n",
    "ax.plot(run_hist_2b.history[\"val_acc\"],'b', marker='.', label=\"Validation Accuracy\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa23c76e160>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_2.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
